import srt
from datetime import timedelta
import tkinter as tk
from tkinter import filedialog
import shutil
from pathlib import Path
import torch
import torchaudio
import torch.nn.functional as F
from pyannote.audio import Pipeline, Audio
from pyannote.core import Annotation, Segment
from pydub import AudioSegment
from speechbrain.inference import EncoderClassifier
import subprocess
import re
import stable_whisper # STTë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from collections import Counter
import traceback
import os


# ========== ì‚¬ìš©ì ì„¤ì • íŒŒë¼ë¯¸í„° ==========
HF_TOKEN = "" # í—ˆê¹…í˜ì´ìŠ¤ í† í°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
FFMPEG_PATH = r"C:\Users\cufs\Desktop\ì—…ë¬´\subtitle\ffmpeg\ffmpeg.exe" # ğŸ“Œ FFmpeg ì‹¤í–‰ íŒŒì¼ì˜ 'ì „ì²´ ê²½ë¡œ'ë¥¼ ì •í™•í•˜ê²Œ ì…ë ¥í•˜ì„¸ìš”.
STT_MODEL_SIZE = "large-v3" # STT ëª¨ë¸ í¬ê¸° ('tiny', 'base', 'small', 'medium', 'large-v3')
FILENAME_SUFFIX = "" # ìµœì¢… íŒŒì¼ëª…ì— ì¶”ê°€ë  ì ‘ë¯¸ì‚¬
MAIN_LANGUAGE = "ko"
SUB_LANGUAGE = None # ë³´ì¡°ì–¸ì–´ë¥¼ ê°•ì œë¡œ ê³ ì •. ê¸°ë³¸ê°’ì€ None


# 1. FFmpeg ë³¼ë¥¨ ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
SILENCE_THRESH_DB = -40  # FFmpegì´ 'ì¹¨ë¬µ'ìœ¼ë¡œ íŒë‹¨í•  ì†Œë¦¬ì˜ í¬ê¸° ê¸°ì¤€ì…ë‹ˆë‹¤. -40dBë³´ë‹¤ ì‘ì€ ì†Œë¦¬ëŠ” ì¹¨ë¬µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
MIN_SILENCE_DURATION_S = 0.3  # ìµœì†Œ 0.3ì´ˆ ì´ìƒ ì§€ì†ë˜ëŠ” ì¹¨ë¬µ êµ¬ê°„ë§Œ ì°¾ì•„ë‚´ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.


# 2. VAD ëª¨ë¸ ì„¸ë¶€ íŒŒë¼ë¯¸í„°
VAD_PARAMS = {
    "min_duration_off": 0.05,  # ìŒì„±ì´ ì—†ëŠ” êµ¬ê°„(ì¹¨ë¬µ)ì´ ìµœì†Œ 0.05ì´ˆëŠ” ë˜ì–´ì•¼ ì¹¨ë¬µìœ¼ë¡œ ì¸ì •í•©ë‹ˆë‹¤.
    "min_duration_on": 0.01,  # ìŒì„±ì´ ìˆëŠ” êµ¬ê°„ì´ ìµœì†Œ 0.01ì´ˆëŠ” ë˜ì–´ì•¼ ìŒì„±ìœ¼ë¡œ ì¸ì •í•©ë‹ˆë‹¤.
    "onset": 0.4,  # ìŒì„± ì‹œì‘ì´ë¼ê³  íŒë‹¨í•  í™•ë¥ ì˜ ì„ê³„ê°’ì…ë‹ˆë‹¤. (0~1 ì‚¬ì´, ë†’ì„ìˆ˜ë¡ ë³´ìˆ˜ì )
    "offset": 0.6  # ìŒì„± ì¢…ë£Œë¼ê³  íŒë‹¨í•  í™•ë¥ ì˜ ì„ê³„ê°’ì…ë‹ˆë‹¤. (0~1 ì‚¬ì´, ë†’ì„ìˆ˜ë¡ ë³´ìˆ˜ì )
}


# 4. ìµœì¢… SRT ìƒì„± ë° ë³‘í•© íŒŒë¼ë¯¸í„°
MIN_SEGMENT_DURATION = 0.1 # VADë¡œ ì°¾ì•„ë‚¸ ìŒì„± êµ¬ê°„ ì¤‘ 0.1ì´ˆë³´ë‹¤ ì§§ì€ êµ¬ê°„ì€ ë„ˆë¬´ ì§§ì€ ë…¸ì´ì¦ˆì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ë¬´ì‹œí•˜ê³  ì œê±°í•©ë‹ˆë‹¤.
MERGE_MAX_SECONDS = 15.0 # STTë¥¼ í•˜ê¸° ì „, ê°™ì€ ì–¸ì–´ì˜ ìŒì„± êµ¬ê°„ë“¤ì„ í•©ì¹  ë•Œ ìµœëŒ€ 15ì´ˆê¹Œì§€ë§Œ í•©ì¹˜ë„ë¡ ì œí•œí•©ë‹ˆë‹¤. ë„ˆë¬´ ê¸¸ë©´ STT ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# --- ìë§‰ ë³‘í•© ì„¤ì • ---
MERGE_THRESHOLD_SECONDS = 1.0  # 1ì°¨ ë³‘í•© ê¸°ì¤€: ìë§‰ì„ í•©ì¹  ê¸°ì¤€ ì‹œê°„ (ì´ˆ)
MAX_CHARS_PER_LINE = 30      # 1ì°¨ ë³‘í•© ê¸°ì¤€: í•©ì³ì§„ ìë§‰ì˜ ìµœëŒ€ ê¸€ì ìˆ˜
MIN_DURATION_SECONDS = 1.0   # 2ì°¨ ë³‘í•© ê¸°ì¤€: ì´ ì‹œê°„(ì´ˆ)ë³´ë‹¤ ì§§ì€ ìë§‰ì€ ì• ìë§‰ì— ê°•ì œë¡œ í•©ì¹¨


# --- í”„ë¡¬í”„íŠ¸ ë”•ì…”ë„ˆë¦¬ ---
en = "Today, we will discuss the importance of renewable energy. The quick brown fox jumps over the lazy dog."
# ja = "ì˜¤ëŠ˜ì€ ã¦å½¢ë‘ è¾æ›¸å½¢ë¥¼ ë³¼ ê±°ì•¼. ã¦å½¢ëŠ” ì—°ê²°Â·ë¶€íƒ(ã€œã¦ãã ã•ã„), è¾æ›¸å½¢ëŠ” ê¸°ë³¸í˜•. ï¼•ç•ªå‡ºå£ì—ì„œ ë§Œë‚˜. ì—˜ë¦¬ë² ì´í„°ëŠ” ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼, ê³„ë‹¨ì€ éšæ®µ."
ja = "ì˜¤ëŠ˜ì€ í…Œí˜•ë‘ ì§€ì‡¼í˜•ë¥¼ ë³¼ ê±°ì•¼. í…Œí˜•ëŠ” ì—°ê²°Â·ë¶€íƒ(~í…Œ ì¿ ë‹¤ì‚¬ì´), ì§€ì‡¼í˜•ëŠ” ê¸°ë³¸í˜•. ê³ ë°˜ ë°êµ¬ì¹˜ì—ì„œ ë§Œë‚˜. ì—˜ë¦¬ë² ì´í„°ëŠ” ì—ë ˆë² -íƒ€-, ê³„ë‹¨ì€ ì¹´ì´ë‹¨."
vi = "ì”¬ì§œì˜¤ ê¹œì–¸ ë˜ì´ ë“œì–µì¡°ì´ í¼ ë°˜ë¯¸ ì‘ì˜¨ ì•„ì‰ ì—  ìì˜¤ë¹„ì—” ë°”ì˜¤ë‹ˆì—ìš°"
es = "ì´ê²ƒì€ í•œêµ­ì–´ì™€ ìŠ¤í˜ì¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤í˜ì¸ì–´ ë¬¸ë²• ê°•ì˜ì…ë‹ˆë‹¤. Me llamo Juan. Â¿DÃ³nde estÃ¡ la biblioteca? Te quiero mucho."
idn = "ìŠ¬ë¼ë§› ë¹ ê¸° ëœ¨ë¦¬ë§ˆ ê¹Œì‹œ ì•„ë¹  ê¹Œë°”ë¥´ ì˜ë¥´ê¸° ì‹¸ì•¼ íŒ…ê°ˆ ë”” ì„œìš¸ ëœ¨ë¦¬ë§ˆ ê¹Œì‹œ ë°”ëƒ‘ ì‚¼ë¹ ì´ ì¤Œë¹  ë¼ê¸°"
zh = "ì˜¤ëŠ˜ì€ æŠŠå­—å¥ë‘ è¢«å­—å¥ë¥¼ ë¹„êµí•  ê±°ì•¼. æŠŠå­—å¥ëŠ” ì²˜ë¶„ ê°•ì¡°, è¢«å­—å¥ëŠ” í”¼ë™.ä¸‰å·å‡ºå£ì—ì„œ ë§Œë‚˜. íƒì‹œëŠ” æ‰“è½¦, ê°ˆì•„íƒ€ê¸°ëŠ” æ¢ä¹˜."
ko = ""
INSTRUCTOR_PROMPT_DICT = {
    'vi': vi,
    'es': es,
    'id': idn,
    'zh': zh,
    'en': en,
    'ja': ja,
    'ko': ko
}


# ========== Symlink ìš°íšŒ Patch (Windows í™˜ê²½ í˜¸í™˜ì„±) ==========
def force_copy(src, dst):
    if src is None or dst is None: return None
    src_path, dst_path = Path(src), Path(dst)
    try:
        dst_path.parent.mkdir(parents=True, exist_ok=True)
        if src_path.is_dir(): shutil.copytree(src_path, dst_path, dirs_exist_ok=True)
        else: shutil.copy2(src_path, dst_path)
        return dst
    except Exception as e:
        print(f"   [ê²½ê³ ] íŒŒì¼ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

import speechbrain.utils.fetching as sb_fetch
sb_fetch.link_with_strategy = lambda src, dst, strategy: force_copy(src, dst)

# ========== ëª¨ë¸ ë¡œë”© (ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰) ==========
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (VAD, Language ID, STT)")
# 1. VAD
vad_pipeline = Pipeline.from_pretrained("pyannote/voice-activity-detection", use_auth_token=HF_TOKEN)
vad_pipeline.to(torch.device(DEVICE))
vad_pipeline.instantiate(VAD_PARAMS)
print("âœ… VAD ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 2. Language ID
lang_id_model = EncoderClassifier.from_hparams(
    source="speechbrain/lang-id-voxlingua107-ecapa",
    savedir="tmp_lang_id"
)
print("âœ… Language ID ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 3. STT (Stable Whisper)
stt_model = stable_whisper.load_model(STT_MODEL_SIZE, device=DEVICE)
print(f"âœ… STT ëª¨ë¸({STT_MODEL_SIZE}) ë¡œë”© ì™„ë£Œ.")


# ========== í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ==========

def get_non_silent_segments_ffmpeg(audio_path):
    print("\nğŸ”Š 0. FFmpegë¡œ ì¹¨ë¬µ êµ¬ê°„ ë¶„ì„ ì‹œì‘...")
    command = [FFMPEG_PATH, '-i', str(audio_path), '-af', f'silencedetect=noise={SILENCE_THRESH_DB}dB:d={MIN_SILENCE_DURATION_S}', '-f', 'null', '-']
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
        ffmpeg_output = result.stderr
    except FileNotFoundError:
        print(f"\n[ì¹˜ëª…ì  ì˜¤ë¥˜] 'ffmpeg'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. FFMPEG_PATH ë³€ìˆ˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {FFMPEG_PATH}")
        return None
    except subprocess.CalledProcessError as e:
        print(f"\n[ì˜¤ë¥˜] FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e.stderr}")
        return None
    
    silence_starts = [float(t) for t in re.findall(r'silence_start: (\d+\.?\d*)', ffmpeg_output)]
    silence_ends = [float(t) for t in re.findall(r'silence_end: (\d+\.?\d*)', ffmpeg_output)]

    if not silence_starts:
        print("   [ì •ë³´] FFmpegê°€ ì¹¨ë¬µ êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
        return "full_audio"

    if len(silence_starts) > len(silence_ends):
        silence_starts = silence_starts[:len(silence_ends)]

    non_silent_segments = []
    last_end = 0.0
    for start, end in zip(silence_starts, silence_ends):
        if start > last_end + 0.01:
            non_silent_segments.append({'start': last_end, 'end': start})
        last_end = end

    try:
        duration = len(AudioSegment.from_file(audio_path)) / 1000.0
        if duration > last_end + 0.01:
            non_silent_segments.append({'start': last_end, 'end': duration})
    except Exception as e:
        print(f"   [ê²½ê³ ] ì˜¤ë””ì˜¤ ì „ì²´ ê¸¸ì´ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print(f"âœ… FFmpeg ë¶„ì„ ì™„ë£Œ. {len(non_silent_segments)}ê°œì˜ ìœ ì„± êµ¬ê°„ ë°œê²¬.")
    return non_silent_segments


def extract_segments_2stage(waveform, sample_rate, non_silent_segments):
    print("\nğŸš€ 1. 2ë‹¨ê³„ VAD ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì‹œì‘...")
    final_vad_annotation = Annotation()
    
    if non_silent_segments == "full_audio":
        print("   - ì „ì²´ ì˜¤ë””ì˜¤ì— ëŒ€í•´ VADë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return vad_pipeline({"waveform": waveform, "sample_rate": sample_rate})

    total_speech_chunks_found = 0
    for i, segment in enumerate(non_silent_segments):
        start, end = segment['start'], segment['end']
        start_frame, end_frame = int(start * sample_rate), int(end * sample_rate)
        chunk_waveform = waveform[:, start_frame:end_frame]
        file_chunk = {"waveform": chunk_waveform, "sample_rate": sample_rate}
        vad_result_chunk = vad_pipeline(file_chunk)
        
        for speech_turn, _, _ in vad_result_chunk.itertracks(yield_label=True):
            offset_speech_turn = Segment(speech_turn.start + start, speech_turn.end + start)
            final_vad_annotation[offset_speech_turn] = "speech"
            total_speech_chunks_found += 1
            
    print(f"âœ… 2ë‹¨ê³„ VAD ë¶„ì„ ì™„ë£Œ. ì´ {total_speech_chunks_found}ê°œì˜ ìŒì„± ì¡°ê° ë°œê²¬.")
    return final_vad_annotation


def detect_language_for_vad_segments(vad_annotation, waveform, sample_rate, lang_id_model):
    """
    pyannote VAD Annotation ê²°ê³¼ì™€ ì´ë¯¸ ë¡œë“œëœ waveformì„ ì‚¬ìš©í•´ ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    """
    print("\nğŸš€ VAD êµ¬ê°„ë³„ ì–¸ì–´ ê°ì§€ ì‹œì‘...")
    
    allowed_langs = {'ko', 'en', 'vi', 'es', 'zh', 'ja', 'id', 'unknown'}
    label_encoder = lang_id_model.hparams.label_encoder
    
    # 1. Annotation ê°ì²´ë¥¼ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    segments_with_lang = []
    for segment in vad_annotation.itersegments():
        segments_with_lang.append({'start': segment.start, 'end': segment.end})

    # 2. ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    for seg in segments_with_lang:
        # ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë‹¤ì‹œ ì½ëŠ” ëŒ€ì‹ , ë©”ëª¨ë¦¬ì— ìˆëŠ” waveformì—ì„œ ë°”ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
        start_sample = int(seg['start'] * sample_rate)
        end_sample = int(seg['end'] * sample_rate)
        segment_waveform = waveform[:, start_sample:end_sample]

        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´(0.5ì´ˆ ë¯¸ë§Œ) 'unknown'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        if segment_waveform.shape[1] < sample_rate * 0.5:
            seg['lang'] = 'unknown'
            continue
        
        # ì˜ë¼ë‚¸ ì˜¤ë””ì˜¤ ì¡°ê°ìœ¼ë¡œ ì–¸ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        prediction = lang_id_model.classify_batch(segment_waveform)
        
        # 1. ì¼ë‹¨ Top 1 ì–¸ì–´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        top_full_label = prediction[3][0]
        top_lang_code = top_full_label.split(':')[0].strip().lower()

        if top_lang_code in allowed_langs:
            # 2. Top 1ì´ í—ˆìš© ëª©ë¡ì— ìˆìœ¼ë©´, ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
            seg['lang'] = top_lang_code
        else:
            # 3. Top 1ì´ í—ˆìš© ëª©ë¡ì— ì—†ìœ¼ë©´, ì „ì²´ í™•ë¥ ì„ ë’¤ì ¸ë´…ë‹ˆë‹¤.
            print(f"    - [ì–¸ì–´ ì¬ì¡°ì •] Top 1 '{top_lang_code}'(ì´)ê°€ í—ˆìš© ëª©ë¡ì— ì—†ìŒ. '{list(allowed_langs)}' ë‚´ì—ì„œ ì¬ê²€ìƒ‰...")

            if (len(prediction) < 1 or
                    not isinstance(prediction[0], torch.Tensor) or
                    prediction[0].numel() == 0):
                print(f"    - [ê²½ê³ ] í™•ë¥  í…ì„œ ì—†ìŒ/ë¹„ì—ˆìŒ ({seg['start']:.2f}s~{seg['end']:.2f}s). 'unknown' ì²˜ë¦¬.")
                seg['lang'] = 'unknown'
                continue

            probabilities = prediction[0]

            allowed_probs = {}
            num_langs_to_check = min(len(probabilities), len(label_encoder.ind2lab))
            for i in range(num_langs_to_check):
                if i not in label_encoder.ind2lab: continue
                label_str = label_encoder.ind2lab[i]
                lang_code = label_str.split(':')[0].strip().lower()

                if lang_code in allowed_langs:
                    if i < len(probabilities):
                         prob = probabilities[i].item()
                         allowed_probs[lang_code] = prob

            if allowed_probs:
                final_lang = max(allowed_probs, key=allowed_probs.get)
                seg['lang'] = final_lang
            else:
                seg['lang'] = 'unknown'

    print("âœ… ì–¸ì–´ ê°ì§€ ì™„ë£Œ")
    return segments_with_lang


def merge_segments_by_language(segments, target_languages, max_gap=2.5, short_threshold=2.5):
    """ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì§€ëŠ¥ì ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤."""
    print("\nğŸš€ 3. ì–¸ì–´ë³„ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•© ì‹œì‘...")
    if not segments:
        return []

    processed_segments = segments.copy()
    
    #allowed_langs = {'ko', 'en', 'vi', 'es', 'zh', 'ja', 'id', 'unknown'}
    #for seg in processed_segments:
    #    if seg.get('lang') not in allowed_langs:
    #        print(f"  - [ì–¸ì–´ ì½”ë“œ ì •ë¦¬] {seg['start']:.2f}s êµ¬ê°„ì˜ ì–¸ì–´ '{seg['lang']}'ë¥¼ 'ko'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
    #        seg['lang'] = 'ko'
            
    if len(processed_segments) >= 3:
        for i in range(1, len(processed_segments) - 1):
            prev_seg, current_seg, next_seg = processed_segments[i-1], processed_segments[i], processed_segments[i+1]
            
            is_sandwiched = (prev_seg['lang'] == next_seg['lang']) and (current_seg['lang'] != prev_seg['lang'])
            is_short = (current_seg['end'] - current_seg['start']) <= short_threshold
            is_target_sandwich = prev_seg['lang'] in target_languages

            if is_sandwiched and is_short and is_target_sandwich:
                print(f"  - [ë³‘í•© ì‚¬ì „ ì²˜ë¦¬] {current_seg['start']:.2f}s êµ¬ê°„({current_seg['lang']})ì„ ì•ë’¤ ì–¸ì–´({prev_seg['lang']})ì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                current_seg['lang'] = prev_seg['lang']
    
    merged = []
    if not processed_segments:
        print("âœ… ë³‘í•©í•  ëŒ€ìƒ ì–¸ì–´ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
        
    current_seg = processed_segments[0].copy()

    for next_seg in processed_segments[1:]:
        gap = next_seg['start'] - current_seg['end']
        
        is_same_language = (next_seg['lang'] == current_seg['lang'])
        is_close_enough = gap <= max_gap
        is_current_target = current_seg['lang'] in target_languages
        is_next_absorbable = (next_seg['lang'] not in target_languages)

        if (is_same_language and is_close_enough) or (is_current_target and is_next_absorbable and is_close_enough): # 1. ì•ë’¤ ì–¸ì–´ê°€ ê°™ìœ¼ë©´ì„œ ë‘ ì„¸ê·¸ë¨¼íŠ¸ì˜ ê°­ì´ 2.5ì´ˆ ë¯¸ë§Œì´ê±°ë‚˜ 2. í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ê°€ íƒ€ê²Ÿì–¸ì–´ì´ë©´ì„œ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ íƒ€ê²Ÿì–¸ì–´ê°€ ì•„ë‹ˆê³  ê°­ì´ 2.5ì´ˆ ë¯¸ë§Œì¼ ê²½ìš°
            if is_current_target and is_next_absorbable: # 2ì˜ ê²½ìš°ë¼ë©´ ë³‘í•©
                print(f"  - [ë³‘í•©] {next_seg['start']:.2f}s êµ¬ê°„({next_seg['lang']})ì„ ì´ì „ êµ¬ê°„({current_seg['lang']})ì— í¡ìˆ˜í•©ë‹ˆë‹¤.")
            current_seg['end'] = next_seg['end']
        else: # 1ì˜ ê²½ìš°ë¼ë©´ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ merged ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            merged.append(current_seg)
            current_seg = next_seg.copy()

    merged.append(current_seg) # ë§¨ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ë„ merged ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

    # final_merged = [seg for seg in merged if seg['lang'] in target_languages]

    print(f"âœ… ë³‘í•© ì™„ë£Œ: ì´ {len(merged)}ê°œ êµ¬ê°„")
    for i, seg in enumerate(merged, 1):
        print(f"  - [{i:03}] {seg['start']:.2f}s ~ {seg['end']:.2f}s (ì–¸ì–´: {seg['lang']})")
    return merged


def merge_subtitle_objects(subs):
    """ë‹¨ì–´ ë‹¨ìœ„ ìë§‰(srt.Subtitle ê°ì²´ ë¦¬ìŠ¤íŠ¸)ì„ 2ë‹¨ê³„ì— ê±¸ì³ ë³‘í•©í•©ë‹ˆë‹¤."""
    if not subs:
        return []

    # --- 1ì°¨ ë³‘í•©: ë¬¸ì¥ ë° ê¸¸ì´ ê¸°ë°˜ ---
    print("  - [ìë§‰ ë³‘í•©] 1ì°¨ ë³‘í•©: ë¬¸ì¥ ë° ê¸¸ì´ ê·œì¹™ì— ë”°ë¼ ë³‘í•© ì¤‘...")
    pass1_subs = []
    current_sub = subs[0]

    for next_sub in subs[1:]:
        gap = (next_sub.start - current_sub.end).total_seconds()
        current_ends_sentence = current_sub.content.endswith('.') or current_sub.content.endswith('?')
        combined_text = current_sub.content + " " + next_sub.content
        
        should_merge = (
            gap <= MERGE_THRESHOLD_SECONDS and
            not current_ends_sentence and
            len(combined_text) <= MAX_CHARS_PER_LINE
        )

        if should_merge:
            current_sub.end = next_sub.end
            current_sub.content = combined_text
        else:
            pass1_subs.append(current_sub)
            current_sub = next_sub
            
    pass1_subs.append(current_sub)

    # --- 2ì°¨ ë³‘í•©: ì§€ë‚˜ì¹˜ê²Œ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ê°•ì œ ë³‘í•© ---
    print("  - [ìë§‰ ë³‘í•©] 2ì°¨ ë³‘í•©: ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë¦¬ ì¤‘...")
    if len(pass1_subs) < 2:
        return pass1_subs

    final_subs = [pass1_subs[0]]
    
    for i in range(1, len(pass1_subs)):
        current_sub_to_check = pass1_subs[i]
        duration = (current_sub_to_check.end - current_sub_to_check.start).total_seconds()
        
        previous_sub = final_subs[-1]
        
        if duration < MIN_DURATION_SECONDS:
            new_text = previous_sub.content + " " + current_sub_to_check.content
            
            if len(new_text) <= MAX_CHARS_PER_LINE * 1.5:
                previous_sub.end = current_sub_to_check.end
                previous_sub.content = new_text
                print(f"    - ì§§ì€ ìë§‰ ë³‘í•©: \"{current_sub_to_check.content}\"")
            else:
                final_subs.append(current_sub_to_check)
        else:
            final_subs.append(current_sub_to_check)
            
    return final_subs


def run_stt_and_save_srt(waveform, sample_rate, audio_path, segments, output_folder, instructor_prompt, done_path):
    """STT ìˆ˜í–‰ í›„, ë‹¨ì–´ ìë§‰ì„ ìƒì„±í•˜ê³  ì´ë¥¼ ë‹¤ì‹œ ë¬¸ì¥ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ìµœì¢… SRTë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\nğŸš€ 4. STT ë° ìë§‰ ë³‘í•© ì‹œì‘...")
    if stt_model is None:
        print("âŒ Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ STTë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        all_word_subs = []
        audio_waveform, sr = waveform, sample_rate

        for i, seg in enumerate(segments, 1):
            start_time, end_time = seg['start'], seg['end']
            lang = seg['lang'] if seg['lang'] != 'unknown' else None

            print(f"  - [{i}/{len(segments)}] STT êµ¬ê°„ ì²˜ë¦¬ ì¤‘: {start_time:.2f}s ~ {end_time:.2f}s (ì–¸ì–´: {lang or 'ìë™ ê°ì§€'})")

            start_sample, end_sample = int(start_time * sr), int(end_time * sr)
            segment_audio = audio_waveform[:, start_sample:end_sample][0]

            result = stt_model.transcribe(
                segment_audio,
                language=lang,
                initial_prompt=instructor_prompt if (lang == MAIN_LANGUAGE) else None,
                word_timestamps=True,
                condition_on_previous_text=False,
                temperature=0.0,
                fp16=True,
                task='transcribe'
            )

            offset = timedelta(seconds=start_time)
            
            for segment in result.segments:
                for word in segment.words:
                    start_ts = timedelta(seconds=word.start) + offset
                    end_ts = timedelta(seconds=word.end) + offset
                    content = word.word.strip()
                    if content:
                        all_word_subs.append(srt.Subtitle(index=0, start=start_ts, end=end_ts, content=content))
        
        if not all_word_subs:
            print("  - [ê²½ê³ ] STT ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ì—†ì–´ SRT íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # STTë¡œ ìƒì„±ëœ ë‹¨ì–´ ìë§‰ë“¤ì„ ë³‘í•©
        merged_subs = merge_subtitle_objects(all_word_subs)
        
        for idx, sub in enumerate(merged_subs, 1):
            sub.index = idx

        srt_content = srt.compose(merged_subs)
        base_filename = Path(audio_path).stem
        srt_filename = f"{base_filename}{FILENAME_SUFFIX}.srt"
        srt_filepath = Path(output_folder) / srt_filename

        with open(srt_filepath, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        print(f"âœ… ìµœì¢… ë³‘í•© SRT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {srt_filepath}")

    except Exception as e:
        print(f"âŒ STT/ë³‘í•© ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        if done_path:
            print("  - ì‘ì—… ì™„ë£Œ í›„ íŒŒì¼ ì´ë™ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            try:
                shutil.move(audio_path, done_path)
                print(f"âœ… ì›ë³¸ WAV íŒŒì¼ ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_path).name}")
            except shutil.Error as move_e:
                print(f"âŒ íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {move_e}")
            except Exception as e:
                 print(f"âŒ íŒŒì¼ ì´ë™ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print("âœ… ì‘ì—… ì™„ë£Œ. íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ìœ ì§€ë©ë‹ˆë‹¤.")



# ========== ë©”ì¸ ì‹¤í–‰ ë¡œì§ ==========
if __name__ == "__main__":
    root = tk.Tk()
    root.withdraw()

    if lang_id_model is not None and stt_model is not None:
        print("\nğŸµ ë¶„ì„í•  WAV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (16kHz, mono ê¶Œì¥, ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)")
        audio_files = filedialog.askopenfilenames(
            title="WAV íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
            filetypes=[("WAV Files", "*.wav")]
        )

        if not audio_files:
            print("âŒ íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            print(f"\nì´ {len(audio_files)}ê°œì˜ íŒŒì¼ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
            print("\nğŸ’¾ ê²°ê³¼ SRT íŒŒì¼ì„ ì €ì¥í•  í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            output_path = filedialog.askdirectory(title="SRT íŒŒì¼ì„ ì €ì¥í•  í´ë” ì„ íƒ")
            
            print("\nğŸ“‚ ì™„ë£Œëœ WAV íŒŒì¼ì„ ì´ë™ì‹œí‚¬ í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš” (ì·¨ì†Œ ì‹œ ì´ë™ ì•ˆ í•¨).")
            done_path = filedialog.askdirectory(title="ì™„ë£Œëœ WAV íŒŒì¼ì„ ì´ë™ì‹œí‚¬ í´ë” ì„ íƒ")

            if not output_path:
                print("âŒ ê²°ê³¼ ì €ì¥ í´ë”ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            else:
                if not done_path:
                    print("\nâš ï¸ 'ì™„ë£Œ' í´ë”ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì²˜ë¦¬ëœ íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ê·¸ëŒ€ë¡œ ë‚¨ìŠµë‹ˆë‹¤.")

                for i, audio_file in enumerate(list(audio_files), 1):
                    if not os.path.exists(audio_file):
                        print(f"âš ï¸ [{i}/{len(audio_files)}] íŒŒì¼ '{os.path.basename(audio_file)}'ê°€ ì´ë¯¸ ì´ë™ë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    print(f"\n{'='*60}")
                    print(f"â–¶ï¸  [{i}/{len(audio_files)}] íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(audio_file)}")
                    print(f"{'='*60}")

                    non_silent_segments = get_non_silent_segments_ffmpeg(audio_file)

                    if non_silent_segments: # FFmpeg ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚˜ë©´
                        print("\nğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ì¤‘...")
                        try:
                            audio_loader = Audio(sample_rate=16000, mono=True)
                            waveform, sample_rate = audio_loader(audio_file)
                            print("âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ì™„ë£Œ.")
                        except Exception as e:
                            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                            exit()
                    
                    vad_annotation = extract_segments_2stage(waveform, sample_rate, non_silent_segments)
                    segment_with_lang = detect_language_for_vad_segments(vad_annotation, waveform, sample_rate, lang_id_model)

                    sub_language = None if (SUB_LANGUAGE==None) else SUB_LANGUAGE
                    instructor_prompt = None
                    target_languages = [MAIN_LANGUAGE]
                    
                    lang_list = [seg['lang'] for seg in segment_with_lang if seg['lang'] not in [MAIN_LANGUAGE, 'unknown']]
                    
                    if sub_language == None:
                        if lang_list:
                            lang_counts = Counter(lang_list)
                            for lang, count in lang_counts.most_common():
                                if lang in INSTRUCTOR_PROMPT_DICT:
                                    sub_language = lang
                                    instructor_prompt = INSTRUCTOR_PROMPT_DICT.get(sub_language)
                                    target_languages.append(sub_language)
                                    break
                        if sub_language:
                            print(f"\nâœ… ë³´ì¡° ì–¸ì–´ ì„¤ì •: {sub_language.upper()}")
                        else:
                            print(f"\nâ„¹ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ ë³´ì¡° ì–¸ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì£¼ ì–¸ì–´({MAIN_LANGUAGE.upper()})ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                    else:
                        target_languages.append(sub_language)
                        instructor_prompt = INSTRUCTOR_PROMPT_DICT.get(sub_language)
                        print(f"\nâœ… ë³´ì¡° ì–¸ì–´ ê°•ì œ ì„¤ì •: {sub_language.upper()}")
                    
                    merged_segments = merge_segments_by_language(segment_with_lang, target_languages)

                    if merged_segments:
                        run_stt_and_save_srt(waveform, sample_rate, audio_file, merged_segments, output_path, instructor_prompt, done_path)
                    else:
                        print("âŒ STTë¥¼ ì§„í–‰í•  ìœ íš¨í•œ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
                        if done_path:
                            print("  - ì›ë³¸ íŒŒì¼ì„ 'ì™„ë£Œ' í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                            try:
                                shutil.move(audio_file, done_path)
                                print(f"  - íŒŒì¼ ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_file).name}")
                            except Exception as e:
                                print(f"  - íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        else:
                            print("âœ… ì‘ì—… ì™„ë£Œ. íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ìœ ì§€ë©ë‹ˆë‹¤.")


                print("\n\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ í•„ìˆ˜ ëª¨ë¸ ì¤‘ ì¼ë¶€ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")