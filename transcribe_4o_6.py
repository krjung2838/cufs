import os
import re
import math
import shutil
import subprocess
import traceback
from pathlib import Path
from datetime import timedelta
from collections import Counter
from dataclasses import dataclass
from typing import List, Optional, Tuple, Any

import tkinter as tk
from tkinter import filedialog

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import soundfile as sf
from pydub import AudioSegment

import srt
from pyannote.audio import Pipeline, Audio
from pyannote.core import Annotation, Segment
from speechbrain.inference import EncoderClassifier, SpeakerRecognition
from inaSpeechSegmenter import Segmenter

from openai import OpenAI


# ============================================================
# 0) ì‚¬ìš©ì ì„¤ì • íŒŒë¼ë¯¸í„°
# ============================================================

PUNCT_ATTACH_TO_PREV = r""".,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šâ€¦"""
CLOSERS_ATTACH_TO_PREV = r""")\]\}ã€‰ã€‹ã€ã€ã€‘"â€â€™'"""

# --- (A) í† í°/í‚¤ ---
HF_TOKEN = os.getenv("HF_TOKEN")  # ì˜ˆ: hf_xxx
# OpenAI API KeyëŠ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì‚¬ìš© (OpenAI SDKê°€ ìë™ ì¸ì‹)

# --- (B) ë””ë°”ì´ìŠ¤ ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- (C) ì™¸ë¶€ ë„êµ¬ ---
FFMPEG_PATH = r"C:\Users\cufs\Desktop\ì—…ë¬´\subtitle\ffmpeg\ffmpeg.exe"  # ffmpeg.exe ì „ì²´ ê²½ë¡œ

# --- (D) ì–¸ì–´/íŒŒì¼ëª… ---
FILENAME_SUFFIX = ""     # ìµœì¢… SRT íŒŒì¼ëª… ì ‘ë¯¸ì‚¬
MAIN_LANGUAGE = "ko"
SUB_LANGUAGE = None      # None: ìë™ / "no_sub": ë³´ì¡°ì–¸ì–´ ì—†ìŒ / 'en','vi' ë“±: ê³ ì •
THIRD_LANG = "no_third"  # "no_third": ì œ3ì–¸ì–´ ì—†ìŒ / ê·¸ ì™¸: ë¡œì§ ìˆ˜í–‰
ALLOWED_LANGS = ['ko', 'en', 'vi', 'es', 'zh', 'ja', 'id']

# --- (E) FFmpeg ì¹¨ë¬µ íƒì§€ íŒŒë¼ë¯¸í„° ---
SILENCE_THRESH_DB = -50
MIN_SILENCE_DURATION_S = 0.05

# --- (F) VAD íŒŒë¼ë¯¸í„° ---
VAD_PARAMS = {
    "min_duration_off": 0.01,
    "min_duration_on": 0.05,
    "onset": 0.01,
    "offset": 0.01
}

MAX_DURATION = 1
MAX_GAP = 0.5
MAX_MERGED_DURATION = 600

# ìµœì¢… SRT ìƒì„± ë° ë³‘í•© íŒŒë¼ë¯¸í„°
MIN_SEGMENT_DURATION = 0.1
MERGE_MAX_SECONDS = 15.0

# --- (G) ìŒì•…/ë…¸ì´ì¦ˆ í•„í„° ---
MUSIC_SIM_THRESHOLD = 0.4
MUSIC_MAX_ITER = 2
SANDWICH_MAX_DURATION = 1.0

# --- (H) OpenAI diarize STT ì„¤ì • (ì—¬ê¸°ë§Œ ê±´ë“œë¦¬ë©´ ë¨) ---
OPENAI_MODEL = "gpt-4o-transcribe-diarize"
SUBTITLE_MAX_CHARS = 35          # âœ… ìµœëŒ€ ê¸€ììˆ˜ (ê¸°ë³¸ 35)
SUBTITLE_ONE_LINE = True         # âœ… í•­ìƒ 1ì¤„
SUBTITLE_MIN_CUE_DUR = 0.25      # ë„ˆë¬´ ì§§ì€ cue ìµœì†Œ ê¸¸ì´ (ì´ˆ)
OPENAI_PAD_SECONDS = 0.0        # ì„¸ê·¸ë¨¼íŠ¸ ì•ë’¤ ì—¬ìœ 
OPENAI_MAX_CHUNK_SECONDS = 600.0  # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ë‚´ë¶€ì—ì„œ ì´ ê¸¸ì´ë¡œ ì˜ë¼ì„œ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ

INCLUDE_SPEAKER_PREFIX = False   # ì›í•˜ë©´ [SPEAKER_00] ê°™ì€ prefix ë¶™ì´ê¸°

# --- í”„ë¡¬í”„íŠ¸ ë”•ì…”ë„ˆë¦¬(ìœ ì§€: diarize ëª¨ë¸ì€ prompt ì•ˆ ë°›ì§€ë§Œ, ë„¤ ë¡œì§ êµ¬ì¡°ìƒ ë‚¨ê²¨ë‘ ) ---
en = "Today, we will discuss the importance of renewable energy. The quick brown fox jumps over the lazy dog."
ja = "í…Œí˜• ë’¤ì— ì´ë£¨ë¥¼ ë¶™ì´ë©´ ì§„í–‰í˜•ì´ ë¼. ì‹œí…Œ ì´ë£¨ëŠ” 'í•˜ê³  ìˆë‹¤'ë¼ëŠ” ëœ»ì´ì•¼. ì˜¤ìŠ¤ìŠ¤ë©” ë©”ë‰´ê°€ ë­ì˜ˆìš”? ë‚˜ë§ˆë¹„ë£¨ ë‘ ì” ì£¼ì„¸ìš”. ì‚¬ì´í›„ë¥¼ ìƒì–´ë²„ë ¤ì„œ ì¼€ì´ì‚¬ì¸ ì— ì‹ ê³ í–ˆì–´."
vi = "ì”¬ì§œì˜¤ ê¹œì–¸ ë˜ì´ ë“œì–µì¡°ì´ í¼ ë°˜ë¯¸ ì‘ì˜¨ ì•„ì‰ ì—  ìì˜¤ë¹„ì—” ë°”ì˜¤ë‹ˆì—ìš°"
es = "ì´ê²ƒì€ í•œêµ­ì–´ì™€ ìŠ¤í˜ì¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤í˜ì¸ì–´ ë¬¸ë²• ê°•ì˜ì…ë‹ˆë‹¤. Me llamo Juan. Â¿DÃ³nde estÃ¡ la biblioteca? Te quiero mucho."
idn = "ìŠ¬ë¼ë§› ë¹ ê¸° ëœ¨ë¦¬ë§ˆ ê¹Œì‹œ ì•„ë¹  ê¹Œë°”ë¥´ ì˜ë¥´ê¸° ì‹¸ì•¼ íŒ…ê°ˆ ë”” ì„œìš¸ ëœ¨ë¦¬ë§ˆ ê¹Œì‹œ ë°”ëƒ‘ ì‚¼ë¹ ì´ ì¤Œë¹  ë¼ê¸°"
zh = "ì˜¤ëŠ˜ì€ æŠŠå­—å¥ë‘ è¢«å­—å¥ë¥¼ ë¹„êµí•  ê±°ì•¼. æŠŠå­—å¥ëŠ” ì²˜ë¶„ ê°•ì¡°, è¢«å­—å¥ëŠ” í”¼ë™.ä¸‰å·å‡ºå£ì—ì„œ ë§Œë‚˜. íƒì‹œëŠ” æ‰“è½¦, ê°ˆì•„íƒ€ê¸°ëŠ” æ¢ä¹˜."
ko = ""
INSTRUCTOR_PROMPT_DICT = {'vi': vi, 'es': es, 'id': idn, 'zh': zh, 'en': en, 'ja': ja, 'ko': ko}


# ============================================================
# 1) Symlink ìš°íšŒ Patch (SpeechBrain Windows í˜¸í™˜)
# ============================================================

def force_copy(src, dst):
    if src is None or dst is None:
        return None
    src_path, dst_path = Path(src), Path(dst)
    try:
        dst_path.parent.mkdir(parents=True, exist_ok=True)
        if src_path.is_dir():
            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)
        else:
            shutil.copy2(src_path, dst_path)
        return dst
    except Exception as e:
        print(f"   [ê²½ê³ ] íŒŒì¼ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

import speechbrain.utils.fetching as sb_fetch
sb_fetch.link_with_strategy = lambda src, dst, strategy: force_copy(src, dst)


# ============================================================
# 2) ëª¨ë¸ ë¡œë”© (ì‹œì‘ ì‹œ 1íšŒ)
# ============================================================

if not HF_TOKEN:
    raise RuntimeError("HF_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ìˆìŒ. HF_TOKENì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ì¤˜.")

print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (VAD, Language ID, Speaker Verification, Music Segmenter)")
# 1) VAD
vad_pipeline = Pipeline.from_pretrained("pyannote/voice-activity-detection", use_auth_token=HF_TOKEN)
vad_pipeline.to(torch.device(DEVICE))
vad_pipeline.instantiate(VAD_PARAMS)
print("âœ… VAD ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 2) Language ID
lang_id_model = EncoderClassifier.from_hparams(
    source="speechbrain/lang-id-voxlingua107-ecapa",
    savedir="tmp_lang_id"
)
print("âœ… Language ID ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 3) Speaker Verification (ìŒì•…/ë…¸ì´ì¦ˆ ê²€ì¶œìš© ì„ë² ë”©)
verification_model = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    savedir="tmp_speaker_verification",
    run_opts={"device": DEVICE}
)
print("âœ… Speaker Verification ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 4) Music/Speech Segmenter
music_segmenter = Segmenter(vad_engine="smn", detect_gender=False)
print("âœ… Music/Speech ì„¸ê·¸ë¨¼í„° ë¡œë”© ì™„ë£Œ.")

# 5) OpenAI Client
client = OpenAI()
print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")


# ============================================================
# 3) ìœ í‹¸ í•¨ìˆ˜ë“¤ (ì—‘ì…€/FFmpeg/ìŒì•…/VAD/ì–¸ì–´ê°ì§€/ë³‘í•©)
# ============================================================

def read_xlsx_and_create_dict(xlsx_file_path):
    """ê°•ì˜ëª…ê³¼ ë³´ì¡°ì–¸ì–´ ë§¤ì¹­ ì—‘ì…€ -> dict ìƒì„±"""
    df = pd.read_excel(io=xlsx_file_path, header=3, usecols="C:D")
    df = df.dropna(subset=['ë³´ì¡°ì–¸ì–´'])
    lang_map = df.set_index('ê°•ì˜ëª…')['ë³´ì¡°ì–¸ì–´'].to_dict()
    keys_view = list(lang_map.keys())
    sorted_list = sorted(keys_view, key=len, reverse=True)
    return lang_map, sorted_list


def get_non_silent_segments_ffmpeg(audio_path):
    print("\nğŸ”Š 0. FFmpegë¡œ ì¹¨ë¬µ êµ¬ê°„ ë¶„ì„ ì‹œì‘...")
    command = [
        FFMPEG_PATH, '-i', str(audio_path),
        '-af', f'silencedetect=noise={SILENCE_THRESH_DB}dB:d={MIN_SILENCE_DURATION_S}',
        '-f', 'null', '-'
    ]
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
        ffmpeg_output = result.stderr
    except FileNotFoundError:
        print(f"\n[ì¹˜ëª…ì  ì˜¤ë¥˜] ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. FFMPEG_PATH í™•ì¸: {FFMPEG_PATH}")
        return None
    except subprocess.CalledProcessError as e:
        print(f"\n[ì˜¤ë¥˜] FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e.stderr}")
        return None

    silence_starts = [float(t) for t in re.findall(r'silence_start: (\d+\.?\d*)', ffmpeg_output)]
    silence_ends = [float(t) for t in re.findall(r'silence_end: (\d+\.?\d*)', ffmpeg_output)]

    if not silence_starts:
        print("   [ì •ë³´] FFmpegê°€ ì¹¨ë¬µ êµ¬ê°„ì„ ì°¾ì§€ ëª»í•¨. ì „ì²´ íŒŒì¼ ë¶„ì„.")
        return "full_audio"

    if len(silence_starts) > len(silence_ends):
        silence_starts = silence_starts[:len(silence_ends)]

    non_silent_segments = []
    last_end = 0.0
    for start, end in zip(silence_starts, silence_ends):
        if start > last_end + 0.01:
            non_silent_segments.append({'start': last_end, 'end': start})
        last_end = end

    try:
        duration = len(AudioSegment.from_file(audio_path)) / 1000.0
        if duration > last_end + 0.01:
            non_silent_segments.append({'start': last_end, 'end': duration})
    except Exception as e:
        print(f"   [ê²½ê³ ] ì „ì²´ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨: {e}")

    print(f"âœ… FFmpeg ë¶„ì„ ì™„ë£Œ. {len(non_silent_segments)}ê°œì˜ ìœ ì„± êµ¬ê°„.")
    return non_silent_segments


def detect_music_segments(audio_path):
    print("\nğŸ¼ 0-1. inaSpeechSegmenterë¡œ ìŒì•…/ìŒì„± êµ¬ê°„ ë¶„ì„...")
    try:
        raw_segments = music_segmenter(str(audio_path))
        dict_segments = []
        for label, start, end in raw_segments:
            dict_segments.append({'label': label, 'start': float(start), 'end': float(end)})
        return dict_segments
    except Exception as e:
        print(f"   [ê²½ê³ ] ìŒì•… êµ¬ê°„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return []


def build_music_blocks(ina_segments, short_speech_max=1.0):
    if not ina_segments:
        return []

    blocks = []
    cur_start, cur_end = None, None

    for item in ina_segments:
        label = item['label']
        start = item['start']
        end = item['end']
        dur = end - start

        if label == "music":
            if cur_start is None:
                cur_start = start
            cur_end = end
        else:
            if cur_start is not None and label in ("speech", "noise") and dur <= short_speech_max:
                cur_end = end
            else:
                if cur_start is not None:
                    blocks.append((cur_start, cur_end))
                    cur_start, cur_end = None, None

    if cur_start is not None:
        blocks.append((cur_start, cur_end))

    if not blocks:
        return []

    blocks = sorted(blocks)
    merged = []
    for start, end in blocks:
        if not merged:
            merged.append([start, end])
        else:
            ls, le = merged[-1]
            if start <= le + 0.2:
                merged[-1][1] = max(le, end)
            else:
                merged.append([start, end])

    music_blocks = [(s, e) for s, e in merged]
    print(f"   - ë³‘í•©ëœ ìŒì•… ë¸”ë¡: {len(music_blocks)}ê°œ")
    return music_blocks


def remove_music_from_non_silent(non_silent_segments, music_blocks, min_len=0.05):
    if not music_blocks:
        return non_silent_segments

    if not non_silent_segments or non_silent_segments == "full_audio":
        return non_silent_segments

    cleaned = []
    for seg in non_silent_segments:
        seg_start = float(seg["start"])
        seg_end = float(seg["end"])
        parts = [(seg_start, seg_end)]

        for m_start, m_end in music_blocks:
            new_parts = []
            for p_start, p_end in parts:
                if p_end <= m_start or p_start >= m_end:
                    new_parts.append((p_start, p_end))
                    continue
                if p_start < m_start:
                    new_parts.append((p_start, m_start))
                if p_end > m_end:
                    new_parts.append((m_end, p_end))
            parts = new_parts
            if not parts:
                break

        for p_start, p_end in parts:
            if p_end - p_start >= min_len:
                cleaned.append({"start": p_start, "end": p_end})

    print(f"   - ìŒì•… ì œê±° ì „ ìœ ì„±: {len(non_silent_segments)}ê°œ â†’ ì œê±° í›„: {len(cleaned)}ê°œ")
    return cleaned


def extract_segments_2stage(waveform, sample_rate, non_silent_segments):
    print("\nğŸš€ 1. 2ë‹¨ê³„ VAD ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ...")
    final_vad_annotation = Annotation()

    if non_silent_segments == "full_audio":
        print("   - ì „ì²´ ì˜¤ë””ì˜¤ VAD ì‹¤í–‰")
        return vad_pipeline({"waveform": waveform, "sample_rate": sample_rate})

    total_speech_chunks_found = 0
    skipped_chunks = 0
    MIN_CHUNK_SAMPLES = int(sample_rate * 0.06)

    for segment in non_silent_segments:
        start, end = segment['start'], segment['end']
        start_frame, end_frame = int(start * sample_rate), int(end * sample_rate)
        if end_frame > waveform.shape[1]:
            end_frame = waveform.shape[1]
        chunk_waveform = waveform[:, start_frame:end_frame]

        if chunk_waveform.shape[1] < MIN_CHUNK_SAMPLES:
            skipped_chunks += 1
            continue

        file_chunk = {"waveform": chunk_waveform, "sample_rate": sample_rate}
        try:
            vad_result_chunk = vad_pipeline(file_chunk)
            for speech_turn, _, _ in vad_result_chunk.itertracks(yield_label=True):
                offset_speech_turn = Segment(speech_turn.start + start, speech_turn.end + start)
                final_vad_annotation[offset_speech_turn] = "speech"
                total_speech_chunks_found += 1
        except Exception as e:
            print(f"   [ê²½ê³ ] VAD ì²˜ë¦¬ ìŠ¤í‚µ ({start:.2f}~{end:.2f}s): {e}")
            continue

    merged_annotation = Annotation()
    for segment in final_vad_annotation.support().itersegments():
        merged_annotation[segment] = "speech"

    print(f"âœ… VAD ì™„ë£Œ. ìŒì„± ì¡°ê° {total_speech_chunks_found}ê°œ (ë„ˆë¬´ ì§§ì•„ ìƒëµ {skipped_chunks}ê°œ)")
    return merged_annotation


def detect_language_for_vad_segments(vad_annotation, waveform, sample_rate, lang_id_model):
    print("\nğŸš€ VAD êµ¬ê°„ë³„ ì–¸ì–´ ê°ì§€ ì‹œì‘ (0.1ì´ˆ ë¯¸ë§Œ ì‚¬ì „ ì œê±°)...")
    label_encoder = lang_id_model.hparams.label_encoder

    segments_with_lang = []
    skipped_short_count = 0

    for segment in vad_annotation.itersegments():
        duration = segment.end - segment.start
        if duration < 0.1:
            skipped_short_count += 1
            continue
        segments_with_lang.append({'start': segment.start, 'end': segment.end})

    print(f"   - âœ‚ï¸ 0.1ì´ˆ ë¯¸ë§Œ ì œê±°: {skipped_short_count}ê°œ")

    for seg in segments_with_lang:
        start_sample = int(seg['start'] * sample_rate)
        end_sample = int(seg['end'] * sample_rate)
        segment_waveform = waveform[:, start_sample:end_sample]

        if segment_waveform.shape[1] < sample_rate * 0.5:
            seg['lang'] = 'ko'
            continue

        prediction = lang_id_model.classify_batch(segment_waveform)
        top_full_label = prediction[3][0]
        top_lang_code = top_full_label.split(':')[0].strip().lower()

        if top_lang_code in ALLOWED_LANGS:
            seg['lang'] = top_lang_code
        else:
            if (len(prediction) < 1 or not isinstance(prediction[0], torch.Tensor) or prediction[0].numel() == 0):
                seg['lang'] = 'ko'
                continue

            probabilities = prediction[0]
            allowed_probs = {}
            num_langs_to_check = min(len(probabilities), len(label_encoder.ind2lab))
            for i in range(num_langs_to_check):
                if i not in label_encoder.ind2lab:
                    continue
                label_str = label_encoder.ind2lab[i]
                lang_code = label_str.split(':')[0].strip().lower()
                if lang_code in ALLOWED_LANGS:
                    if i < len(probabilities):
                        allowed_probs[lang_code] = probabilities[i].item()

            seg['lang'] = max(allowed_probs, key=allowed_probs.get) if allowed_probs else 'ko'

    print("âœ… ì–¸ì–´ ê°ì§€ ì™„ë£Œ")
    return segments_with_lang


def tag_noise_by_music_blacklist_iterative(vad_segments, ina_segments, waveform, sample_rate,
                                          verification_model, threshold=0.4, max_iterations=2):
    print(f"\nğŸ¼ [Iterative Blacklist] ìŒì•… ì œê±° ì‹œì‘ (ìµœëŒ€ {max_iterations}íšŒ)...")

    if not vad_segments:
        return []

    seg_list = vad_segments if isinstance(vad_segments, list) else [{'start': s.start, 'end': s.end} for s in vad_segments.itersegments()]
    total_len = waveform.shape[1]

    music_embeddings_pool = []
    for item in ina_segments:
        if item['label'] != 'music':
            continue
        start = item['start']
        end = item['end']
        curr = start
        while curr < end:
            chunk_end = min(curr + 5.0, end)
            if chunk_end - curr < 1.0:
                break
            s_sample = int(curr * sample_rate)
            e_sample = int(chunk_end * sample_rate)
            try:
                emb = verification_model.encode_batch(waveform[:, s_sample:e_sample]).flatten()
                music_embeddings_pool.append(emb)
            except:
                pass
            curr += 5.0

    if not music_embeddings_pool:
        print("   âš ï¸ ì´ˆê¸° ìŒì•… êµ¬ê°„ ì—†ìŒ. í•„í„°ë§ ì¤‘ë‹¨.")
        return seg_list

    for it in range(max_iterations):
        print(f"   ğŸ”„ Round {it+1} (í‘œë³¸ {len(music_embeddings_pool)}ê°œ)")
        music_centroid = torch.mean(torch.stack(music_embeddings_pool), dim=0)
        tagged = 0

        for seg in seg_list:
            if seg.get('audio_type') in ['noise_or_music', 'noise_short', 'noise_music']:
                continue

            start = seg['start']
            end = seg['end']
            duration = end - start
            if duration < 0.1:
                continue

            s_sample = int(start * sample_rate)
            e_sample = int(end * sample_rate)
            if e_sample > total_len:
                e_sample = total_len

            try:
                curr_emb = verification_model.encode_batch(waveform[:, s_sample:e_sample]).flatten()
                score = F.cosine_similarity(music_centroid, curr_emb, dim=0).item()
                if score >= threshold:
                    seg['audio_type'] = 'noise_music'
                    seg['music_sim'] = f"{score:.2f}"
                    music_embeddings_pool.append(curr_emb)
                    tagged += 1
                else:
                    if 'audio_type' not in seg:
                        seg['audio_type'] = 'speech'
            except:
                pass

        print(f"     ğŸ‘‰ Round {it+1}: ì¶”ê°€ ìŒì•… {tagged}ê°œ")
        if tagged == 0:
            print("     âœ… ë” ì´ìƒ ìƒˆ ìŒì•… ì—†ìŒ. ì¢…ë£Œ.")
            break

    total_music = sum(1 for s in seg_list if s.get('audio_type') == 'noise_music')
    print(f"âœ… ìŒì•… ë¶„ë¥˜ ì™„ë£Œ. ì´ ìŒì•… {total_music}ê°œ")
    return seg_list


def apply_sandwich_smoothing(segments, max_duration=1.0):
    print(f"\nğŸ¥ª ìƒŒë“œìœ„ì¹˜ ê·œì¹™ ì ìš© (ê¸°ì¤€ {max_duration}s ì´í•˜)...")
    if len(segments) < 3:
        return segments

    changed = 0
    for i in range(1, len(segments) - 1):
        prev_seg = segments[i - 1]
        curr_seg = segments[i]
        next_seg = segments[i + 1]

        dur = curr_seg['end'] - curr_seg['start']
        if dur > max_duration:
            continue

        prev_type = prev_seg.get('audio_type', 'speech')
        curr_type = curr_seg.get('audio_type', 'speech')
        next_type = next_seg.get('audio_type', 'speech')

        if curr_type == 'speech' and prev_type == 'noise_music' and next_type == 'noise_music':
            curr_seg['audio_type'] = 'noise_music'
            curr_seg['change_log'] = 'Sandwich Correction (Speech->Music)'
            changed += 1
        elif curr_type == 'noise_music' and prev_type == 'speech' and next_type == 'speech':
            curr_seg['audio_type'] = 'speech'
            curr_seg['change_log'] = 'Sandwich Correction (Music->Speech)'
            changed += 1

    print(f"âœ… ìƒŒë“œìœ„ì¹˜ ë³´ì • ì™„ë£Œ. ìˆ˜ì • {changed}ê°œ")
    return segments


def select_sub_language(audio_file, lang_map, sorted_list, segment_with_lang):
    filename = re.sub(r'\s+|_', "", Path(audio_file).stem)
    filename = re.sub(r'0(\d)ì£¼ì°¨', r'\1ì£¼ì°¨', filename)

    sub_lang = None
    for prefix in sorted_list:
        if filename.startswith(prefix):
            sub_lang = lang_map[prefix]
            print(f'ë³´ì¡°ì–¸ì–´ë¥¼ {sub_lang}ìœ¼ë¡œ ì„¤ì •.')
            break

    if sub_lang is None:
        print("ì—‘ì…€ì— ê°•ì˜ëª… ì—†ìŒ â†’ ì£¼ì–¸ì–´ ë‹¤ìŒìœ¼ë¡œ ë§ì´ ë“±ì¥í•œ ì–¸ì–´ë¡œ ë³´ì¡°ì–¸ì–´ ìë™ ì„¤ì •.")
        lang_list = [seg['lang'] for seg in segment_with_lang if seg['lang'] not in [MAIN_LANGUAGE, 'unknown']]
        if lang_list:
            sub_lang = Counter(lang_list).most_common(1)[0][0]
            print(f'ë³´ì¡°ì–¸ì–´ë¥¼ {sub_lang}ìœ¼ë¡œ ì„¤ì •.')

    return sub_lang


def define_third_language(segment_with_lang, target_languages):
    print('\nì œ3ì–¸ì–´ ì„¤ì •ì„ ìœ„í•´ VAD ë¶„ì„...')
    set_to_remove = set(['unknown']) | set(target_languages)
    allowed_langs = set(ALLOWED_LANGS) - set_to_remove

    lang_durations = {}
    for segment in segment_with_lang:
        if segment['lang'] in allowed_langs:
            lang = segment['lang']
            dur = segment['end'] - segment['start']
            lang_durations[lang] = lang_durations.get(lang, 0.0) + dur

    if lang_durations:
        third_lang = max(lang_durations, key=lang_durations.get)
        if lang_durations[third_lang] < 60:
            third_lang = None
            print('ì œ3ì–¸ì–´ í›„ë³´ê°€ 60ì´ˆ ë¯¸ë§Œ â†’ ì œ3ì–¸ì–´ ì—†ìŒ')
        print(f'ì œ3ì–¸ì–´: {third_lang}')
    else:
        third_lang = None
        print('ì œ3ì–¸ì–´ ì—†ìŒ')

    return third_lang


def convert_to_unknown(third_lang, segment_with_lang, target_languages):
    set_to_remove = set(['unknown']) | set(target_languages)

    if third_lang is None or third_lang == 'no_sub':
        for segment in segment_with_lang:
            if segment['lang'] not in set_to_remove:
                segment['lang'] = 'unknown'
    else:
        set_to_remove = set_to_remove | {third_lang}
        for segment in segment_with_lang:
            if segment['lang'] not in set_to_remove:
                segment['lang'] = 'unknown'

    return segment_with_lang


def merge_unknown(segment_with_lang):
    print('unknown VADë¥¼ ë°”ë¡œ ì§ì „ VADì— í¡ìˆ˜...')
    
    if not segment_with_lang:
        return []
    
    if segment_with_lang[0]['lang'] == 'unknown':
        segment_with_lang[0]['lang'] = 'ko'
    
    merged = []
    merged.append(segment_with_lang[0])
    for seg in segment_with_lang[1:]:
        if seg['lang'] == 'unknown':
            merged[-1]['end'] = seg['end']
        else:
            merged.append(seg)
    return merged


def duration_up_and_down(segment, MAX_DURATION):
    duration = segment['end'] - segment['start']
    return "down" if duration < MAX_DURATION else "up"


def gap_up_and_down(previous, segment, MAX_GAP):
    gap = segment['start'] - previous['end']
    return "down" if gap < MAX_GAP else "up"


def merge_vad(merge_list, first, case, MAX_DURATION, MAX_MERGED_DURATION):
    work_list = []
    temp_list = []
    final_list = []

    if len(merge_list) != 1:
        for i, seg in enumerate(merge_list):
            work_list.append(seg)
            merged_duration = seg['end'] - work_list[0]['start']

            if merged_duration > MAX_MERGED_DURATION:
                if i == len(merge_list) - 2 and duration_up_and_down(merge_list[i+1], MAX_DURATION) == 'down':
                    chunk_1 = {'start': work_list[0]['start'], 'end': merge_list[i+1]['end'], 'lang': case['chunk_1_lang']}
                    temp_list.append(chunk_1)
                    break

                elif len(work_list) == 1:
                    final_list.append(seg)
                    work_list = []

                elif i == len(merge_list) - 1:
                    if duration_up_and_down(merge_list[i], MAX_DURATION) == 'down':
                        chunk_1 = {'start': work_list[0]['start'], 'end': work_list[-1]['end'], 'lang': case['chunk_1_lang']}
                        temp_list.append(chunk_1)
                    else:
                        chunk_1 = {'start': work_list[0]['start'], 'end': work_list[-2]['end'], 'lang': case['chunk_1_lang']}
                        final_list.append(chunk_1)
                        temp_list.append(seg)

                else:
                    chunk_1 = {'start': work_list[0]['start'], 'end': work_list[-2]['end'], 'lang': case['chunk_1_lang']}
                    final_list.append(chunk_1)
                    work_list = [seg]

            else:
                if i == len(merge_list) - 1:
                    chunk_1 = {'start': work_list[0]['start'], 'end': work_list[-1]['end'], 'lang': case['chunk_1_lang']}
                    temp_list.append(chunk_1)
                else:
                    continue

    else:
        final_list = [{'start': merge_list[0]['start'], 'end': merge_list[0]['end'], 'lang': case['chunk_1_lang']}]
        temp_list = [{'start': first['start'], 'end': first['end'], 'lang': case['chunk_2_lang']}]

    return final_list, temp_list


# ============================================================
# 4) final_merge_VAD_by_lang (âœ… ë„¤ ì½”ë“œ ê·¸ëŒ€ë¡œ. ê±´ë“œë¦¬ë©´ final_segment_3 ì˜ë¯¸ ê¹¨ì§)
# ============================================================
def final_merge_VAD_by_lang(segment_with_lang, sub, third, MAX_DURATION, MAX_GAP, MAX_MERGED_DURATION):
    """ì •ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡ì„ ë°›ì•„ ì •í•´ì§„ ê·œì¹™ì— ë”°ë¼ ë³‘í•©í•©ë‹ˆë‹¤."""
    
    if not segment_with_lang:
        return []
    

    case_1 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_2 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':sub, 'temp':'chunk_1'}
    case_3 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':third, 'temp':'chunk_1'}
    case_4 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_5 = {'chunk_1_seg':['temp'], 'chunk_1_lang':'ko', 'chunk_2_seg':['first'], 'chunk_2_lang':'ko', 'final':'chunk_1', 'temp':'chunk_2'}
    case_6 = {'chunk_1_seg':['temp'], 'chunk_1_lang':'ko', 'chunk_2_seg':['first'], 'chunk_2_lang':third, 'final':'chunk_1', 'temp':'chunk_2'}
    case_7 = {'chunk_1_seg':['temp'], 'chunk_1_lang':'ko', 'chunk_2_seg':['first'], 'chunk_2_lang':sub, 'final':'chunk_1', 'temp':'chunk_2'}
    case_8 = {'chunk_1_seg':['temp'], 'chunk_1_lang':sub, 'chunk_2_seg':['first'], 'chunk_2_lang':'ko', 'final':'chunk_1', 'temp':'chunk_2'}
    case_9 = {'chunk_1_seg':['temp'], 'chunk_1_lang':sub, 'chunk_2_seg':['first'], 'chunk_2_lang':third, 'final':'chunk_1', 'temp':'chunk_2'}
    case_10 = {'chunk_1_seg':['temp'], 'chunk_1_lang':sub, 'chunk_2_seg':['first'], 'chunk_2_lang':sub, 'final':'chunk_1', 'temp':'chunk_2'}
    case_11 = {'chunk_1_seg':['temp'], 'chunk_1_lang':third, 'chunk_2_seg':['first'], 'chunk_2_lang':'ko', 'final':'chunk_1', 'temp':'chunk_2'}
    case_12 = {'chunk_1_seg':['temp'], 'chunk_1_lang':third, 'chunk_2_seg':['first'], 'chunk_2_lang':third, 'final':'chunk_1', 'temp':'chunk_2'}
    case_13 = {'chunk_1_seg':['temp'], 'chunk_1_lang':third, 'chunk_2_seg':['first'], 'chunk_2_lang':sub, 'final':'chunk_1', 'temp':'chunk_2'}
    case_14 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_15 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':third, 'temp':'chunk_1'}
    case_16 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':sub, 'temp':'chunk_1'}
    case_17 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_18 = {'chunk_1_seg':['temp', 'first', 'second', 'third'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_19 = {'chunk_1_seg':['temp', 'first', 'second', 'third'], 'chunk_1_lang':third, 'temp':'chunk_1'}
    case_20 = {'chunk_1_seg':['temp', 'first', 'second', 'third'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}


    all_case = [
        {'temp_lang':'ko', 'first_gap':'down', 'first_lang':'ko', 'case':case_1}, # 1
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_1}, # 2
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 3
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 4
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_1}, # 5
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 6
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_1}, # 7
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_1}, # 8
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'down', 'case':case_1}, # 9
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'up', 'case':case_1}, # 10
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_1}, # 11
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_1}, # 12
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 13
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'down', 'case':case_1}, # 14
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'up', 'case':case_1}, # 15
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_lang':sub, 'case':case_2}, # 16
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'up', 'case':case_2}, # 17
        {'temp_lang':sub, 'first_gap':'down', 'first_lang':sub, 'case':case_2}, # 18
        {'temp_lang':sub, 'temp_dur':'up', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_2}, # 19
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'third_lang':sub, 'case':case_2}, # 20
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'case':case_2}, # 21
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_2}, # 22
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'up', 'second_gap':'down', 'case':case_2}, # 23
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'up', 'second_gap':'up', 'case':case_2}, # 24
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_3}, # 25
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'up', 'case':case_3}, # 26
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_3}, # 27
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'up', 'case':case_3}, # 28
        {'temp_lang':third, 'first_gap':'down', 'first_lang':third, 'case':case_3}, # 29
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_3}, # 30
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_3}, # 31
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'second_dur':'up', 'case':case_4}, # 32
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'case':case_4}, # 33
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_4}, # 34
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':sub, 'case':case_4}, # 35
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':third, 'case':case_4}, # 36
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_4}, # 37
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_4}, # 38
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_4}, # 39
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_4}, # 40
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_4}, # 41
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'case':case_4}, # 42
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_4}, # 43
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'up', 'case':case_4}, # 44
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_4}, # 45
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_4}, # 46
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'case':case_4}, # 47
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_4}, # 48
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_4}, # 49
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':'ko', 'case':case_4}, # 50
        {'temp_lang':'ko', 'first_gap':'up', 'first_lang':'ko', 'case':case_5}, #51
        {'temp_lang':'ko', 'first_gap':'up', 'first_lang':third, 'case':case_6}, #52
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'case':case_6}, #53
        {'temp_lang':'ko', 'first_gap':'up', 'first_lang':sub, 'case':case_7}, #54
        {'temp_lang':'ko', 'first_gap':'down', 'first_lang':sub, 'first_dur':'up', 'case':case_7}, #55
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'case':case_7}, #56
        {'temp_lang':sub,  'first_gap':'up', 'first_lang':'ko', 'case':case_8}, #57
        {'temp_lang':sub, 'temp_dur':'up', 'first_gap':'down', 'first_lang':'ko', 'case':case_8}, #58
        {'temp_lang':sub, 'first_gap':'up', 'first_lang':third, 'case':case_9}, #59
        {'temp_lang':sub, 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'case':case_9}, #60
        {'temp_lang':sub, 'first_gap':'up', 'first_lang':sub, 'case':case_10}, #61
        {'temp_lang':third, 'first_gap':'up', 'first_lang':'ko', 'case':case_11}, #62
        {'temp_lang':third, 'temp_dur':'up', 'first_gap':'down', 'first_lang':'ko', 'case':case_11}, #63
        {'temp_lang':third, 'first_gap':'up', 'first_lang':third, 'case':case_12}, #64
        {'temp_lang':third, 'first_gap':'up', 'first_lang':sub, 'case':case_13}, #65
        {'temp_lang':third, 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'case':case_13}, #66
        {'temp_lang':'ko', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_14}, #67
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_15}, #68
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_15}, #69
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_15}, #70
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_16}, #71
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'up', 'third_lang':third, 'case':case_16}, #72
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_16}, #73
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_16}, #74
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'up', 'case':case_17}, #75
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'second_dur':'down', 'case':case_17}, #76
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'up', 'case':case_17}, #77
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'up', 'case':case_17}, #78
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_17}, #79
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'up', 'third_lang':'ko', 'case':case_17}, #80
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'up', 'case':case_17}, #81
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'case':case_17}, #82
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':sub, 'case':case_17}, #83
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':third, 'case':case_17}, #84
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'up', 'case':case_17}, #85
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_18}, #86
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_18}, #87
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'up', 'third_gap':'down', 'third_lang':'ko', 'case':case_18}, #88
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_19}, #89
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'down', 'case':case_20}, #90
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'down', 'case':case_20}, #91
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'down', 'case':case_20}, #92
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'down', 'case':case_20} #93
    ]
   


    sorted_all_case = sorted(all_case, key=len, reverse=True)

    final_segment = [] # ë³‘í•©ì„ ì™„ë£Œí•œ ê²°ê³¼ë¬¼ë“¤ì„ ëª¨ì•„ë†“ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    temp_list = [] # ë³‘í•©ì‘ì—…ì„ ì‹¤í–‰í•  ê³µê°„ì„ ìƒì„±í•©ë‹ˆë‹¤.

   
    temp_list.append(segment_with_lang[0]) # ì²« ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì„ì‹œë¦¬ìŠ¤íŠ¸ì— ë°”ë¡œ ë„£ìŠµë‹ˆë‹¤.
    work_list = segment_with_lang[1:5] # case íŒë‹¨ì„ ìœ„í•œ ì‘ì—…ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    next_list = segment_with_lang[5:] # ë³‘í•©ì‘ì—…ì„ ì§„í–‰í•˜ë©° í•­ëª©ì´ ë¹ ì ¸ë‚˜ê°€ëŠ” ì‘ì—…ë¦¬ìŠ¤íŠ¸ì— ë¹ ì ¸ë‚˜ê°„ë§Œí¼ ë‹¤ìŒ í•­ëª©ì„ ì¶”ê°€í•  ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.

    while work_list: # ìˆœí™˜ì¢…ë£Œ ê¸°ì¤€ì€ ì‘ì—…ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        # âš¡ [ë³´ìŠ¤ì˜ ì˜ë¬¸ì„ í•´ê²°í•˜ëŠ” ì½”ë“œ]
        # merge_vadê°€ tempë¥¼ ë¹„ì›Œì„œ ë³´ëƒˆë‹¤ëŠ” ê±´, ì´ì „ ë©ì–´ë¦¬ê°€ ì™„ê²°ë‚¬ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
        # ê·¸ëŸ¬ë¯€ë¡œ ëŒ€ê¸°ì—´(work_list)ì˜ ì²« ë²ˆì§¸ íƒ€ìë¥¼ ìƒˆë¡œìš´ ê¸°ì¤€ì (temp)ìœ¼ë¡œ ì„¸ì›Œì•¼ í•©ë‹ˆë‹¤.
        if not temp_list:
            if work_list:
                # ëŒ€ê¸°ì—´ì—ì„œ í•˜ë‚˜ êº¼ë‚´ì„œ tempë¡œ ìŠ¹ê²©
                temp_list.append(work_list.pop(0))
                
                # work_listê°€ í•˜ë‚˜ ì¤„ì—ˆìœ¼ë‹ˆ next_listì—ì„œ í•˜ë‚˜ ì¶©ì „
                if next_list:
                    work_list.append(next_list.pop(0))
                
                # ê¸°ì¤€ì ì´ ìƒˆë¡œ ìƒê²¼ìœ¼ë‹ˆ ë‹¤ì‹œ ë£¨í”„ ì‹œì‘ (Case íŒë‹¨)
                continue
            
        if len(work_list) >= 4: # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì˜ ê¸°ë³¸ì ì¸ í•­ëª© ìˆ˜ëŠ” 4ê°œì…ë‹ˆë‹¤.
            temp = temp_list[0] # ì„ì‹œë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í•­ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            first = work_list[0]
            second = work_list[1]
            third = work_list[2] # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í•­ëª©ì„ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

            sequence = {'temp':temp, 'first':first, 'second':second, 'third':third}
            # ì¸ë±ì‹±ì„ ìœ„í•œ ê° ì„¸ê·¸ë¨¼íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
          

            temp_lang = temp['lang']
            temp_dur = duration_up_and_down(temp, MAX_DURATION)
            first_gap = gap_up_and_down(temp, first, MAX_GAP)

            first_lang = first['lang']
            first_dur = duration_up_and_down(first, MAX_DURATION)
            second_gap = gap_up_and_down(first, second, MAX_GAP)

            second_lang = second['lang']
            second_dur = duration_up_and_down(second, MAX_DURATION)
            third_gap = gap_up_and_down(second, third, MAX_GAP)

            third_lang = third['lang']
            third_dur = duration_up_and_down(third, MAX_DURATION)


            temp_dict = {'temp_lang':temp_lang, 'temp_dur':temp_dur, 'first_gap':first_gap, 'first_lang':first_lang, 'first_dur':first_dur, 'second_gap':second_gap, 'second_lang':second_lang, 'second_dur':second_dur, 'third_gap':third_gap, 'third_lang':third_lang, 'third_dur':third_dur}
            # ì°¸ê³ í•  ëª¨ë“  ê°’ë“¤ì„ ì €ì¥í•  ì„ì‹œ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
       

            work_case = {} # ìµœì¢…ì ìœ¼ë¡œ all_case ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ëŠ” keyë¥¼ ê°€ì§„ caseë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            selected_case = None # ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ caseë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¹ˆ ë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            for case in sorted_all_case: # 93ê°€ì§€ì˜ caseë¥¼ ê°€ì¥ keyë¥¼ ë§ì´ ê°€ì§„ caseë¶€í„° í•˜ë‚˜ì”© ì‚´í´ë´…ë‹ˆë‹¤.
                key_list = list(case.keys()) # í•´ë‹¹ caseê°€ ê°€ì§„ keyë“¤ë§Œ ë½‘ì•„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
                key_list.remove('case') # work_caseì—ëŠ” 'case' keyê°€ ì—†ê¸° ë•Œë¬¸ì— 'case' keyë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                for key in key_list: # ì—¬ê¸°ì— ì¡´ì¬í•˜ëŠ” key ëª…ì¹­ì„ í•˜ë‚˜ì”© ê°€ì ¸ì™€
                    work_case[key] = temp_dict[key] # ì°¸ê³ í•  ëª¨ë“  ê°’ë“¤ì„ ì €ì¥í•œ ì„ì‹œ ë”•ì…”ë„ˆë¦¬ ë‚´ì—ì„œ í•´ë‹¹ key, valueë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
                temp_case = case.copy() # work_caseì—ëŠ” 'case' keyê°€ ì—†ê¸° ë•Œë¬¸ì— ë³¸ caseë¥¼ ë³µì‚¬í•˜ì—¬ ì„ì‹œ caseë¥¼ ë§Œë“­ë‹ˆë‹¤.
                del temp_case['case'] # work_caseì™€ ì¼ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ 'case' keyë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                if work_case == temp_case: # ì´ ë‘˜ì´ ì¼ì¹˜í•˜ë©´
                    selected_case = case['case'] # ë³¸ caseì˜ 'case' key ê°’ì„ ê°€ì ¸ì™€ selected_caseì— í• ë‹¹í•©ë‹ˆë‹¤.
                    break
                else : 
                    work_case = {}

            if selected_case == None:
                print(temp_dict)
                print(key_list)
                print(temp_case)
                print(work_case)
       

            merge_list = [sequence[name] for name in selected_case['chunk_1_seg']] # íŒë‹¨ëœ caseì— í•´ë‹¹í•˜ëŠ” ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            long_segments, temp_segment = merge_vad(merge_list, first, selected_case, MAX_DURATION, MAX_MERGED_DURATION) # ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ ê¸¸ì´ ì´í•˜ê°€ ë˜ë„ë¡ ë³‘í•©í•˜ì—¬ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì™€ ì„ì‹œë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            final_segment += long_segments # ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ ë³‘í•© ì‘ì—… ì¤‘ ë‚˜ì˜¨ ìµœëŒ€ ê¸¸ì´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¯¸ë¦¬ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            temp_list = temp_segment # ì„ì‹œë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ ë³‘í•© ì‘ì—… ì¤‘ ë‚˜ì˜¨ ì„ì‹œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ìµœì‹ í™”í•©ë‹ˆë‹¤.
            del_count = len(selected_case['chunk_1_seg']) - 1 # ë³‘í•©ì‘ì—… í›„ ì‘ì—…ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¹ ì ¸ë‚˜ê°€ëŠ” í•­ëª© ê°œìˆ˜ì…ë‹ˆë‹¤.
            if del_count == 0: # 'chunk_1_seg'ê°€ 1ê°œì¸ ê²½ìš°ëŠ” 'chunk_2_seg'ê°€ 1ê°œì¸ ê²½ìš°ë°–ì— ì—†ê¸° ë•Œë¬¸ì— 0ì´ ë˜ëŠ” ìˆœê°„ 1ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
                del_count = 1
            work_list.extend(next_list[:del_count]) # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì˜ ë’¤ì— ë‹¤ìŒ í•­ëª©ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            del work_list[:del_count]
            del next_list[:del_count] # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì™€ ë‹¤ìŒë¦¬ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ëœ ë§Œí¼ì˜ í•­ëª© ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.


        else: # ë³‘í•©ì‘ì—…ì„ ì§„í–‰í•˜ë‹¤ ìµœí›„ì— ì‘ì—…ë¦¬ìŠ¤íŠ¸ê°€ 4ê°œ ë¯¸ë§Œì´ ëœë‹¤ë©´
            final_segment = final_segment + temp_list + work_list # ì„ì‹œë¦¬ìŠ¤íŠ¸ì™€ ì‘ì—…ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•˜ì§€ ì•Šê³  ì „ë¶€ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            work_list = [] # ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ while ë¬¸ì„ ë¹ ì ¸ë‚˜ê°€ê¸° ìœ„í•œ ì¡°ê±´ì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.

    return final_segment


def redetect_language_for_merged_segments(merged_segments, waveform, sample_rate, lang_id_model, sub_lang, third_lang):
    print("\nğŸ” [Re-detection] ë³‘í•©ëœ êµ¬ê°„ ì–¸ì–´ ì¬ê°ì§€ (íƒ€ê²Ÿ ì–¸ì–´ í•œì •)...")
    label_encoder = lang_id_model.hparams.label_encoder
    changed_count = 0

    target_langs = {'ko'}
    if sub_lang:
        target_langs.add(sub_lang)
    if third_lang is not None:
        target_langs.add(third_lang)

    if isinstance(waveform, np.ndarray):
        waveform = torch.from_numpy(waveform)
    if DEVICE == "cuda":
        waveform = waveform.to(DEVICE)

    for seg in merged_segments:
        start = seg['start']
        end = seg['end']
        old_lang = seg['lang']

        start_sample = int(start * sample_rate)
        end_sample = int(end * sample_rate)
        segment_waveform = waveform[:, start_sample:end_sample]

        if segment_waveform.shape[1] < sample_rate * 0.1:
            continue

        try:
            prediction = lang_id_model.classify_batch(segment_waveform)
        except:
            continue

        top_full_label = prediction[3][0]
        top_lang_code = top_full_label.split(':')[0].strip().lower()

        final_new_lang = old_lang
        if top_lang_code in target_langs:
            final_new_lang = top_lang_code
        else:
            probabilities = prediction[0].squeeze()
            allowed_probs = {}
            num_check = min(len(probabilities), len(label_encoder.ind2lab))
            for idx in range(num_check):
                if idx not in label_encoder.ind2lab:
                    continue
                label_str = label_encoder.ind2lab[idx]
                lang_code = label_str.split(':')[0].strip().lower()
                if lang_code in target_langs:
                    allowed_probs[lang_code] = probabilities[idx].item()

            if allowed_probs:
                final_new_lang = max(allowed_probs, key=allowed_probs.get)

        if final_new_lang != old_lang:
            seg['lang'] = final_new_lang
            seg['change_log'] = seg.get('change_log', '') + f" | Re-detected ({old_lang}->{final_new_lang})"
            changed_count += 1

    print(f"âœ… ì¬ê°ì§€ ì™„ë£Œ. ìˆ˜ì • {changed_count}ê°œ")
    return merged_segments


# ============================================================
# 5) OpenAI diarize STT + 35ì 1ì¤„ SRT ìƒì„± (âœ… ìƒˆë¡œ ì¶”ê°€ëœ í•µì‹¬)
# ============================================================

@dataclass
class Cue:
    start: float
    end: float
    text: str

def srt_ts(t: float) -> str:
    """ì´ˆ(float)ë¥¼ SRT ì‹œê°„ í¬ë§·(00:00:00,000)ìœ¼ë¡œ ë³€í™˜"""
    if t < 0:
        t = 0.0
    ms = int(round(t * 1000.0))
    hh = ms // 3_600_000
    ms -= hh * 3_600_000
    mm = ms // 60_000
    ms -= mm * 60_000
    ss = ms // 1000
    ms -= ss * 1000
    return f"{hh:02d}:{mm:02d}:{ss:02d},{ms:03d}"

def normalize_one_line(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”: ì¤„ë°”ê¿ˆ ì œê±°, ê³µë°± ì •ë¦¬, êµ¬ë‘ì  ì• ê³µë°± ì œê±°"""
    text = text.replace("\n", " ").replace("\r", " ")
    text = re.sub(r"\s+", " ", text).strip()

    # êµ¬ë‘ì /ë‹«ëŠ”ê¸°í˜¸ ì• ê³µë°± ì œê±°: "word ." -> "word."
    text = re.sub(rf"\s+([{re.escape(PUNCT_ATTACH_TO_PREV)}])", r"\1", text)
    text = re.sub(rf"\s+([{re.escape(CLOSERS_ATTACH_TO_PREV)}])", r"\1", text)
    return text

# -----------------------------------------------------------------------------
# 3. í•µì‹¬ ë¡œì§: í…ìŠ¤íŠ¸ ê· í˜• ìë¥´ê¸° (ë³´ìŠ¤ ì•„ì´ë””ì–´ ì ìš©)
# -----------------------------------------------------------------------------

def split_text_max_chars(text: str, max_chars: int) -> List[str]:
    """
    [ì—…ê·¸ë ˆì´ë“œ ë²„ì „] 
    í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœíˆ ì•ì—ì„œë¶€í„° ìë¥´ëŠ” ê²Œ ì•„ë‹ˆë¼, 
    ì „ì²´ ê¸¸ì´ë¥¼ ê³ ë ¤í•´ 'ê· í˜• ìˆê²Œ(Në“±ë¶„)' ë‚˜ëˆ•ë‹ˆë‹¤.
    """
    text = normalize_one_line(text)
    if not text:
        return []

    chunks: List[str] = []
    s = text

    # ë‹¤ìŒ chunkê°€ ë¬¸ì¥ë¶€í˜¸ë¡œ ì‹œì‘í•˜ë©´ ì•ì¤„ë¡œ ë‹¹ê¸°ëŠ” ì •ê·œì‹
    leading_attach_re = re.compile(
        rf"^[{re.escape(PUNCT_ATTACH_TO_PREV + CLOSERS_ATTACH_TO_PREV)}]+"
    )
    # ìë¥¼ í›„ë³´(ê³µë°±, êµ¬ë‘ì ) ì°¾ëŠ” ì •ê·œì‹
    split_re = re.compile(rf"[ \t]+|[{re.escape(PUNCT_ATTACH_TO_PREV)}]")

    while len(s) > max_chars:
        # 1. ì•ìœ¼ë¡œ ëª‡ ì¤„ì´ í•„ìš”í•œì§€ ê³„ì‚° (ì˜¬ë¦¼)
        lines_needed = math.ceil(len(s) / max_chars)
        
        # 2. ì´ë²ˆ ì¤„ì˜ 'ëª©í‘œ ê¸¸ì´' ì„¤ì • (ê· í˜•ì  ì°¾ê¸°)
        target_len = int(len(s) / lines_needed)

        # íƒìƒ‰ ë²”ìœ„: ìµœëŒ€ ê¸¸ì´(max_chars)ë¥¼ ë„˜ì„ ìˆœ ì—†ìŒ
        window = s[: max_chars + 1]

        best_cut = None
        min_diff = float('inf') 

        # ìë¥¼ í›„ë³´ë“¤ ì¤‘ target_lenì— ê°€ì¥ ê°€ê¹Œìš´ ê³³ ì„ íƒ
        for m in split_re.finditer(window):
            if m.group(0).isspace():
                cand = m.start()
            else:
                cand = m.end()  # êµ¬ë‘ì ì€ í¬í•¨

            diff = abs(cand - target_len)
            
            # ë” ê°€ê¹ê±°ë‚˜, ê±°ë¦¬ê°€ ê°™ë‹¤ë©´ ìµœëŒ€í•œ ë’¤ìª½ì„ ì„ íƒ
            if diff < min_diff:
                min_diff = diff
                best_cut = cand
            elif diff == min_diff:
                best_cut = max(best_cut if best_cut else 0, cand)

        # ìë¥¼ ê³³ì„ ëª» ì°¾ì•˜ê±°ë‚˜, ë„ˆë¬´ ì•ìª½ì´ë©´ ê°•ì œë¡œ max_charsì—ì„œ ìë¦„
        if best_cut is None or best_cut <= 0:
            best_cut = max_chars

        part = s[:best_cut].rstrip()
        rest = s[best_cut:].lstrip()

        # ë‚¨ì€ ë’·ë¶€ë¶„ì´ ë¬¸ì¥ë¶€í˜¸ë¡œ ì‹œì‘í•˜ë©´ ì•ì¤„ë¡œ ë‹¹ê²¨ì˜¤ê¸°
        while rest:
            mm = leading_attach_re.match(rest)
            if not mm:
                break
            part += mm.group(0)
            rest = rest[len(mm.group(0)) :].lstrip()

        if part:
            chunks.append(part)
        s = rest
        if not s:
            break

    if s:
        chunks.append(s)

    return chunks

def openai_diarize_segments(wav_path: str, language: Optional[str] = None) -> List[dict]:
    # (ì‹¤ì œ client ê°ì²´ëŠ” ì™¸ë¶€ì— ìˆë‹¤ê³  ê°€ì •)
    # from your_module import client 
    
    with open(wav_path, "rb") as f:
        kwargs = dict(
            model=OPENAI_MODEL,
            file=f,
            response_format="diarized_json", # í™”ì ë¶„ë¦¬ í¬ë§· ìš”ì²­
            chunking_strategy="auto",
        )
        if language:
            kwargs["language"] = language

        # client í˜¸ì¶œ ë¶€ë¶„ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        try:
            out = client.audio.transcriptions.create(**kwargs)
        except Exception as e:
            print(f"âŒ API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            return []

    # ê²°ê³¼ ì •ê·œí™” ë¡œì§ (segments ì¶”ì¶œ)
    if hasattr(out, "segments") and out.segments is not None:
        segs = list(out.segments)
    elif isinstance(out, dict) and "segments" in out:
        segs = out["segments"]
    elif hasattr(out, "model_dump"):
        d = out.model_dump()
        segs = d.get("segments", [])
    else:
        segs = []

    norm: List[dict] = []
    for s in segs:
        if isinstance(s, dict):
            norm.append(s)
            continue
        if hasattr(s, "model_dump"):
            norm.append(s.model_dump())
            continue
        norm.append({
            "start": getattr(s, "start", 0.0),
            "end": getattr(s, "end", getattr(s, "start", 0.0)),
            "speaker": getattr(s, "speaker", None),
            "text": getattr(s, "text", "") or "",
        })
    return norm

def diarize_segments_to_cues(
    diar_segs: List[dict],
    offset_sec: float,
    max_chars: int = 35,
    min_cue_dur: float = 0.25,
    include_speaker_prefix: bool = True, # âœ… True ê¸°ë³¸ê°’ (í™”ì í‘œì‹œ)
) -> List[Cue]:
    """
    OpenAI ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ìë§‰(Cue) ê°ì²´ë¡œ ë³€í™˜.
    - í…ìŠ¤íŠ¸ëŠ” ê· í˜• ìˆê²Œ ìë¦„
    - ì‹œê°„ ë°°ë¶„ì€ 'í™”ì íƒœê·¸ ì œì™¸' ìˆœìˆ˜ í…ìŠ¤íŠ¸ ê¸°ì¤€ (ì‹±í¬ ì •í™•ë„)
    - í™”ì íƒœê·¸ëŠ” ì²« ë©ì–´ë¦¬ì—ë§Œ ë¶€ì°©
    """
    cues: List[Cue] = []
    
    for seg in diar_segs:
        start = float(seg.get("start", 0.0))
        end = float(seg.get("end", start))
        speaker = seg.get("speaker")
        text = seg.get("text", "") or ""
        text = normalize_one_line(text)

        if end <= start or not text:
            continue

        # 1. 'ìˆœìˆ˜ í…ìŠ¤íŠ¸' ê¸°ì¤€ìœ¼ë¡œ ê· í˜• ìˆê²Œ ìë¥´ê¸°
        chunks = split_text_max_chars(text, max_chars=max_chars)
        if not chunks:
            continue

        dur = end - start
        
        # 2. ì‹œê°„ ë°°ë¶„ (í™”ì íƒœê·¸ ì—†ì´ ìˆœìˆ˜ ê¸€ì ìˆ˜ë¡œ ê³„ì‚° -> ì‹±í¬ ì •í™•)
        lengths = [max(1, len(c)) for c in chunks]
        total = sum(lengths)

        cum = 0
        for i, (c, ln) in enumerate(zip(chunks, lengths)):
            c0 = cum / total
            cum += ln
            c1 = cum / total

            cs = start + dur * c0
            ce = start + dur * c1
            
            if ce - cs < min_cue_dur:
                ce = min(end, cs + min_cue_dur)
            
            # ë§ˆì§€ë§‰ ì¡°ê°ì€ ì„¸ê·¸ë¨¼íŠ¸ ëì‹œê°„ì— ë”± ë§ì¶¤
            if i == len(chunks) - 1:
                ce = end

            # 3. ìë§‰ í…ìŠ¤íŠ¸ ì™„ì„± (í™”ì íƒœê·¸ ë¶™ì´ê¸°)
            final_text = c
            
            # âœ… í™”ìê°€ ë°”ë€ŒëŠ” 'ì²« ë²ˆì§¸ ë©ì–´ë¦¬'ì—ë§Œ ì´ë¦„í‘œ ë¶™ì´ê¸°
            if include_speaker_prefix and speaker is not None and i == 0:
                final_text = f"[{speaker}] {c}"
            
            cues.append(Cue(start=cs + offset_sec, end=ce + offset_sec, text=final_text))

    # ìœ íš¨ì„± ê²€ì‚¬
    cues = [c for c in cues if c.end > c.start and c.text.strip()]
    return cues

def cues_to_srt(cues: List[Cue]) -> str:
    lines = []
    for idx, c in enumerate(cues, 1):
        lines.append(str(idx))
        lines.append(f"{srt_ts(c.start)} --> {srt_ts(c.end)}")
        lines.append(c.text)
        lines.append("")
    return "\n".join(lines)

def extract_temp_wav_from_waveform(
    waveform: torch.Tensor,
    sample_rate: int,
    start_sec: float,
    end_sec: float,
    pad_sec: float,
    temp_dir: Path,
    temp_name: str,
) -> Tuple[str, float, float]:
    temp_dir.mkdir(parents=True, exist_ok=True)

    start_sec2 = max(0.0, start_sec - pad_sec)
    end_sec2 = max(start_sec2, end_sec + pad_sec)

    start_sample = int(start_sec2 * sample_rate)
    end_sample = int(end_sec2 * sample_rate)
    if end_sample > waveform.shape[1]:
        end_sample = waveform.shape[1]

    seg = waveform[:, start_sample:end_sample].squeeze().detach().cpu().numpy()
    if seg.ndim > 1:
        seg = seg.flatten()

    # ì–‘ìí™” ë° ì €ì¥
    seg = np.clip(seg, -1.0, 1.0)
    seg_i16 = (seg * 32767).astype(np.int16)
    seg_clean = seg_i16.astype(np.float32) / 32767.0

    wav_path = str(temp_dir / temp_name)
    sf.write(wav_path, seg_clean, sample_rate)
    return wav_path, start_sec2, end_sec2

def run_openai_diarize_and_save_srt(
    waveform: torch.Tensor,
    sample_rate: int,
    audio_path: str,
    final_segments: List[dict],
    output_folder: str,
    done_path: str,
):
    print("\nğŸš€ OpenAI diarize STT ì‹œì‘ (35ì/ê· í˜•ë¶„í• /í™”ìí‘œì‹œ)...")

    base_filename = Path(audio_path).stem
    srt_filename = f"{base_filename}{FILENAME_SUFFIX}.srt"
    output_srt_path = str(Path(output_folder) / srt_filename)

    temp_dir = Path(output_folder) / "_tmp_openai_segments"
    all_cues: List[Cue] = []

    try:
        for i, seg in enumerate(final_segments, 1):
            seg_start = float(seg["start"])
            seg_end = float(seg["end"])
            seg_lang = seg.get("lang", None)

            if seg_end <= seg_start:
                continue

            seg_dur = seg_end - seg_start
            print(f"  - [{i}/{len(final_segments)}] {seg_start:.2f}s~{seg_end:.2f}s ({seg_lang}), dur={seg_dur:.2f}s")

            # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ chunking
            cursor = seg_start
            chunk_idx = 0
            while cursor < seg_end - 0.01:
                chunk_idx += 1
                chunk_start = cursor
                chunk_end = min(seg_end, cursor + OPENAI_MAX_CHUNK_SECONDS)
                cursor = chunk_end

                temp_wav_path, chunk_offset, _ = extract_temp_wav_from_waveform(
                    waveform=waveform,
                    sample_rate=sample_rate,
                    start_sec=chunk_start,
                    end_sec=chunk_end,
                    pad_sec=OPENAI_PAD_SECONDS,
                    temp_dir=temp_dir,
                    temp_name=f"{base_filename}_seg{i:04d}_chunk{chunk_idx:02d}.wav",
                )

                # API í˜¸ì¶œ (ì—¬ê¸°ì„œ í™”ì ë¶„ë¦¬ëœ ì •ë³´ íšë“)
                diar_segs = openai_diarize_segments(temp_wav_path, language=seg_lang)

                # ìë§‰ ë³€í™˜ (âœ… INCLUDE_SPEAKER_PREFIX=True ë°˜ì˜ë¨)
                cues = diarize_segments_to_cues(
                    diar_segs=diar_segs,
                    offset_sec=chunk_offset,
                    max_chars=SUBTITLE_MAX_CHARS,
                    min_cue_dur=SUBTITLE_MIN_CUE_DUR,
                    include_speaker_prefix=INCLUDE_SPEAKER_PREFIX, # í™”ì í‘œì‹œ ì¼œê¸°
                )

                # íŒ¨ë”© ë²”ìœ„ ë°– í´ë¦¬í•‘
                clipped = []
                for c in cues:
                    cs = max(c.start, chunk_start)
                    ce = min(c.end, chunk_end)
                    if ce > cs:
                        text = normalize_one_line(c.text) if SUBTITLE_ONE_LINE else c.text
                        clipped.append(Cue(start=cs, end=ce, text=text))
                all_cues.extend(clipped)

        # ---------------------------------------------------------------------
        # ìë§‰ ì •ë ¬ ë° ê²¹ì¹¨ ì²˜ë¦¬ (í™”ì ë°”ë€” ë•Œ ìë§‰ ëŠê¸° ë¡œì§ í¬í•¨)
        # ---------------------------------------------------------------------
        all_cues.sort(key=lambda x: (x.start, x.end))
        cleaned: List[Cue] = []
        for c in all_cues:
            if not cleaned:
                cleaned.append(c)
                continue
            prev = cleaned[-1]
            
            # âœ… ê²¹ì¹˜ë©´(ë’·ì‚¬ëŒì´ ë§ ì‹œì‘í•˜ë©´) ì•ì‚¬ëŒ ìë§‰ì„ ëŠì–´ë²„ë¦¼
            if c.start < prev.end:
                prev_end_new = max(prev.start + 0.05, c.start)
                cleaned[-1] = Cue(start=prev.start, end=prev_end_new, text=prev.text)
                if c.end <= c.start:
                    continue
            cleaned.append(c)

        srt_text = cues_to_srt(cleaned)
        with open(output_srt_path, "w", encoding="utf-8") as f:
            f.write(srt_text)
        print(f"âœ… SRT ì €ì¥ ì™„ë£Œ: {output_srt_path} (cue {len(cleaned)}ê°œ)")

    except Exception as e:
        print(f"âŒ OpenAI diarize STT ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return

    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
        except:
            pass

        # ì›ë³¸ ì´ë™
        if done_path:
            try:
                shutil.move(audio_path, done_path)
                print(f"âœ… ì›ë³¸ WAV ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_path).name}")
            except Exception as e:
                print(f"âŒ ì›ë³¸ ì´ë™ ì‹¤íŒ¨: {e}")
        else:
            print("âœ… ì‘ì—… ì™„ë£Œ. ì›ë³¸ WAVëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€.")



def vad_annotation_to_srt_empty_with_lang(final_merged, output_folder, audio_path, file_suffix):
    print("\nğŸ“„ 4. SRT íŒŒì¼ ìƒì„± ì‹œì‘...")

    subs = []
    idx = 1
    for seg in final_merged:
        start, end = float(seg['start']), float(seg['end'])
        
        start_td, end_td = timedelta(seconds=start), timedelta(seconds=end)
        if end_td <= start_td: end_td = start_td + timedelta(milliseconds=10)
        
        content_items = []
        for key, value in seg.items():
            if key not in ['start', 'end']:
                content_items.append(f"{key}: {value}")
                
        if not content_items:
            content_str = " "
        else:
            content_str = "|".join(content_items)
                
        subs.append(srt.Subtitle(index=idx, start=start_td, end=end_td, content=content_str))
        idx += 1
        
    print(f"   - ìµœì¢… SRTì— í¬í•¨ë  êµ¬ê°„ ìˆ˜: {len(subs)}ê°œ")
    srt_text = srt.compose(subs)
    
    base_filename = Path(audio_path).stem # ì›ë³¸ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
    srt_filename = f"{base_filename}{FILENAME_SUFFIX}_{file_suffix}.srt" # ìƒˆ íŒŒì¼ëª…
    srt_filepath = Path(output_folder) / srt_filename # í´ë” ê²½ë¡œì™€ íŒŒì¼ëª… ì¡°í•©
    Path(srt_filepath).write_text(srt_text, encoding="utf-8")
    if not subs: print("\n[âš ï¸ ê²½ê³ ] ìµœì¢… SRT íŒŒì¼ì— í¬í•¨ëœ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"âœ… SRT ì €ì¥ ì™„ë£Œ: {srt_filepath} (ì´ {len(subs)}êµ¬ê°„)")
    
    

# ============================================================
# 6) ë©”ì¸ ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    root = tk.Tk()
    root.withdraw()

    lang_map = {}
    sorted_list = []

    print("\nğŸ“Š ê°•ì˜ ì •ë³´ ì—‘ì…€(xlsx)ì´ ìˆìœ¼ë©´ ì„ íƒ (ì·¨ì†Œ ì‹œ ìë™ ê°ì§€)")
    xlsx_file_path = filedialog.askopenfilename(title="xlsx íŒŒì¼ ì„ íƒ (ì„ íƒ ì•ˆ í•¨: ì·¨ì†Œ)", filetypes=[("xlsx File", "*.xlsx")])

    if xlsx_file_path:
        try:
            lang_map, sorted_list = read_xlsx_and_create_dict(xlsx_file_path)
            print(f"âœ… ì—‘ì…€ ë¡œë“œ ì™„ë£Œ: {len(lang_map)}ê°œ ê°•ì˜")
        except Exception as e:
            print(f"âš ï¸ ì—‘ì…€ ì½ê¸° ì‹¤íŒ¨ â†’ ìë™ ê°ì§€ë¡œ ì§„í–‰: {e}")
    else:
        print("âš ï¸ ì—‘ì…€ ë¯¸ì„ íƒ â†’ ë³´ì¡°ì–¸ì–´ ìë™ ê°ì§€")

    print("\nğŸµ ë¶„ì„í•  WAV íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)")
    audio_files = filedialog.askopenfilenames(title="WAV íŒŒì¼ ì„ íƒ", filetypes=[("WAV Files", "*.wav")])
    if not audio_files:
        print("âŒ íŒŒì¼ ë¯¸ì„ íƒ â†’ ì¢…ë£Œ")
        raise SystemExit(0)

    print("\nğŸ’¾ ê²°ê³¼ SRT ì €ì¥ í´ë” ì„ íƒ")
    output_path = filedialog.askdirectory(title="SRT ì €ì¥ í´ë”")
    if not output_path:
        print("âŒ ì €ì¥ í´ë” ë¯¸ì„ íƒ â†’ ì¢…ë£Œ")
        raise SystemExit(0)

    print("\nğŸ“‚ ì™„ë£Œëœ WAV ì´ë™ í´ë” ì„ íƒ (ì·¨ì†Œ ì‹œ ì´ë™ ì•ˆ í•¨)")
    done_path = filedialog.askdirectory(title="ì™„ë£Œ WAV ì´ë™ í´ë”")
    if not done_path:
        done_path = ""
        print("âš ï¸ ì™„ë£Œ í´ë” ë¯¸ì„ íƒ â†’ ì›ë³¸ ìœ ì§€")

    for i, audio_file in enumerate(list(audio_files), 1):
        if not os.path.exists(audio_file):
            print(f"âš ï¸ [{i}/{len(audio_files)}] íŒŒì¼ ì—†ìŒ: {audio_file} â†’ ìŠ¤í‚µ")
            continue

        print(f"\n{'='*60}")
        print(f"â–¶ï¸  [{i}/{len(audio_files)}] ì²˜ë¦¬ ì‹œì‘: {os.path.basename(audio_file)}")
        print(f"{'='*60}")

        non_silent_segments = get_non_silent_segments_ffmpeg(audio_file)
        if non_silent_segments is None:
            print("âŒ FFmpeg ë¶„ì„ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ")
            continue

        # ìŒì•… ê°ì§€ + ë³‘í•©
        #ina_segments = detect_music_segments(audio_file)
        #music_blocks = build_music_blocks(ina_segments, short_speech_max=1.0)

        # full_audio ì²˜ë¦¬
        #if non_silent_segments == "full_audio":
        #    if music_blocks:
        #        try:
        #            audio_for_duration = AudioSegment.from_file(str(audio_file))
        #            total_dur = len(audio_for_duration) / 1000.0
        #            base_segments = [{"start": 0.0, "end": total_dur}]
        #            non_silent_segments = remove_music_from_non_silent(base_segments, music_blocks)
        #        except Exception as e:
        #            print(f"   [ê²½ê³ ] ì˜¤ë””ì˜¤ ê¸¸ì´ ì‹¤íŒ¨: {e}")
        #else:
        #    non_silent_segments = remove_music_from_non_silent(non_silent_segments, music_blocks)
#
        #if not non_silent_segments:
        #    print("   [ì •ë³´] ìŒì•… ì œê±° í›„ ë‚¨ëŠ” êµ¬ê°„ ì—†ìŒ â†’ ìŠ¤í‚µ")
        #    continue

        # ì˜¤ë””ì˜¤ ë¡œë”©
        print("\nğŸµ ì˜¤ë””ì˜¤ ë¡œë”©...")
        try:
            audio_loader = Audio(sample_rate=16000, mono=True)
            waveform, sample_rate = audio_loader(audio_file)
            print("âœ… ë¡œë”© ì™„ë£Œ.")
        except Exception as e:
            print(f"[ì¹˜ëª…] ë¡œë”© ì‹¤íŒ¨: {e}")
            continue

        # VAD + ì–¸ì–´ ê°ì§€
        vad_annotation = extract_segments_2stage(waveform, sample_rate, non_silent_segments)
        segment_with_lang = detect_language_for_vad_segments(vad_annotation, waveform, sample_rate, lang_id_model)

        # ìŒì•… ì œê±°(ì„ë² ë”© ê¸°ë°˜)
        #segment_with_lang_and_music = tag_noise_by_music_blacklist_iterative(
        #    segment_with_lang,
        #    ina_segments,
        #    waveform,
        #    sample_rate,
        #    verification_model,
        #    threshold=MUSIC_SIM_THRESHOLD,
        #    max_iterations=MUSIC_MAX_ITER
        #)
        #segment_with_lang_and_music2 = apply_sandwich_smoothing(segment_with_lang_and_music, max_duration=SANDWICH_MAX_DURATION)
#
        #print(f"ğŸ§¹ ìŒì•… ì œê±° ì „: {len(segment_with_lang)}ê°œ")
        #segment_with_lang_2 = [seg for seg in segment_with_lang_and_music2 if seg.get('audio_type') != 'noise_music']
        #print(f"ğŸ§¹ ìŒì•… ì œê±° í›„: {len(segment_with_lang)}ê°œ")

        if not segment_with_lang:
            print("âŒ ìœ íš¨ êµ¬ê°„ ì—†ìŒ â†’ ìŠ¤í‚µ/ì´ë™")
            if done_path:
                try:
                    shutil.move(audio_file, done_path)
                    print(f"âœ… ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_file).name}")
                except Exception as e:
                    print(f"âŒ ì´ë™ ì‹¤íŒ¨: {e}")
            continue

        # ë³´ì¡°ì–¸ì–´ ê²°ì •
        target_languages = [MAIN_LANGUAGE]
        if SUB_LANGUAGE is None:
            sub_language = select_sub_language(audio_file, lang_map, sorted_list, segment_with_lang)
        elif SUB_LANGUAGE == "no_sub":
            sub_language = None
        else:
            sub_language = SUB_LANGUAGE

        if sub_language is not None:
            target_languages.append(sub_language)

        # diarize ëª¨ë¸ì€ promptë¥¼ ì•ˆ ë°›ì§€ë§Œ, ê¸°ì¡´ êµ¬ì¡° ìœ ì§€ìš©
        instructor_prompt = INSTRUCTOR_PROMPT_DICT.get(sub_language)

        # ì œ3ì–¸ì–´
        if THIRD_LANG == 'no_third':
            third_lang = None
        else:
            third_lang = define_third_language(segment_with_lang, target_languages)

        # unknown ì²˜ë¦¬
        segment_with_lang_still_unknown_in = convert_to_unknown(third_lang, segment_with_lang, target_languages)
        segment_with_lang_without_unknown = merge_unknown(segment_with_lang_still_unknown_in)

        # âœ… ì—¬ê¸°ë¶€í„°ê°€ ë„¤ê°€ ë§í•œ "final segment 3" ë§Œë“œëŠ” êµ¬ê°„ (ê·¸ëŒ€ë¡œ ìœ ì§€)
        final_segment_2 = final_merge_VAD_by_lang(
            segment_with_lang_without_unknown,
            sub_language,
            third_lang,
            MAX_DURATION,
            MAX_GAP,
            MAX_MERGED_DURATION
        )
        final_segment_3 = redetect_language_for_merged_segments(
            final_segment_2,
            waveform,
            sample_rate,
            lang_id_model,
            sub_language,
            third_lang
        )
        
        
        # VAD ê´€ì°°ìš© ì½”ë“œ
        # vad_annotation_to_srt_empty_with_lang(final_segment_3, output_path, audio_file, file_suffix="vad_view")
        final_segment_4 = final_merge_VAD_by_lang(final_segment_3, sub_language, third_lang, MAX_DURATION, MAX_GAP, MAX_MERGED_DURATION)
        # vad_annotation_to_srt_empty_with_lang(segment_with_lang, output_path, audio_file, file_suffix="segment_with_lang(2)")

        # âœ… ì—¬ê¸°ì„œë¶€í„° STTë§Œ OpenAI diarizeë¡œ ìˆ˜í–‰ (all_cues ë£¨í”„ í¬í•¨)
        run_openai_diarize_and_save_srt(waveform=waveform, 
                                        sample_rate=sample_rate, 
                                        audio_path=audio_file, 
                                        final_segments=final_segment_4, 
                                        output_folder=output_path, 
                                        done_path=done_path
                                        )

    print("\n\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
