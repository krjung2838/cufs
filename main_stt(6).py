import srt
from datetime import timedelta
import tkinter as tk
from tkinter import filedialog
import shutil
from pathlib import Path
import torch
import torchaudio
import torch.nn.functional as F
import numpy as np
from pyannote.audio import Pipeline, Audio
from pyannote.core import Annotation, Segment
from pydub import AudioSegment
from speechbrain.inference import EncoderClassifier
import subprocess
import re
import stable_whisper # STTë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from collections import Counter
import traceback
import os
import pandas as pd
from speechbrain.inference import SpeakerRecognition
from inaSpeechSegmenter import Segmenter
import soundfile as sf


# ========== ì‚¬ìš©ì ì„¤ì • íŒŒë¼ë¯¸í„° ==========
HF_TOKEN = "" # í—ˆê¹…í˜ì´ìŠ¤ í† í°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
FFMPEG_PATH = r"" # ğŸ“Œ FFmpeg ì‹¤í–‰ íŒŒì¼ì˜ 'ì „ì²´ ê²½ë¡œ'ë¥¼ ì •í™•í•˜ê²Œ ì…ë ¥í•˜ì„¸ìš”.
STT_MODEL_SIZE = "large-v3-turbo" # STT ëª¨ë¸ í¬ê¸° ('tiny', 'base', 'small', 'medium', 'large-v3')
FILENAME_SUFFIX = "" # ìµœì¢… íŒŒì¼ëª…ì— ì¶”ê°€ë  ì ‘ë¯¸ì‚¬
MAIN_LANGUAGE = "ko"
SUB_LANGUAGE = None # ë³´ì¡°ì–¸ì–´ë¥¼ ê°•ì œë¡œ ê³ ì •. ê¸°ë³¸ê°’ì€ None. # ë³´ì¡°ì–¸ì–´ë¥¼ ë‘ì§€ ì•Šìœ¼ë ¤ë©´ "no_sub"ìœ¼ë¡œ ì„¤ì •
THIRD_LANG = "no_third" # 
ALLOWED_LANGS = ['ko', 'en', 'vi', 'es', 'zh', 'ja', 'id']


# 1. FFmpeg ë³¼ë¥¨ ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
SILENCE_THRESH_DB = -50  # FFmpegì´ 'ì¹¨ë¬µ'ìœ¼ë¡œ íŒë‹¨í•  ì†Œë¦¬ì˜ í¬ê¸° ê¸°ì¤€ì…ë‹ˆë‹¤. -40dBë³´ë‹¤ ì‘ì€ ì†Œë¦¬ëŠ” ì¹¨ë¬µìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
MIN_SILENCE_DURATION_S = 0.05  # ìµœì†Œ 0.05ì´ˆ ì´ìƒ ì§€ì†ë˜ëŠ” ì¹¨ë¬µ êµ¬ê°„ë§Œ ì°¾ì•„ë‚´ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.


# 2. VAD ëª¨ë¸ ì„¸ë¶€ íŒŒë¼ë¯¸í„°
VAD_PARAMS = {
    "min_duration_off": 0.01,  # ìŒì„±ì´ ì—†ëŠ” êµ¬ê°„(ì¹¨ë¬µ)ì´ ìµœì†Œ 0.01ì´ˆëŠ” ë˜ì–´ì•¼ ì¹¨ë¬µìœ¼ë¡œ ì¸ì •í•©ë‹ˆë‹¤.
    "min_duration_on": 0.05,  # ìŒì„±ì´ ìˆëŠ” êµ¬ê°„ì´ ìµœì†Œ 0.01ì´ˆëŠ” ë˜ì–´ì•¼ ìŒì„±ìœ¼ë¡œ ì¸ì •í•©ë‹ˆë‹¤.
    "onset": 0.01,  # ìŒì„± ì‹œì‘ì´ë¼ê³  íŒë‹¨í•  í™•ë¥ ì˜ ì„ê³„ê°’ì…ë‹ˆë‹¤. (0~1 ì‚¬ì´, ë†’ì„ìˆ˜ë¡ ë³´ìˆ˜ì )
    "offset": 0.01  # ìŒì„± ì¢…ë£Œë¼ê³  íŒë‹¨í•  í™•ë¥ ì˜ ì„ê³„ê°’ì…ë‹ˆë‹¤. (0~1 ì‚¬ì´, ë†’ì„ìˆ˜ë¡ ë³´ìˆ˜ì )
}

# VAD ì„¸ë¶€ì„¤ì •
MAX_DURATION = 1 # ì¶©ë¶„íˆ ê¸¸ë‹¤ê³  íŒë‹¨í•  ë³‘í•© ì „ VADì˜ ê¸¸ì´ ê¸°ì¤€ì…ë‹ˆë‹¤.
MAX_GAP = 0.5 # ì¶©ë¶„íˆ ê¸¸ë‹¤ê³  íŒë‹¨í•  ë³‘í•© ì „ ê° VADì˜ ê°­ì˜ ê¸¸ì´ ê¸°ì¤€ì…ë‹ˆë‹¤.
MAX_MERGED_DURATION = 5 # ë³‘í•©ëœ ì„¸ê·¸ë¨¼íŠ¸ì˜ ìµœëŒ€ê¸¸ì´ì…ë‹ˆë‹¤.

# â–¼â–¼â–¼ í™”ì ë¶„ë¦¬ ë¯¼ê°ë„ íŠœë‹ â–¼â–¼â–¼
diarization_params = {
    # 1. ì¡ìŒ/ëŠê¹€ ì²˜ë¦¬
    "segmentation": {
        "min_duration_on": 0.05, 
        "min_duration_off": 0.01  
    },
    
    # 2. í™”ì êµ¬ë¶„ ë¯¼ê°ë„
    "clustering": {
        "method": "centroid", # ì¤‘ì‹¬ì  ê¸°ì¤€ (ê¸°ë³¸ê°’)
        "min_cluster_size": 12, # ìµœì†Œ ì´ ì •ë„ í¬ê¸°ëŠ” ë˜ì–´ì•¼ í™”ìë¡œ ì¸ì • (ê¸°ë³¸ 12~15)
        "threshold": 1.0, # â˜… í•µì‹¬: 0.0 ~ 1.0 ì‚¬ì´ (ê¸°ë³¸ê°’ì€ ë³´í†µ 0.7 ë‚´ì™¸)
    }
}

# 4. ìµœì¢… SRT ìƒì„± ë° ë³‘í•© íŒŒë¼ë¯¸í„°
MIN_SEGMENT_DURATION = 0.1 # VADë¡œ ì°¾ì•„ë‚¸ ìŒì„± êµ¬ê°„ ì¤‘ 0.1ì´ˆë³´ë‹¤ ì§§ì€ êµ¬ê°„ì€ ë„ˆë¬´ ì§§ì€ ë…¸ì´ì¦ˆì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ë¬´ì‹œí•˜ê³  ì œê±°í•©ë‹ˆë‹¤.
MERGE_MAX_SECONDS = 15.0 # STTë¥¼ í•˜ê¸° ì „, ê°™ì€ ì–¸ì–´ì˜ ìŒì„± êµ¬ê°„ë“¤ì„ í•©ì¹  ë•Œ ìµœëŒ€ 15ì´ˆê¹Œì§€ë§Œ í•©ì¹˜ë„ë¡ ì œí•œí•©ë‹ˆë‹¤. ë„ˆë¬´ ê¸¸ë©´ STT ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# --- ìë§‰ ë³‘í•© ì„¤ì • ---
MERGE_THRESHOLD_SECONDS = 1.0  # 1ì°¨ ë³‘í•© ê¸°ì¤€: ìë§‰ì„ í•©ì¹  ê¸°ì¤€ ì‹œê°„ (ì´ˆ)
MAX_CHARS_PER_LINE = 30      # 1ì°¨ ë³‘í•© ê¸°ì¤€: í•©ì³ì§„ ìë§‰ì˜ ìµœëŒ€ ê¸€ì ìˆ˜
MIN_DURATION_SECONDS = 1.0   # 2ì°¨ ë³‘í•© ê¸°ì¤€: ì´ ì‹œê°„(ì´ˆ)ë³´ë‹¤ ì§§ì€ ìë§‰ì€ ì• ìë§‰ì— ê°•ì œë¡œ í•©ì¹¨


# --- í”„ë¡¬í”„íŠ¸ ë”•ì…”ë„ˆë¦¬ ---
en = "Today, we will discuss the importance of renewable energy. The quick brown fox jumps over the lazy dog."
# ja = "ì˜¤ëŠ˜ì€ ã¦å½¢ë‘ è¾æ›¸å½¢ë¥¼ ë³¼ ê±°ì•¼. ã¦å½¢ëŠ” ì—°ê²°Â·ë¶€íƒ(ã€œã¦ãã ã•ã„), è¾æ›¸å½¢ëŠ” ê¸°ë³¸í˜•. ï¼•ç•ªå‡ºå£ì—ì„œ ë§Œë‚˜. ì—˜ë¦¬ë² ì´í„°ëŠ” ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼, ê³„ë‹¨ì€ éšæ®µ. "
ja = "í…Œí˜• ë’¤ì— ì´ë£¨ë¥¼ ë¶™ì´ë©´ ì§„í–‰í˜•ì´ ë¼. ì‹œí…Œ ì´ë£¨ëŠ” 'í•˜ê³  ìˆë‹¤'ë¼ëŠ” ëœ»ì´ì•¼. ì˜¤ìŠ¤ìŠ¤ë©” ë©”ë‰´ê°€ ë­ì˜ˆìš”? ë‚˜ë§ˆë¹„ë£¨ ë‘ ì” ì£¼ì„¸ìš”. ì‚¬ì´í›„ë¥¼ ìƒì–´ë²„ë ¤ì„œ ì¼€ì´ì‚¬ì¸ ì— ì‹ ê³ í–ˆì–´."
vi = "ì”¬ì§œì˜¤ ê¹œì–¸ ë˜ì´ ë“œì–µì¡°ì´ í¼ ë°˜ë¯¸ ì‘ì˜¨ ì•„ì‰ ì—  ìì˜¤ë¹„ì—” ë°”ì˜¤ë‹ˆì—ìš°"
es = "ì´ê²ƒì€ í•œêµ­ì–´ì™€ ìŠ¤í˜ì¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ìŠ¤í˜ì¸ì–´ ë¬¸ë²• ê°•ì˜ì…ë‹ˆë‹¤. Me llamo Juan. Â¿DÃ³nde estÃ¡ la biblioteca? Te quiero mucho."
idn = "ìŠ¬ë¼ë§› ë¹ ê¸° ëœ¨ë¦¬ë§ˆ ê¹Œì‹œ ì•„ë¹  ê¹Œë°”ë¥´ ì˜ë¥´ê¸° ì‹¸ì•¼ íŒ…ê°ˆ ë”” ì„œìš¸ ëœ¨ë¦¬ë§ˆ ê¹Œì‹œ ë°”ëƒ‘ ì‚¼ë¹ ì´ ì¤Œë¹  ë¼ê¸°"
zh = "ì˜¤ëŠ˜ì€ æŠŠå­—å¥ë‘ è¢«å­—å¥ë¥¼ ë¹„êµí•  ê±°ì•¼. æŠŠå­—å¥ëŠ” ì²˜ë¶„ ê°•ì¡°, è¢«å­—å¥ëŠ” í”¼ë™.ä¸‰å·å‡ºå£ì—ì„œ ë§Œë‚˜. íƒì‹œëŠ” æ‰“è½¦, ê°ˆì•„íƒ€ê¸°ëŠ” æ¢ä¹˜."
ko = ""
INSTRUCTOR_PROMPT_DICT = {
    'vi': vi,
    'es': es,
    'id': idn,
    'zh': zh,
    'en': en,
    'ja': ja,
    'ko': ko
}


# ========== Symlink ìš°íšŒ Patch (Windows í™˜ê²½ í˜¸í™˜ì„±) ==========
def force_copy(src, dst):
    if src is None or dst is None: return None
    src_path, dst_path = Path(src), Path(dst)
    try:
        dst_path.parent.mkdir(parents=True, exist_ok=True)
        if src_path.is_dir(): shutil.copytree(src_path, dst_path, dirs_exist_ok=True)
        else: shutil.copy2(src_path, dst_path)
        return dst
    except Exception as e:
        print(f"   [ê²½ê³ ] íŒŒì¼ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

import speechbrain.utils.fetching as sb_fetch
sb_fetch.link_with_strategy = lambda src, dst, strategy: force_copy(src, dst)

# ========== ëª¨ë¸ ë¡œë”© (ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰) ==========
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘... (VAD, Language ID, STT)")
# 1. VAD
vad_pipeline = Pipeline.from_pretrained("pyannote/voice-activity-detection", use_auth_token=HF_TOKEN)
vad_pipeline.to(torch.device(DEVICE))
vad_pipeline.instantiate(VAD_PARAMS)
print("âœ… VAD ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 2. Language ID
lang_id_model = EncoderClassifier.from_hparams(
    source="speechbrain/lang-id-voxlingua107-ecapa",
    savedir="tmp_lang_id"
)
print("âœ… Language ID ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# 3. STT (Stable Whisper)
stt_model = stable_whisper.load_model(STT_MODEL_SIZE, device=DEVICE)
print(f"âœ… STT ëª¨ë¸({STT_MODEL_SIZE}) ë¡œë”© ì™„ë£Œ.")


# í™”ì ì¸ì‹(ê²€ì¦) ì „ìš© ëª¨ë¸ì…ë‹ˆë‹¤. ì„±ëŠ¥ì´ ì•„ì£¼ ë›°ì–´ë‚©ë‹ˆë‹¤.
print("ğŸ”„ Speaker Verification ëª¨ë¸ ë¡œë”© ì¤‘... (SpeechBrain)")
verification_model = SpeakerRecognition.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb", 
    savedir="tmp_speaker_verification",
    run_opts={"device": DEVICE}
)
print("âœ… Speaker Verification ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

# ìŒì•… ê°ì§€ ëª¨ë¸
print("ğŸ”„ Music/Speech ì„¸ê·¸ë¨¼í„° ë¡œë”© ì¤‘... (inaSpeechSegmenter)")
music_segmenter = Segmenter(vad_engine="smn", detect_gender=False)
print("âœ… Music/Speech ì„¸ê·¸ë¨¼í„° ë¡œë”© ì™„ë£Œ.")


# ========== í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ==========

def read_xlsx_and_create_dict(xlsx_file_path):
    """ê°•ì˜ëª…ê³¼ ë³´ì¡°ì–¸ì–´ê°€ ë§¤ì¹­ë˜ì–´ ìˆëŠ” ì—‘ì…€íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ë”•ì…”ë„ˆë¦¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    
    df = pd.read_excel(
    io=xlsx_file_path,     # 1. ì´ íŒŒì¼ì„
    header=3,          # 2. 4ë²ˆì§¸ ì¤„ì„ í—¤ë”ë¡œ ì‚¼ì•„ì„œ
    usecols="C:D"      # 3. Cì—´ê³¼ Dì—´ë§Œ ì½ì–´ë¼
    )     
    df = df.dropna(subset=['ë³´ì¡°ì–¸ì–´']) # ë³´ì¡°ì–¸ì–´ê°€ ë¹„ì–´ìˆëŠ” í–‰ì€ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.
    
    lang_map = df.set_index('ê°•ì˜ëª…')['ë³´ì¡°ì–¸ì–´'].to_dict() # ê°•ì˜ëª… : ë³´ì¡°ì–¸ì–´ ì˜ í˜•ì‹ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    keys_view = list(lang_map.keys()) # lang_mapì—ì„œ keyê°’ë§Œ ë½‘ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” í•œ ê³µí†µì ‘ë‘ì–´ê°€ ë‹¤ë¥¸ ê³µí†µì ‘ë‘ì–´ë¥¼ í¬í•¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    sorted_list = sorted(keys_view, key=len, reverse=True) # ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸¸ì´ê°€ ê¸´ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    
    return lang_map, sorted_list




def get_non_silent_segments_ffmpeg(audio_path):
    print("\nğŸ”Š 0. FFmpegë¡œ ì¹¨ë¬µ êµ¬ê°„ ë¶„ì„ ì‹œì‘...")
    command = [FFMPEG_PATH, '-i', str(audio_path), '-af', f'silencedetect=noise={SILENCE_THRESH_DB}dB:d={MIN_SILENCE_DURATION_S}', '-f', 'null', '-']
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True, encoding='utf-8')
        ffmpeg_output = result.stderr
    except FileNotFoundError:
        print(f"\n[ì¹˜ëª…ì  ì˜¤ë¥˜] 'ffmpeg'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. FFMPEG_PATH ë³€ìˆ˜ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {FFMPEG_PATH}")
        return None
    except subprocess.CalledProcessError as e:
        print(f"\n[ì˜¤ë¥˜] FFmpeg ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e.stderr}")
        return None
    
    silence_starts = [float(t) for t in re.findall(r'silence_start: (\d+\.?\d*)', ffmpeg_output)]
    silence_ends = [float(t) for t in re.findall(r'silence_end: (\d+\.?\d*)', ffmpeg_output)]

    if not silence_starts:
        print("   [ì •ë³´] FFmpegê°€ ì¹¨ë¬µ êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
        return "full_audio"

    if len(silence_starts) > len(silence_ends):
        silence_starts = silence_starts[:len(silence_ends)]

    non_silent_segments = []
    last_end = 0.0
    for start, end in zip(silence_starts, silence_ends):
        if start > last_end + 0.01:
            non_silent_segments.append({'start': last_end, 'end': start})
        last_end = end

    try:
        duration = len(AudioSegment.from_file(audio_path)) / 1000.0
        if duration > last_end + 0.01:
            non_silent_segments.append({'start': last_end, 'end': duration})
    except Exception as e:
        print(f"   [ê²½ê³ ] ì˜¤ë””ì˜¤ ì „ì²´ ê¸¸ì´ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print(f"âœ… FFmpeg ë¶„ì„ ì™„ë£Œ. {len(non_silent_segments)}ê°œì˜ ìœ ì„± êµ¬ê°„ ë°œê²¬.")
    return non_silent_segments




def detect_music_segments(audio_path):
    """
    inaSpeechSegmenter ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ì¦‰ì‹œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    Output: [{'label': 'music', 'start': 0.0, 'end': 10.0}, ...]
    """
    print("\nğŸ¼ 0-1. inaSpeechSegmenterë¡œ ìŒì•…/ìŒì„± êµ¬ê°„ ë¶„ì„ ì‹œì‘...")
    try:
        # ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë±‰ìŠµë‹ˆë‹¤: [('music', 0.0, 5.0), ...]
        raw_segments = music_segmenter(str(audio_path))
        print(f"   - inaSpeechSegmenter ì„¸ê·¸ë¨¼íŠ¸ ê°œìˆ˜: {len(raw_segments)}")
        
        # â˜… ì—¬ê¸°ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜!
        dict_segments = []
        for label, start, end in raw_segments:
            dict_segments.append({
                'label': label,
                'start': float(start),
                'end': float(end)
            })
            
        return dict_segments

    except Exception as e:
        print(f"   [ê²½ê³ ] ìŒì•… êµ¬ê°„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return []
    
    
    

def build_music_blocks(ina_segments, short_speech_max=1.0):
    if not ina_segments: return []

    blocks = []
    cur_start = None
    cur_end = None


    for item in ina_segments:
        label = item['label']
        start = item['start']
        end = item['end']
        dur = end - start

        if label == "music":
            if cur_start is None: cur_start = start
            cur_end = end
        else:
            if cur_start is not None and label in ("speech", "noise") and dur <= short_speech_max:
                cur_end = end
            else:
                if cur_start is not None:
                    blocks.append((cur_start, cur_end))
                    cur_start = None
                    cur_end = None

    if cur_start is not None:
        blocks.append((cur_start, cur_end))

    if not blocks: return []


    blocks = sorted(blocks)
    merged = []
    for start, end in blocks:
        if not merged:
            merged.append([start, end])
        else:
            last_start, last_end = merged[-1]
            if start <= last_end + 0.2:
                merged[-1][1] = max(last_end, end)
            else:
                merged.append([start, end])

    music_blocks = [(s, e) for s, e in merged]
    print(f"   - ë³‘í•©ëœ ìŒì•… ë¸”ë¡ ê°œìˆ˜: {len(music_blocks)}")
    return music_blocks




def remove_music_from_non_silent(non_silent_segments, music_blocks, min_len=0.05):
    """
    ffmpegë¡œ ì–»ì€ ìœ ì„± êµ¬ê°„(non_silent_segments)ì—ì„œ
    music_blocksë¥¼ ì „ë¶€ ë¹¼ê³  ë‚¨ì€ êµ¬ê°„ë§Œ ë°˜í™˜.

    non_silent_segments: [{'start': float, 'end': float}, ...]
    music_blocks: [(start, end), ...]
    """
    if not music_blocks:
        return non_silent_segments

    if not non_silent_segments or non_silent_segments == "full_audio":
        # "full_audio"ëŠ” ì—¬ê¸°ì„œ ì²˜ë¦¬í•˜ì§€ ì•Šê³ , í˜¸ì¶œë¶€ì—ì„œ ë³„ë„ ì²˜ë¦¬
        return non_silent_segments

    cleaned = []

    for seg in non_silent_segments:
        seg_start = float(seg["start"])
        seg_end   = float(seg["end"])
        parts = [(seg_start, seg_end)]

        for m_start, m_end in music_blocks:
            new_parts = []
            for p_start, p_end in parts:
                # ê²¹ì¹˜ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                if p_end <= m_start or p_start >= m_end:
                    new_parts.append((p_start, p_end))
                    continue

                # ê²¹ì¹˜ë©´ ìŒì•… ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê³  ì–‘ìª½ë§Œ ìœ ì§€
                if p_start < m_start:
                    new_parts.append((p_start, m_start))
                if p_end > m_end:
                    new_parts.append((m_end, p_end))

            parts = new_parts
            if not parts:
                break

        # ë„ˆë¬´ ì§§ì€ êµ¬ê°„ì€ ë²„ë¦¬ê³ , ì¼ì • ê¸¸ì´ ì´ìƒë§Œ ì±„íƒ
        for p_start, p_end in parts:
            if p_end - p_start >= min_len:
                cleaned.append({"start": p_start, "end": p_end})

    print(f"   - ìŒì•… ì œê±° ì „ ìœ ì„± êµ¬ê°„: {len(non_silent_segments)}ê°œ â†’ ì œê±° í›„: {len(cleaned)}ê°œ")
    return cleaned




def extract_segments_2stage(waveform, sample_rate, non_silent_segments):
    print("\nğŸš€ 1. 2ë‹¨ê³„ VAD ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ì‹œì‘...")
    final_vad_annotation = Annotation()
    
    if non_silent_segments == "full_audio":
        print("   - ì „ì²´ ì˜¤ë””ì˜¤ì— ëŒ€í•´ VADë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return vad_pipeline({"waveform": waveform, "sample_rate": sample_rate})

    total_speech_chunks_found = 0
    skipped_chunks = 0
    
    # Pyannoteê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ê¸¸ì´ (ì•ˆì „í•˜ê²Œ 0.06ì´ˆ ì •ë„ë¡œ ì¡ìŒ)
    MIN_CHUNK_SAMPLES = int(sample_rate * 0.06) 

    for i, segment in enumerate(non_silent_segments):
        start, end = segment['start'], segment['end']
        start_frame, end_frame = int(start * sample_rate), int(end * sample_rate)
        
        # ì¸ë±ìŠ¤ ë²”ìœ„ ë³´í˜¸
        if end_frame > waveform.shape[1]:
            end_frame = waveform.shape[1]
            
        chunk_waveform = waveform[:, start_frame:end_frame]
        
        # â˜… [í•µì‹¬ ìˆ˜ì •] ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ ì¡°ê°(0.06ì´ˆ ë¯¸ë§Œ)ì€ VAD ì—ëŸ¬ë¥¼ ìœ ë°œí•˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.
        if chunk_waveform.shape[1] < MIN_CHUNK_SAMPLES:
            skipped_chunks += 1
            continue

        file_chunk = {"waveform": chunk_waveform, "sample_rate": sample_rate}
        
        try:
            # VAD ì‹¤í–‰ (ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë”ë¼ë„ í”„ë¡œê·¸ë¨ì´ ì£½ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)
            vad_result_chunk = vad_pipeline(file_chunk)
            
            for speech_turn, _, _ in vad_result_chunk.itertracks(yield_label=True):
                offset_speech_turn = Segment(speech_turn.start + start, speech_turn.end + start)
                final_vad_annotation[offset_speech_turn] = "speech"
                total_speech_chunks_found += 1
                
        except Exception as e:
            # í˜¹ì‹œ ëª¨ë¥¼ ë‚´ë¶€ ì—ëŸ¬ ë°©ì§€ (ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰)
            print(f"   [ê²½ê³ ] VAD ì²˜ë¦¬ ì¤‘ ì¡°ê° ìŠ¤í‚µë¨ ({start:.2f}~{end:.2f}s): {e}")
            continue
            
    merged_annotation = Annotation()
    for segment in final_vad_annotation.support().itersegments():
        merged_annotation[segment] = "speech"
        
    print(f"âœ… 2ë‹¨ê³„ VAD ë¶„ì„ ì™„ë£Œ. ì´ {total_speech_chunks_found}ê°œì˜ ìŒì„± ì¡°ê° ë°œê²¬. (ë„ˆë¬´ ì§§ì•„ ìƒëµëœ ì¡°ê°: {skipped_chunks}ê°œ)")
    return merged_annotation




def detect_language_for_vad_segments(vad_annotation, waveform, sample_rate, lang_id_model):
    """
    pyannote VAD Annotation ê²°ê³¼ì™€ ì´ë¯¸ ë¡œë“œëœ waveformì„ ì‚¬ìš©í•´ ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    (â˜…ìˆ˜ì •: 0.1ì´ˆ ë¯¸ë§Œ êµ¬ê°„ì€ ì–¸ì–´ ê°ì§€ ëŒ€ìƒì—ì„œ ì‚¬ì „ì— ì œì™¸í•©ë‹ˆë‹¤)
    """
    print("\nğŸš€ VAD êµ¬ê°„ë³„ ì–¸ì–´ ê°ì§€ ì‹œì‘ (0.1ì´ˆ ë¯¸ë§Œ ì‚¬ì „ ì œê±°)...")
    
    label_encoder = lang_id_model.hparams.label_encoder
    
    # 1. Annotation ê°ì²´ë¥¼ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    segments_with_lang = []
    skipped_short_count = 0

    for segment in vad_annotation.itersegments():
        duration = segment.end - segment.start
        
        # âš¡ [í•µì‹¬ ìˆ˜ì •] 0.1ì´ˆ ë¯¸ë§Œì´ë©´ ì•„ì˜ˆ ë¦¬ìŠ¤íŠ¸ì— ë„£ì§€ ì•Šê³  ê±´ë„ˆëœë‹ˆë‹¤.
        if duration < 0.1:
            skipped_short_count += 1
            continue
            
        segments_with_lang.append({'start': segment.start, 'end': segment.end})

    print(f"   - âœ‚ï¸ 0.1ì´ˆ ë¯¸ë§Œ ì´ˆë‹¨íŒŒ {skipped_short_count}ê°œ ì‚¬ì „ ì œê±°ë¨.")

    # 2. ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    for seg in segments_with_lang:
        # ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë‹¤ì‹œ ì½ëŠ” ëŒ€ì‹ , ë©”ëª¨ë¦¬ì— ìˆëŠ” waveformì—ì„œ ë°”ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
        start_sample = int(seg['start'] * sample_rate)
        end_sample = int(seg['end'] * sample_rate)
        segment_waveform = waveform[:, start_sample:end_sample]

        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´(0.5ì´ˆ ë¯¸ë§Œ) 'unknown'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        if segment_waveform.shape[1] < sample_rate * 0.5:
            seg['lang'] = 'ko'
            continue
        
        # ì˜ë¼ë‚¸ ì˜¤ë””ì˜¤ ì¡°ê°ìœ¼ë¡œ ì–¸ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        prediction = lang_id_model.classify_batch(segment_waveform)
        
        # 1. ì¼ë‹¨ Top 1 ì–¸ì–´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        top_full_label = prediction[3][0]
        top_lang_code = top_full_label.split(':')[0].strip().lower()

        if top_lang_code in ALLOWED_LANGS:
            # 2. Top 1ì´ í—ˆìš© ëª©ë¡ì— ìˆìœ¼ë©´, ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê°€ì¥ ë¹ ë¦„)
            seg['lang'] = top_lang_code
        else:
            # 3. Top 1ì´ í—ˆìš© ëª©ë¡ì— ì—†ìœ¼ë©´, ì „ì²´ í™•ë¥ ì„ ë’¤ì ¸ë´…ë‹ˆë‹¤.
            print(f"    - [ì–¸ì–´ ì¬ì¡°ì •] Top 1 '{top_lang_code}'(ì´)ê°€ í—ˆìš© ëª©ë¡ì— ì—†ìŒ. '{ALLOWED_LANGS}' ë‚´ì—ì„œ ì¬ê²€ìƒ‰...")

            if (len(prediction) < 1 or
                    not isinstance(prediction[0], torch.Tensor) or
                    prediction[0].numel() == 0):
                print(f"    - [ê²½ê³ ] í™•ë¥  í…ì„œ ì—†ìŒ/ë¹„ì—ˆìŒ ({seg['start']:.2f}s~{seg['end']:.2f}s). 'ko' ì²˜ë¦¬.")
                seg['lang'] = 'ko'
                continue

            probabilities = prediction[0]

            allowed_probs = {}
            num_langs_to_check = min(len(probabilities), len(label_encoder.ind2lab))
            for i in range(num_langs_to_check):
                if i not in label_encoder.ind2lab: continue
                label_str = label_encoder.ind2lab[i]
                lang_code = label_str.split(':')[0].strip().lower()

                if lang_code in ALLOWED_LANGS:
                    if i < len(probabilities):
                         prob = probabilities[i].item()
                         allowed_probs[lang_code] = prob

            if allowed_probs:
                final_lang = max(allowed_probs, key=allowed_probs.get)
                seg['lang'] = final_lang
            else:
                seg['lang'] = 'ko'

    print("âœ… ì–¸ì–´ ê°ì§€ ì™„ë£Œ")
    return segments_with_lang # ë°ì´í„° í˜•íƒœ : {'start':..., 'end':..., 'lang':...}




def tag_noise_by_music_blacklist_iterative(vad_segments, ina_segments, waveform, sample_rate, verification_model, threshold=0.4, max_iterations=2):
    print(f"\nğŸ¼ [Iterative Blacklist] ë°˜ë³µ ì •ì œ ë°©ì‹ìœ¼ë¡œ ìŒì•… ì œê±° ì‹œì‘ (ìµœëŒ€ {max_iterations}íšŒ ë°˜ë³µ)...")
    
    if not vad_segments: return []

    # VAD ë°ì´í„° í‘œì¤€í™”
    if isinstance(vad_segments, Annotation):
        seg_list = [{'start': s.start, 'end': s.end} for s in vad_segments.itersegments()]
    else: seg_list = vad_segments

    total_len = waveform.shape[1]
    
    # === [Step 1] ì´ˆê¸° ìŒì•… ëª½íƒ€ì£¼ ìƒì„± (inaSpeechSegmenter ê¸°ë°˜) ===
    # ì´ê³³ì— ëª¨ì¸ ì„ë² ë”©ë“¤ì´ 'ìŒì•… ê¸°ì¤€ì 'ì´ ë©ë‹ˆë‹¤.
    music_embeddings_pool = [] 
    
    for item in ina_segments:
        label = item['label']
        start = item['start']
        end = item['end']
        
        if label == 'music':
            curr = start
            while curr < end:
                chunk_end = min(curr + 5.0, end)
                if chunk_end - curr < 1.0: break 
                
                s_sample = int(curr * sample_rate)
                e_sample = int(chunk_end * sample_rate)
                
                try:
                    # ìŒì•… êµ¬ê°„ì˜ ì„ë² ë”© ì¶”ì¶œí•˜ì—¬ í’€(pool)ì— ì €ì¥
                    emb = verification_model.encode_batch(waveform[:, s_sample:e_sample]).flatten()
                    music_embeddings_pool.append(emb)
                except: pass
                curr += 5.0

    if not music_embeddings_pool:
        print("   âš ï¸ ì´ˆê¸° ìŒì•… êµ¬ê°„ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°˜ë³µ í•„í„°ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return seg_list

    # === [Step 2] ë°˜ë³µ í•„í„°ë§ (Iterative Loop) ===
    for i in range(max_iterations):
        print(f"   ğŸ”„ [Round {i+1}] ìŒì•… ëª½íƒ€ì£¼ ì—…ë°ì´íŠ¸ ë° í•„í„°ë§ ì¤‘... (í˜„ì¬ í‘œë³¸ ìˆ˜: {len(music_embeddings_pool)}ê°œ)")
        
        # 1. í˜„ì¬ í’€(Pool)ì— ìˆëŠ” ëª¨ë“  ìŒì•… ì„ë² ë”©ì˜ í‰ê· (Centroid) ê³„ì‚°
        #    Roundê°€ ê±°ë“­ë ìˆ˜ë¡ 1ì°¨ì—ì„œ ê±¸ëŸ¬ì§„ 'ì• ë§¤í•œ ìŒì•…'ë“¤ì˜ íŠ¹ì§•ì´ ë°˜ì˜ë©ë‹ˆë‹¤.
        music_centroid = torch.mean(torch.stack(music_embeddings_pool), dim=0)

        tagged_in_this_round = 0
        
        # 2. VAD ì„¸ê·¸ë¨¼íŠ¸ ì „ìˆ˜ ì¡°ì‚¬
        for seg in seg_list:
            # ì´ë¯¸ ë…¸ì´ì¦ˆ/ìŒì•…ìœ¼ë¡œ íŒëª…ë‚œ ê±´ ê±´ë„ˆë›°ë˜, ì„ë² ë”© í’€ì—ëŠ” ê¸°ì—¬í–ˆìŒ
            if seg.get('audio_type') in ['noise_or_music', 'noise_short', 'noise_music']:
                continue
            
            start = seg['start']
            end = seg['end']
            duration = end - start
            
            # ë„ˆë¬´ ì§§ì€ê±´ íŒ¨ìŠ¤ (0.1ì´ˆ ë¯¸ë§Œ)
            if duration < 0.1: continue

            s_sample = int(start * sample_rate)
            e_sample = int(end * sample_rate)
            if e_sample > total_len: e_sample = total_len
            
            try:
                # í˜„ì¬ ê²€ì‚¬í•  êµ¬ê°„ì˜ ì„ë² ë”©
                curr_emb = verification_model.encode_batch(waveform[:, s_sample:e_sample]).flatten()
                
                # ì—…ë°ì´íŠ¸ëœ ëª½íƒ€ì£¼ì™€ ë¹„êµ
                score = F.cosine_similarity(music_centroid, curr_emb, dim=0).item()
                
                if score >= threshold:
                    # ìŒì•…ìœ¼ë¡œ íŒëª…!
                    seg['audio_type'] = 'noise_music'
                    seg['music_sim'] = f"{score:.2f}"
                    
                    # ğŸ”¥ [í•µì‹¬] ì¡ì•„ë‚¸ ì´ ë…€ì„ì˜ ì„ë² ë”©ì„ ë‹¤ìŒ ë¼ìš´ë“œ ê¸°ì¤€ì ì— ì¶”ê°€!
                    music_embeddings_pool.append(curr_emb) 
                    tagged_in_this_round += 1
                else:
                    # ì•„ì§ì€ speechë¡œ ìœ ì§€ (ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ë‹¤ì‹œ ê²€ì‚¬ ë‹¹í•  ìˆ˜ ìˆìŒ)
                    if 'audio_type' not in seg:
                        seg['audio_type'] = 'speech'
                        
            except Exception:
                pass
        
        print(f"     ğŸ‘‰ Round {i+1} ê²°ê³¼: {tagged_in_this_round}ê°œì˜ ìˆ¨ê²¨ì§„ ìŒì•… êµ¬ê°„ ì¶”ê°€ ê²€ê±°.")
        
        # ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ìƒˆë¡œ ì¡ì€ ê²Œ ì—†ìœ¼ë©´ ë” ëŒë¦´ í•„ìš” ì—†ìŒ
        if tagged_in_this_round == 0:
            print("     âœ… ë” ì´ìƒ ìƒˆë¡œìš´ ìŒì•… êµ¬ê°„ì´ ë°œê²¬ë˜ì§€ ì•Šì•„ ì¡°ê¸° ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    total_music_count = sum(1 for s in seg_list if s.get('audio_type') == 'noise_music')
    print(f"âœ… ìµœì¢… í•„í„°ë§ ì™„ë£Œ. (ì´ ìŒì•… ë¶„ë¥˜: {total_music_count}ê°œ)")
    return seg_list




def apply_sandwich_smoothing(segments, max_duration=1.0):
    """
    1ì´ˆ ì´í•˜ì˜ ì§§ì€ êµ¬ê°„ì´ ì–‘ì˜†ê³¼ ë‹¤ë¥¸ íƒ€ì…ì¼ ê²½ìš°, ì–‘ì˜†ì˜ íƒ€ì…(Context)ì— ë§ì¶° ë³€ê²½í•©ë‹ˆë‹¤.
    - Music (Speech) Music -> Music (Speechë¥¼ Musicìœ¼ë¡œ ë³€ê²½)
    - Speech (Music) Speech -> Speech (Musicì„ Speechë¡œ ë³€ê²½)
    """
    print(f"\nğŸ¥ª [Smoothing] ìƒŒë“œìœ„ì¹˜ ê·œì¹™ ì ìš© ì¤‘ (ê¸°ì¤€: {max_duration}ì´ˆ ì´í•˜)...")
    
    if len(segments) < 3:
        return segments

    changed_count = 0
    
    # ë¦¬ìŠ¤íŠ¸ì˜ ë‘ ë²ˆì§¸ë¶€í„° ë’¤ì—ì„œ ë‘ ë²ˆì§¸ê¹Œì§€ ìˆœíšŒ (ì–‘ì˜†ì„ ë´ì•¼ í•˜ë‹ˆê¹Œìš”)
    for i in range(1, len(segments) - 1):
        prev_seg = segments[i-1]
        curr_seg = segments[i]
        next_seg = segments[i+1]
        
        # í˜„ì¬ êµ¬ê°„ì˜ ê¸¸ì´ ê³„ì‚°
        duration = curr_seg['end'] - curr_seg['start']
        
        # 1ì´ˆ ì´ˆê³¼ë©´ íŒ¨ìŠ¤
        if duration > max_duration:
            continue

        # ê° êµ¬ê°„ì˜ íƒ€ì… ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 'speech'ë¡œ ê°„ì£¼)
        prev_type = prev_seg.get('audio_type', 'speech')
        curr_type = curr_seg.get('audio_type', 'speech')
        next_type = next_seg.get('audio_type', 'speech')

        # Case 1: [ìŒì•…] - (ì§§ì€ ë§) - [ìŒì•…] => ë§ -> ìŒì•…ìœ¼ë¡œ ë³€ê²½
        if curr_type == 'speech' and prev_type == 'noise_music' and next_type == 'noise_music':
            curr_seg['audio_type'] = 'noise_music'
            curr_seg['change_log'] = 'Sandwich Correction (Speech->Music)'
            changed_count += 1
            # print(f"   ğŸ‘‰ {curr_seg['start']:.1f}s: ì§§ì€ ìŒì„±({duration:.2f}s)ì„ ìŒì•… ì‚¬ì´ì— ë§ì¶° ìŒì•…ìœ¼ë¡œ ë³€ê²½")

        # Case 2: [ë§] - (ì§§ì€ ìŒì•…) - [ë§] => ìŒì•… -> ë§ë¡œ ë³€ê²½
        elif curr_type == 'noise_music' and prev_type == 'speech' and next_type == 'speech':
            curr_seg['audio_type'] = 'speech'
            curr_seg['change_log'] = 'Sandwich Correction (Music->Speech)'
            changed_count += 1
            # print(f"   ğŸ‘‰ {curr_seg['start']:.1f}s: ì§§ì€ ìŒì•…({duration:.2f}s)ì„ ìŒì„± ì‚¬ì´ì— ë§ì¶° ìŒì„±ìœ¼ë¡œ ë³€ê²½")

    print(f"âœ… ìƒŒë“œìœ„ì¹˜ ë³´ì • ì™„ë£Œ. (ì´ {changed_count}êµ¬ê°„ ìˆ˜ì •ë¨)")
    return segments




def select_sub_language(audio_file, lang_map, sorted_list, segment_with_lang):
    """ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì„ í† ëŒ€ë¡œ ë³´ì¡°ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    
    filename = re.sub(r'\s+|_', "", Path(audio_file).stem) # ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ë„ì–´ì“°ê¸°ì™€ _ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    filename = re.sub(r'0(\d)ì£¼ì°¨', r'\1ì£¼ì°¨', filename) # ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì˜ ì£¼ì°¨ìˆ«ìë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
    
    sub_lang = None
    for prefix in sorted_list:
        if filename.startswith(prefix): # ê¸¸ì´ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¨ë¡€ë¡œ ìˆœí™˜í•˜ë©° ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì´ í•´ë‹¹ í•­ëª©ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
            sub_lang = lang_map[prefix] # ì°¾ì•„ë‚´ë©´ ìƒì„±ë¼ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì°¸ê³ í•´ ë³´ì¡°ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
            print(f'ë³´ì¡°ì–¸ì–´ë¥¼ {sub_lang}ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.')
            break
    
    if sub_lang == None: # ë§Œì•½ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ê°•ì˜ëª…ì´ ì—†ë‹¤ë©´ ë³´ì¡°ì–¸ì–´ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
        print(f'ì—‘ì…€íŒŒì¼ì— í•´ë‹¹ ê°•ì˜ëª…ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì£¼ì–¸ì–´ ë‹¤ìŒìœ¼ë¡œ ë§ì´ ë“±ì¥í•œ ì–¸ì–´ë¡œ ë³´ì¡°ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.')
        lang_list = [seg['lang'] for seg in segment_with_lang if seg['lang'] not in [MAIN_LANGUAGE, 'unknown']] # ì„¸ê·¸ë¨¼íŠ¸ì™€ ì–¸ì–´ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì£¼ì–¸ì–´ê°€ ì•„ë‹Œ ì–¸ì–´ë“¤ë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
        if lang_list:
            lang_counts = Counter(lang_list) # ì£¼ì–¸ì–´ê°€ ì•„ë‹Œ ì–¸ì–´ë“¤ê³¼ ê·¸ ì–¸ì–´ë“¤ì´ ë‚˜ì˜¨ íšŸìˆ˜ë¥¼ íŠœí”Œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            sub_lang = lang_counts.most_common(1)[0][0] # ì£¼ì–¸ì–´ê°€ ì•„ë‹ˆë©´ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ì–¸ì–´ë¡œ ë³´ì¡°ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.            
            print(f'ë³´ì¡°ì–¸ì–´ë¥¼ {sub_lang}ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.')
    
    return sub_lang




def define_third_language(segment_with_lang, target_languages):
    """ì œ 3ì–¸ì–´ë¥¼ ì„¤ì •í•˜ê³  unknown VADë¥¼ ì´ì „ VADì— í¡ìˆ˜ì‹œí‚µë‹ˆë‹¤."""
    print('\n ì œ 3ì–¸ì–´ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ VADë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.')
    
    set_to_remove = ['unknown']
    set_to_remove = set(set_to_remove) | set(target_languages)
    allowed_langs = set(ALLOWED_LANGS) - set_to_remove
    
    operation_segment_list = segment_with_lang.copy()  # ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡ì„ í˜¹ì‹œ ë°œìƒí• ì§€ ëª¨ë¥¼ ë³€ê²½ì—ì„œ ì˜¨ì „íˆ ë³´ì¡´í•˜ê¸° ìœ„í•´ ë³µì‚¬í•©ë‹ˆë‹¤. 
    lang_durations = {}  # íƒ€ê²Ÿì–¸ì–´ì™€ unknownì„ ì œì™¸í•œ ë‹¤ë¥¸ ì–¸ì–´ë“¤ì˜ durationì„ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•  ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    for segment in operation_segment_list:
        if segment['lang'] in allowed_langs:
            lang = segment['lang']
            duration = segment['end'] - segment['start']
            if lang not in lang_durations:
                lang_durations[lang] = duration
            else:
                duration = lang_durations[lang] + duration
                lang_durations[lang] = duration
        else:
            continue
    
    # lang_duration ë°ì´í„° í˜•íƒœ : {'en': 9.0, 'ja': 3.0, ...}
                
        
    if lang_durations:
        third_lang = max(lang_durations, key=lang_durations.get)
        if lang_durations[third_lang] < 30: # ì œ 3ì–¸ì–´ ê¸¸ì´ì˜ í•©ì´ ì´ 30ì´ˆê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì œ 3ì–¸ì–´ë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            third_lang = None
            print('ì´í•© ê¸¸ì´ê°€ ê°€ì¥ ê¸´ ì œ 3ì–¸ì–´ì˜ ê¸¸ì´ê°€ 30ì´ˆ ë¯¸ë§Œì…ë‹ˆë‹¤.')
        print(f'ì œ 3ì–¸ì–´ë¥¼ {third_lang}ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.')
    else:
        third_lang = None  # íƒ€ê²Ÿì–¸ì–´ì™€ unknownì„ ì œì™¸í•œ ë‹¤ë¥¸ ì–¸ì–´ê°€ ì—†ë‹¤ë©´ ì œ 3ì–¸ì–´ë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        print('ì œ 3ì–¸ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì œ 3ì–¸ì–´ë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
        
    return third_lang




def convert_to_unknown(third_lang, segment_with_lang, target_languages):
    """ì§€ì •ëœ ì œ 3ì–¸ì–´ë¥¼ ë°›ì•„ì™€ íƒ€ê²Ÿì–¸ì–´ì™€ unknown,  ì œ 3ì–¸ì–´ë¥¼ ì œì™¸í•œ ì–¸ì–´ë¥¼ ëª¨ë‘ unknownìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤."""

    set_to_remove = ['unknown']
    set_to_remove = set(set_to_remove) | set(target_languages)


    if third_lang == None or third_lang == 'no_sub':
        print('ì œ 3ì–¸ì–´ê°€ ì§€ì •ë˜ì§€ ì•Šì•„ íƒ€ê²Ÿì–¸ì–´ì™€ unknownì„ ì œì™¸í•œ ì–¸ì–´ë¥¼ ëª¨ë‘ unknownìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.')
        for segment in segment_with_lang:
            if segment['lang'] not in set_to_remove:
                segment['lang'] ='unknown'
    else:
        set_to_remove = set_to_remove | {third_lang}
        print('íƒ€ê²Ÿì–¸ì–´ì™€ unknown, ì œ 3ì–¸ì–´ë¥¼ ì œì™¸í•œ ì–¸ì–´ë¥¼ ëª¨ë‘ unknownìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.')
        for segment in segment_with_lang:
            if segment['lang'] not in set_to_remove:
                segment['lang'] ='unknown'  
    

    return segment_with_lang




def merge_unknown(segment_with_lang):
    ## unknown VAD í¡ìˆ˜ì‘ì—…
    print('unknown VADë¥¼ ë°”ë¡œ ì§ì „ VADì— í¡ìˆ˜ì‹œí‚µë‹ˆë‹¤.')
    segment_with_lang_unknown_merged = []
    for segment in segment_with_lang:
        if segment == segment_with_lang[0]: # ë§¨ ì²«ë²ˆì§¸ 
            segment_with_lang_unknown_merged.append(segment)
        else:
            if segment['lang'] == 'unknown':
                segment_with_lang_unknown_merged[-1]['end'] = segment['end']
            else:
                segment_with_lang_unknown_merged.append(segment)
                
    # segment_with_lang_unknown_merged = [seg for seg in segment_with_lang_unknown_merged if seg['lang'] != 'unknown']
        
        
    return segment_with_lang_unknown_merged




def duration_up_and_down(segment, MAX_DURATION):
    """durationê°’ì„ ê¸°ì¤€ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ up and downìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. upì€ ì´ìƒ, downì€ ë¯¸ë§Œì…ë‹ˆë‹¤."""
    duration = segment['end'] - segment['start']
    if duration < MAX_DURATION:
        return "down"
    else:
        return "up"




def gap_up_and_down(previous, segment, MAX_GAP):
    """gapê°’ì„ ê¸°ì¤€ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ up and downìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. upì€ ì´ìƒ, downì€ ë¯¸ë§Œì…ë‹ˆë‹¤."""
    gap = segment['start'] - previous['end']
    if gap < MAX_GAP:
        return "down"
    else:
        return "up"
   



def merge_vad(merge_list, first, case, MAX_DURATION, MAX_MERGED_DURATION):
    """ë³‘í•©í•˜ê¸°ë¡œ íŒë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì•Œë§ì€ í˜•íƒœë¡œ ë³‘í•©í•˜ê³  ìµœì¢…ë¦¬ìŠ¤íŠ¸ì™€ ì„ì‹œë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""

    work_list = []
    temp_list = []
    final_list = []
    if len(merge_list) != 1: # caseì— final keyê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ”
        for i, seg in enumerate(merge_list):
            work_list.append(seg)
            merged_duration = seg['end'] - work_list[0]['start']
            if merged_duration > MAX_MERGED_DURATION: # ì´ ì‹œê°„ì„ ë„˜ëŠ” ìˆœê°„
                if i == len(merge_list) - 2 and duration_up_and_down(merge_list[i+1], MAX_DURATION) == 'down': 
                    # ê·¸ ìˆœê°„ì˜ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë³‘í•© ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ë§ˆì§€ë§‰ í•­ëª© ë°”ë¡œ ì´ì „ì˜ í•­ëª©ì´ë©´ì„œ ë§ˆì§€ë§‰ í•­ëª©ì˜ ê¸¸ì´ê°€ ìµœëŒ€ì‹œê°„ ë¯¸ë§Œì¼ ê²½ìš°
                    chunk_1 = {'start':work_list[0]['start'], 'end':merge_list[i+1]['end'], 'lang':case['chunk_1_lang']} 
                    # êµ³ì´ ë¶„ë¦¬í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ í•©ì¹©ë‹ˆë‹¤.
                    temp_list.append(chunk_1)
                    break

                elif len(work_list) == 1: # 1ê°œ í•­ëª© ê·¸ ìì²´ë§Œìœ¼ë¡œë„ ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ëŠ”ë‹¤ë©´
                    final_list.append(seg) # ê·¸ í•­ëª©ì€ ë°”ë¡œ ìµœì¢… ë¦¬ìŠ¤íŠ¸ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
                    work_list = [] # ë‹¤ìŒ ì‘ì—…ì„ ìœ„í•´ ì‘ì—… ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ì›ë‹ˆë‹¤.

                elif i == len(merge_list) - 1: # ì „ë¶€ ë³‘í•©í–ˆì„ ë•Œì—ë§Œ ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ëŠ”ë‹¤ë©´
                    if duration_up_and_down(merge_list[i], MAX_DURATION) == 'down': # ë§¨ ë§ˆì§€ë§‰ í•­ëª©ì˜ ê¸¸ì´ê°€ ë¯¸ë§Œì´ë¼ë©´
                        chunk_1 = {'start':work_list[0]['start'], 'end':work_list[-1]['end'], 'lang':case['chunk_1_lang']}
                        temp_list.append(chunk_1) # ì „ë¶€ í•©ì³ì„œ ì„ì‹œ ë¦¬ìŠ¤íŠ¸ì— ë„£ìŠµë‹ˆë‹¤. ì–´ì°¨í”¼ ë‹¤ìŒ ì‘ì—…ì—ì„œ ë§¨ ì²˜ìŒì— ê±¸ëŸ¬ì§ˆ ê²ë‹ˆë‹¤.
                    else: # ë§¨ ë§ˆì§€ë§‰ í•­ëª©ì˜ ê¸¸ì´ê°€ ì´ìƒì´ë¼ë©´
                        chunk_1 = {'start':work_list[0]['start'], 'end':work_list[-2]['end'], 'lang':case['chunk_1_lang']}
                        final_list.append(chunk_1) # ê·¸ ë°”ë¡œ ì•ê¹Œì§€ë§Œ ìë¥¸ ê±¸ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì— ë³´ë‚´ê³ 
                        temp_list.append(seg) # ê¸¸ì´ê°€ ê¸´ ë§¨ ë§ˆì§€ë§‰ í•­ëª©ì€ ì„ì‹œë¦¬ìŠ¤íŠ¸ë¡œ ë³´ëƒ…ë‹ˆë‹¤.

                else: # ê·¸ ì™¸ì˜ ê²½ìš°ì—ëŠ” í•´ë‹¹ í•­ëª©ì„ ì‘ì—… ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê¸° ì§ì „ê¹Œì§€ë§Œ ë³‘í•©í•©ë‹ˆë‹¤.
                    chunk_1 = {'start':work_list[0]['start'], 'end':work_list[-2]['end'], 'lang':case['chunk_1_lang']}
                    final_list.append(chunk_1)
                    work_list = [seg] # ë‹¤ìŒ ì‘ì—…ì„ ìœ„í•´ ì‘ì—… ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹ í•­ëª©ë§Œ ë‚¨ê²¨ë†“ìŠµë‹ˆë‹¤.

            else: # ì´ ì‹œê°„ì„ ë„˜ì§€ ì•Šìœ¼ë©´
                if i == len(merge_list) - 1: # ì „ë¶€ ë³‘í•©í–ˆì„ ë•Œì—ë„ ìµœëŒ€ ê¸¸ì´ë¥¼ ë„˜ì§€ ì•ŠëŠ”ë‹¤ë©´
                    chunk_1 = {'start':work_list[0]['start'], 'end':work_list[-1]['end'], 'lang':case['chunk_1_lang']}
                    temp_list.append(chunk_1) # ì „ë¶€ í•©ì³ì„œ ì„ì‹œ ë¦¬ìŠ¤íŠ¸ì— ë„£ìŠµë‹ˆë‹¤.

                else: # ê·¸ ì™¸ì—ëŠ” ë‹¤ìŒ í•­ëª©ì„ ì¶”ê°€ë¡œ ë°›ì•„ì™€ durationì„ í™•ì¸í•˜ê¸° ìœ„í•´ ê·¸ëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
                    continue

   
    else: # caseì— final keyê°€ ìˆë‹¤ë©´
        final_list = [{'start':merge_list[0]['start'], 'end':merge_list[0]['end'], 'lang':case['chunk_1_lang']}]
        temp_list = [{'start':first['start'], 'end':first['end'], 'lang':case['chunk_2_lang']}]

    return final_list, temp_list




def final_merge_VAD_by_lang(segment_with_lang, sub, third, MAX_DURATION, MAX_GAP, MAX_MERGED_DURATION):
    """ì •ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡ì„ ë°›ì•„ ì •í•´ì§„ ê·œì¹™ì— ë”°ë¼ ë³‘í•©í•©ë‹ˆë‹¤."""
    
    if not segment_with_lang:
        return []
    

    case_1 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_2 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':sub, 'temp':'chunk_1'}
    case_3 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':third, 'temp':'chunk_1'}
    case_4 = {'chunk_1_seg':['temp', 'first'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_5 = {'chunk_1_seg':['temp'], 'chunk_1_lang':'ko', 'chunk_2_seg':['first'], 'chunk_2_lang':'ko', 'final':'chunk_1', 'temp':'chunk_2'}
    case_6 = {'chunk_1_seg':['temp'], 'chunk_1_lang':'ko', 'chunk_2_seg':['first'], 'chunk_2_lang':third, 'final':'chunk_1', 'temp':'chunk_2'}
    case_7 = {'chunk_1_seg':['temp'], 'chunk_1_lang':'ko', 'chunk_2_seg':['first'], 'chunk_2_lang':sub, 'final':'chunk_1', 'temp':'chunk_2'}
    case_8 = {'chunk_1_seg':['temp'], 'chunk_1_lang':sub, 'chunk_2_seg':['first'], 'chunk_2_lang':'ko', 'final':'chunk_1', 'temp':'chunk_2'}
    case_9 = {'chunk_1_seg':['temp'], 'chunk_1_lang':sub, 'chunk_2_seg':['first'], 'chunk_2_lang':third, 'final':'chunk_1', 'temp':'chunk_2'}
    case_10 = {'chunk_1_seg':['temp'], 'chunk_1_lang':sub, 'chunk_2_seg':['first'], 'chunk_2_lang':sub, 'final':'chunk_1', 'temp':'chunk_2'}
    case_11 = {'chunk_1_seg':['temp'], 'chunk_1_lang':third, 'chunk_2_seg':['first'], 'chunk_2_lang':'ko', 'final':'chunk_1', 'temp':'chunk_2'}
    case_12 = {'chunk_1_seg':['temp'], 'chunk_1_lang':third, 'chunk_2_seg':['first'], 'chunk_2_lang':third, 'final':'chunk_1', 'temp':'chunk_2'}
    case_13 = {'chunk_1_seg':['temp'], 'chunk_1_lang':third, 'chunk_2_seg':['first'], 'chunk_2_lang':sub, 'final':'chunk_1', 'temp':'chunk_2'}
    case_14 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_15 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':third, 'temp':'chunk_1'}
    case_16 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':sub, 'temp':'chunk_1'}
    case_17 = {'chunk_1_seg':['temp', 'first', 'second'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_18 = {'chunk_1_seg':['temp', 'first', 'second', 'third'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}
    case_19 = {'chunk_1_seg':['temp', 'first', 'second', 'third'], 'chunk_1_lang':third, 'temp':'chunk_1'}
    case_20 = {'chunk_1_seg':['temp', 'first', 'second', 'third'], 'chunk_1_lang':'ko', 'temp':'chunk_1'}


    all_case = [
        {'temp_lang':'ko', 'first_gap':'down', 'first_lang':'ko', 'case':case_1}, # 1
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_1}, # 2
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 3
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 4
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_1}, # 5
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 6
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_1}, # 7
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_1}, # 8
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'down', 'case':case_1}, # 9
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'up', 'case':case_1}, # 10
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_1}, # 11
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_1}, # 12
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_1}, # 13
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'down', 'case':case_1}, # 14
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'up', 'second_gap':'up', 'case':case_1}, # 15
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_lang':sub, 'case':case_2}, # 16
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'up', 'case':case_2}, # 17
        {'temp_lang':sub, 'first_gap':'down', 'first_lang':sub, 'case':case_2}, # 18
        {'temp_lang':sub, 'temp_dur':'up', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_2}, # 19
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'third_lang':sub, 'case':case_2}, # 20
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'case':case_2}, # 21
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_2}, # 22
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'up', 'second_gap':'down', 'case':case_2}, # 23
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'up', 'second_gap':'up', 'case':case_2}, # 24
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_3}, # 25
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'up', 'case':case_3}, # 26
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_3}, # 27
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'up', 'case':case_3}, # 28
        {'temp_lang':third, 'first_gap':'down', 'first_lang':third, 'case':case_3}, # 29
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_3}, # 30
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_3}, # 31
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'second_dur':'up', 'case':case_4}, # 32
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'case':case_4}, # 33
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_4}, # 34
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':sub, 'case':case_4}, # 35
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':third, 'case':case_4}, # 36
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_4}, # 37
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'up', 'second_lang':third, 'case':case_4}, # 38
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_4}, # 39
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_4}, # 40
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_4}, # 41
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'case':case_4}, # 42
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_4}, # 43
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'up', 'case':case_4}, # 44
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':'ko', 'case':case_4}, # 45
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'up', 'second_lang':sub, 'case':case_4}, # 46
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'up', 'case':case_4}, # 47
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_4}, # 48
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_4}, # 49
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':'ko', 'case':case_4}, # 50
        {'temp_lang':'ko', 'first_gap':'up', 'first_lang':'ko', 'case':case_5}, #51
        {'temp_lang':'ko', 'first_gap':'up', 'first_lang':third, 'case':case_6}, #52
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'case':case_6}, #53
        {'temp_lang':'ko', 'first_gap':'up', 'first_lang':sub, 'case':case_7}, #54
        {'temp_lang':'ko', 'first_gap':'down', 'first_lang':sub, 'first_dur':'up', 'case':case_7}, #55
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'case':case_7}, #56
        {'temp_lang':sub,  'first_gap':'up', 'first_lang':'ko', 'case':case_8}, #57
        {'temp_lang':sub, 'temp_dur':'up', 'first_gap':'down', 'first_lang':'ko', 'case':case_8}, #58
        {'temp_lang':sub, 'first_gap':'up', 'first_lang':third, 'case':case_9}, #59
        {'temp_lang':sub, 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'case':case_9}, #60
        {'temp_lang':sub, 'first_gap':'up', 'first_lang':sub, 'case':case_10}, #61
        {'temp_lang':third, 'first_gap':'up', 'first_lang':'ko', 'case':case_11}, #62
        {'temp_lang':third, 'temp_dur':'up', 'first_gap':'down', 'first_lang':'ko', 'case':case_11}, #63
        {'temp_lang':third, 'first_gap':'up', 'first_lang':third, 'case':case_12}, #64
        {'temp_lang':third, 'first_gap':'up', 'first_lang':sub, 'case':case_13}, #65
        {'temp_lang':third, 'temp_dur':'up', 'first_gap':'down', 'first_lang':sub, 'case':case_13}, #66
        {'temp_lang':'ko', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'case':case_14}, #67
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_15}, #68
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_15}, #69
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'case':case_15}, #70
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_16}, #71
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'up', 'third_lang':third, 'case':case_16}, #72
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_16}, #73
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'case':case_16}, #74
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'up', 'case':case_17}, #75
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':third, 'second_dur':'down', 'case':case_17}, #76
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'up', 'case':case_17}, #77
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'up', 'case':case_17}, #78
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_17}, #79
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'up', 'third_lang':'ko', 'case':case_17}, #80
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'up', 'case':case_17}, #81
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'case':case_17}, #82
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':sub, 'case':case_17}, #83
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'up', 'third_lang':third, 'case':case_17}, #84
        {'temp_lang':third, 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'up', 'case':case_17}, #85
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_18}, #86
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'case':case_18}, #87
        {'temp_lang':'ko', 'temp_dur':'up', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'up', 'third_gap':'down', 'third_lang':'ko', 'case':case_18}, #88
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':third, 'case':case_19}, #89
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':sub, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'down', 'case':case_20}, #90
        {'temp_lang':'ko', 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'down', 'case':case_20}, #91
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':'ko', 'first_dur':'down', 'second_gap':'down', 'second_lang':sub, 'second_dur':'down', 'third_gap':'down', 'third_lang':'ko', 'third_dur':'down', 'case':case_20}, #92
        {'temp_lang':sub, 'temp_dur':'down', 'first_gap':'down', 'first_lang':third, 'first_dur':'down', 'second_gap':'down', 'second_lang':'ko', 'second_dur':'down', 'third_gap':'down', 'third_lang':sub, 'third_dur':'down', 'case':case_20} #93
    ]
   


    sorted_all_case = sorted(all_case, key=len, reverse=True)

    final_segment = [] # ë³‘í•©ì„ ì™„ë£Œí•œ ê²°ê³¼ë¬¼ë“¤ì„ ëª¨ì•„ë†“ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    temp_list = [] # ë³‘í•©ì‘ì—…ì„ ì‹¤í–‰í•  ê³µê°„ì„ ìƒì„±í•©ë‹ˆë‹¤.

   
    temp_list.append(segment_with_lang[0]) # ì²« ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì„ì‹œë¦¬ìŠ¤íŠ¸ì— ë°”ë¡œ ë„£ìŠµë‹ˆë‹¤.
    work_list = segment_with_lang[1:5] # case íŒë‹¨ì„ ìœ„í•œ ì‘ì—…ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    next_list = segment_with_lang[5:] # ë³‘í•©ì‘ì—…ì„ ì§„í–‰í•˜ë©° í•­ëª©ì´ ë¹ ì ¸ë‚˜ê°€ëŠ” ì‘ì—…ë¦¬ìŠ¤íŠ¸ì— ë¹ ì ¸ë‚˜ê°„ë§Œí¼ ë‹¤ìŒ í•­ëª©ì„ ì¶”ê°€í•  ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.

    while work_list: # ìˆœí™˜ì¢…ë£Œ ê¸°ì¤€ì€ ì‘ì—…ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        # âš¡ [ë³´ìŠ¤ì˜ ì˜ë¬¸ì„ í•´ê²°í•˜ëŠ” ì½”ë“œ]
        # merge_vadê°€ tempë¥¼ ë¹„ì›Œì„œ ë³´ëƒˆë‹¤ëŠ” ê±´, ì´ì „ ë©ì–´ë¦¬ê°€ ì™„ê²°ë‚¬ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
        # ê·¸ëŸ¬ë¯€ë¡œ ëŒ€ê¸°ì—´(work_list)ì˜ ì²« ë²ˆì§¸ íƒ€ìë¥¼ ìƒˆë¡œìš´ ê¸°ì¤€ì (temp)ìœ¼ë¡œ ì„¸ì›Œì•¼ í•©ë‹ˆë‹¤.
        if not temp_list:
            if work_list:
                # ëŒ€ê¸°ì—´ì—ì„œ í•˜ë‚˜ êº¼ë‚´ì„œ tempë¡œ ìŠ¹ê²©
                temp_list.append(work_list.pop(0))
                
                # work_listê°€ í•˜ë‚˜ ì¤„ì—ˆìœ¼ë‹ˆ next_listì—ì„œ í•˜ë‚˜ ì¶©ì „
                if next_list:
                    work_list.append(next_list.pop(0))
                
                # ê¸°ì¤€ì ì´ ìƒˆë¡œ ìƒê²¼ìœ¼ë‹ˆ ë‹¤ì‹œ ë£¨í”„ ì‹œì‘ (Case íŒë‹¨)
                continue
            
        if len(work_list) >= 4: # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì˜ ê¸°ë³¸ì ì¸ í•­ëª© ìˆ˜ëŠ” 4ê°œì…ë‹ˆë‹¤.
            temp = temp_list[0] # ì„ì‹œë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í•­ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            first = work_list[0]
            second = work_list[1]
            third = work_list[2] # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” í•­ëª©ì„ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

            sequence = {'temp':temp, 'first':first, 'second':second, 'third':third}
            # ì¸ë±ì‹±ì„ ìœ„í•œ ê° ì„¸ê·¸ë¨¼íŠ¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
          

            temp_lang = temp['lang']
            temp_dur = duration_up_and_down(temp, MAX_DURATION)
            first_gap = gap_up_and_down(temp, first, MAX_GAP)

            first_lang = first['lang']
            first_dur = duration_up_and_down(first, MAX_DURATION)
            second_gap = gap_up_and_down(first, second, MAX_GAP)

            second_lang = second['lang']
            second_dur = duration_up_and_down(second, MAX_DURATION)
            third_gap = gap_up_and_down(second, third, MAX_GAP)

            third_lang = third['lang']
            third_dur = duration_up_and_down(third, MAX_DURATION)


            temp_dict = {'temp_lang':temp_lang, 'temp_dur':temp_dur, 'first_gap':first_gap, 'first_lang':first_lang, 'first_dur':first_dur, 'second_gap':second_gap, 'second_lang':second_lang, 'second_dur':second_dur, 'third_gap':third_gap, 'third_lang':third_lang, 'third_dur':third_dur}
            # ì°¸ê³ í•  ëª¨ë“  ê°’ë“¤ì„ ì €ì¥í•  ì„ì‹œ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
       

            work_case = {} # ìµœì¢…ì ìœ¼ë¡œ all_case ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ëŠ” keyë¥¼ ê°€ì§„ caseë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            selected_case = None # ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ caseë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë¹ˆ ë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            for case in sorted_all_case: # 93ê°€ì§€ì˜ caseë¥¼ ê°€ì¥ keyë¥¼ ë§ì´ ê°€ì§„ caseë¶€í„° í•˜ë‚˜ì”© ì‚´í´ë´…ë‹ˆë‹¤.
                key_list = list(case.keys()) # í•´ë‹¹ caseê°€ ê°€ì§„ keyë“¤ë§Œ ë½‘ì•„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
                key_list.remove('case') # work_caseì—ëŠ” 'case' keyê°€ ì—†ê¸° ë•Œë¬¸ì— 'case' keyë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                for key in key_list: # ì—¬ê¸°ì— ì¡´ì¬í•˜ëŠ” key ëª…ì¹­ì„ í•˜ë‚˜ì”© ê°€ì ¸ì™€
                    work_case[key] = temp_dict[key] # ì°¸ê³ í•  ëª¨ë“  ê°’ë“¤ì„ ì €ì¥í•œ ì„ì‹œ ë”•ì…”ë„ˆë¦¬ ë‚´ì—ì„œ í•´ë‹¹ key, valueë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
                temp_case = case.copy() # work_caseì—ëŠ” 'case' keyê°€ ì—†ê¸° ë•Œë¬¸ì— ë³¸ caseë¥¼ ë³µì‚¬í•˜ì—¬ ì„ì‹œ caseë¥¼ ë§Œë“­ë‹ˆë‹¤.
                del temp_case['case'] # work_caseì™€ ì¼ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ 'case' keyë¥¼ ì œê±°í•©ë‹ˆë‹¤.
                if work_case == temp_case: # ì´ ë‘˜ì´ ì¼ì¹˜í•˜ë©´
                    selected_case = case['case'] # ë³¸ caseì˜ 'case' key ê°’ì„ ê°€ì ¸ì™€ selected_caseì— í• ë‹¹í•©ë‹ˆë‹¤.
                    break
                else : 
                    work_case = {}

            if selected_case == None:
                print(temp_dict)
                print(key_list)
                print(temp_case)
                print(work_case)
       

            merge_list = [sequence[name] for name in selected_case['chunk_1_seg']] # íŒë‹¨ëœ caseì— í•´ë‹¹í•˜ëŠ” ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            long_segments, temp_segment = merge_vad(merge_list, first, selected_case, MAX_DURATION, MAX_MERGED_DURATION) # ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ ê¸¸ì´ ì´í•˜ê°€ ë˜ë„ë¡ ë³‘í•©í•˜ì—¬ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì™€ ì„ì‹œë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            final_segment += long_segments # ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ ë³‘í•© ì‘ì—… ì¤‘ ë‚˜ì˜¨ ìµœëŒ€ ê¸¸ì´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¯¸ë¦¬ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            temp_list = temp_segment # ì„ì‹œë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘í•© ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ ë³‘í•© ì‘ì—… ì¤‘ ë‚˜ì˜¨ ì„ì‹œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ìµœì‹ í™”í•©ë‹ˆë‹¤.
            del_count = len(selected_case['chunk_1_seg']) - 1 # ë³‘í•©ì‘ì—… í›„ ì‘ì—…ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¹ ì ¸ë‚˜ê°€ëŠ” í•­ëª© ê°œìˆ˜ì…ë‹ˆë‹¤.
            if del_count == 0: # 'chunk_1_seg'ê°€ 1ê°œì¸ ê²½ìš°ëŠ” 'chunk_2_seg'ê°€ 1ê°œì¸ ê²½ìš°ë°–ì— ì—†ê¸° ë•Œë¬¸ì— 0ì´ ë˜ëŠ” ìˆœê°„ 1ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤.
                del_count = 1
            work_list.extend(next_list[:del_count]) # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì˜ ë’¤ì— ë‹¤ìŒ í•­ëª©ë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
            del work_list[:del_count]
            del next_list[:del_count] # ì‘ì—…ë¦¬ìŠ¤íŠ¸ì™€ ë‹¤ìŒë¦¬ìŠ¤íŠ¸ì—ì„œ ì¶”ê°€ëœ ë§Œí¼ì˜ í•­ëª© ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.


        else: # ë³‘í•©ì‘ì—…ì„ ì§„í–‰í•˜ë‹¤ ìµœí›„ì— ì‘ì—…ë¦¬ìŠ¤íŠ¸ê°€ 4ê°œ ë¯¸ë§Œì´ ëœë‹¤ë©´
            final_segment = final_segment + temp_list + work_list # ì„ì‹œë¦¬ìŠ¤íŠ¸ì™€ ì‘ì—…ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘í•©í•˜ì§€ ì•Šê³  ì „ë¶€ ìµœì¢…ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            work_list = [] # ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ while ë¬¸ì„ ë¹ ì ¸ë‚˜ê°€ê¸° ìœ„í•œ ì¡°ê±´ì„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.

    return final_segment




def redetect_language_for_merged_segments(merged_segments, waveform, sample_rate, lang_id_model, sub_lang, third_lang):
    """
    ë³‘í•©ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì–¸ì–´ ê°ì§€ë¥¼ ë‹¤ì‹œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë‹¨, ì „ì²´ í—ˆìš© ëª©ë¡ì´ ì•„ë‹ˆë¼ [í•œêµ­ì–´ + ë³´ì¡°ì–¸ì–´ + ì œ3ì–¸ì–´] ë‚´ì—ì„œë§Œ ê²°ì •í•©ë‹ˆë‹¤.
    """
    print("\nğŸ” [Re-detection] ë³‘í•©ëœ êµ¬ê°„ ì–¸ì–´ ì¬ê°ì§€ ì‹œì‘ (íƒ€ê²Ÿ ì–¸ì–´ í•œì •)...")
    
    label_encoder = lang_id_model.hparams.label_encoder
    changed_count = 0

    # 1. ì¬ê°ì§€ í›„ë³´êµ°(Target Languages) ì„¤ì •
    # ë¬´ì¡°ê±´ í•œêµ­ì–´ëŠ” í¬í•¨
    target_langs = {'ko'} 
    if sub_lang:
        target_langs.add(sub_lang)
    if third_lang != None:
        target_langs.add(third_lang)
        
    print(f"   ğŸ¯ ì¬ê°ì§€ í›„ë³´ ì–¸ì–´: {target_langs}")

    # SpeechBrainìš© Tensor ë³€í™˜
    if isinstance(waveform, np.ndarray):
        waveform = torch.from_numpy(waveform)
    
    if DEVICE == "cuda":
        waveform = waveform.to(DEVICE)

    for i, seg in enumerate(merged_segments):
        start = seg['start']
        end = seg['end']
        old_lang = seg['lang']
        
        start_sample = int(start * sample_rate)
        end_sample = int(end * sample_rate)
        
        segment_waveform = waveform[:, start_sample:end_sample]
        
        if segment_waveform.shape[1] < sample_rate * 0.1: 
            continue

        try:
            prediction = lang_id_model.classify_batch(segment_waveform)
        except Exception as e:
            continue

        # ----------------------------------------------------------
        # ğŸ¯ íƒ€ê²Ÿ ì–¸ì–´ ë‚´ í•„í„°ë§ ë¡œì§ (ìˆ˜ì •ë¨)
        # ----------------------------------------------------------
        top_full_label = prediction[3][0]
        top_lang_code = top_full_label.split(':')[0].strip().lower()

        final_new_lang = old_lang 

        # 1ìˆœìœ„ê°€ ìš°ë¦¬ íƒ€ê²Ÿ ëª©ë¡ì— ìˆìœ¼ë©´ ë°”ë¡œ ì±„íƒ
        if top_lang_code in target_langs:
            final_new_lang = top_lang_code
        else:
            # 1ìˆœìœ„ê°€ ì—‰ëš±í•œ ì–¸ì–´ë¼ë©´, íƒ€ê²Ÿ ëª©ë¡ ì¤‘ì—ì„œ í™•ë¥ ì´ ì œì¼ ë†’ì€ ë†ˆ ì°¾ê¸°
            probabilities = prediction[0].squeeze()
            allowed_probs = {}
            
            num_check = min(len(probabilities), len(label_encoder.ind2lab))
            for idx in range(num_check):
                if idx not in label_encoder.ind2lab: continue
                label_str = label_encoder.ind2lab[idx]
                lang_code = label_str.split(':')[0].strip().lower()
                
                # â˜… ì—¬ê¸°ê°€ í•µì‹¬: ì „ì²´ í—ˆìš© ëª©ë¡ì´ ì•„ë‹ˆë¼, 'íƒ€ê²Ÿ ëª©ë¡'ì— ìˆëŠ” ê²ƒë§Œ ê²€ì‚¬
                if lang_code in target_langs:
                    allowed_probs[lang_code] = probabilities[idx].item()
            
            if allowed_probs:
                final_new_lang = max(allowed_probs, key=allowed_probs.get)
            else:
                # íƒ€ê²Ÿ ì–¸ì–´ í™•ë¥ ì´ ì „ë¶€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê·¸ëƒ¥ ì›ë˜ ì–¸ì–´ ìœ ì§€
                final_new_lang = old_lang

        # ----------------------------------------------------------
        # ê²°ê³¼ ë°˜ì˜
        # ----------------------------------------------------------
        if final_new_lang != old_lang:
            # print(f"   ğŸ‘‰ [{i}] ì–¸ì–´ êµì •: {old_lang} -> {final_new_lang} ({start:.1f}s~{end:.1f}s)")
            seg['lang'] = final_new_lang
            seg['change_log'] = seg.get('change_log', '') + f" | Re-detected ({old_lang}->{final_new_lang})"
            changed_count += 1

    print(f"âœ… ì¬ê°ì§€ ì™„ë£Œ. ì´ {len(merged_segments)}ê°œ ì¤‘ {changed_count}ê°œ êµ¬ê°„ ìˆ˜ì •ë¨.")
    return merged_segments




def merge_subtitle_objects(subs):
    """ë‹¨ì–´ ë‹¨ìœ„ ìë§‰(srt.Subtitle ê°ì²´ ë¦¬ìŠ¤íŠ¸)ì„ 2ë‹¨ê³„ì— ê±¸ì³ ë³‘í•©í•©ë‹ˆë‹¤."""
    if not subs:
        return []

    # ====================================================
    # 1ë‹¨ê³„: ë¬¸ì¥ ë° ê¸¸ì´ ê¸°ë°˜ ë³‘í•©
    # ====================================================
    print("  - [ìë§‰ ë³‘í•©] 1ì°¨ ë³‘í•©: ë¬¸ì¥ ë° ê¸¸ì´ ê·œì¹™ì— ë”°ë¼ ë³‘í•© ì¤‘...")
    pass1_subs = []
    current_sub = subs[0]

    for next_sub in subs[1:]:
        gap = (next_sub.start - current_sub.end).total_seconds()
        current_ends_sentence = current_sub.content.endswith(('.', '?', '!'))
        combined_text = current_sub.content + " " + next_sub.content
        
        # ğŸ”¥ [ìˆ˜ì • ìœ„ì¹˜ 1] ë§ˆì»¤ì¸ì§€ í™•ì¸ (ë‚´ìš©ì´ '###'ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ë§ˆì»¤ì„)
        # .strip()ì„ ì¨ì„œ í˜¹ì‹œ ëª¨ë¥¼ ê³µë°±ì„ ì œê±°í•˜ê³  í™•ì¸í•˜ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤.
        is_marker = current_sub.content.strip().startswith('###') or next_sub.content.strip().startswith('###')
        
        should_merge = (
            gap <= MERGE_THRESHOLD_SECONDS and
            not current_ends_sentence and
            len(combined_text) <= MAX_CHARS_PER_LINE and
            not is_marker # ğŸ”¥ ë§ˆì»¤ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì ˆëŒ€ í•©ì¹˜ì§€ ì•ŠìŒ
        )

        if should_merge:
            current_sub.end = next_sub.end
            current_sub.content = combined_text
        else:
            pass1_subs.append(current_sub)
            current_sub = next_sub
            
    pass1_subs.append(current_sub)

    # ====================================================
    # 2ë‹¨ê³„: ì§€ë‚˜ì¹˜ê²Œ ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë¦¬ (ëˆ„ë½ ë°©ì§€ + ë§ˆì»¤ ë³´í˜¸)
    # ====================================================
    print("  - [ìë§‰ ë³‘í•©] 2ì°¨ ë³‘í•©: ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ ì •ë¦¬ ì¤‘ (ëˆ„ë½ ë°©ì§€ ì ìš©)...")
    if len(pass1_subs) < 2:
        return pass1_subs

    final_subs = [pass1_subs[0]]
    
    for i in range(1, len(pass1_subs)):
        previous_sub = final_subs[-1]
        current_sub_to_check = pass1_subs[i]
        
        duration = (current_sub_to_check.end - current_sub_to_check.start).total_seconds()
        gap = (current_sub_to_check.start - previous_sub.end).total_seconds()
        
        # ğŸ”¥ [ìˆ˜ì • ìœ„ì¹˜ 2] ì—¬ê¸°ì„œë„ ë§ˆì»¤ì¸ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì•ˆ ê·¸ëŸ¬ë©´ ì§§ì€ '### ë¯¸ì¸ì‹ ###' ìë§‰ì´ ì• ë¬¸ì¥ì— í¡ìˆ˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        is_marker = previous_sub.content.strip().startswith('###') or current_sub_to_check.content.strip().startswith('###')

        # ì¡°ê±´: (ì§§ìŒ AND ê°€ê¹Œì›€) AND (ë§ˆì»¤ê°€ ì•„ë‹˜)
        if duration < MIN_DURATION_SECONDS and gap < 1.0 and not is_marker:
            new_text = previous_sub.content + " " + current_sub_to_check.content
            
            if len(new_text) <= MAX_CHARS_PER_LINE * 1.5:
                previous_sub.end = current_sub_to_check.end
                previous_sub.content = new_text
            else:
                final_subs.append(current_sub_to_check)
        else:
            final_subs.append(current_sub_to_check)
            
    return final_subs




def refine_segments_with_speaker_analysis(segments, waveform, sample_rate, verification_model, sub_lang):
    print(f"\nğŸ‘‘ [Global] í™”ì ë¶„ì„ ë° ì—­í• (Role) íƒœê¹… ì‹œì‘ (ë³´ì¡°ì–¸ì–´: {sub_lang})")
    
    # === íŠœë‹ íŒŒë¼ë¯¸í„° ===
    CLUSTERING_THRESHOLD = 0.25      # í™”ì êµ¬ë¶„ ìœ ì‚¬ë„ ê¸°ì¤€
    FOREIGNER_RATIO_LIMIT = 0.6      # ì „ì²´ ë°œí™” ì¤‘ 60% ì´ìƒì´ ì™¸êµ­ì–´ë©´ 'ì›ì–´ë¯¼'ìœ¼ë¡œ ê°„ì£¼
    INSTRUCTOR_SHORT_LIMIT = 10.0    # ê°•ì‚¬ì˜ ë°œí™” ì¤‘ ì´ë³´ë‹¤ ì§§ì€ ë³´ì¡°ì–¸ì–´ëŠ” í•œêµ­ì–´ë¡œ ë³€ê²½
    
    total_len = waveform.shape[1]
    
    # --- 1ë‹¨ê³„: í™”ì í´ëŸ¬ìŠ¤í„°ë§ ---
    clusters = [] 
    
    print("   ğŸ“Š ëª©ì†Œë¦¬ ë°ì´í„° ìˆ˜ì§‘ ë° ê·¸ë£¹í™” ì¤‘...")
    
    for i, seg in enumerate(segments):
        start = seg['start']
        end = seg['end']
        lang = seg['lang']
        duration = end - start
        
        is_valid_sample = duration >= 1.0
        
        start_sample = int(start * sample_rate)
        end_sample = int(end * sample_rate)
        if end_sample > total_len: end_sample = total_len
        if start_sample >= end_sample: continue

        seg_waveform = waveform[:, start_sample:end_sample]
        
        if seg_waveform.shape[1] < sample_rate * 0.1:
            current_emb = None
        else:
            current_emb = verification_model.encode_batch(seg_waveform).flatten()
            
        if current_emb is None: continue

        matched_idx = -1
        best_score = -1.0
        
        for c_idx, cluster in enumerate(clusters):
            score = F.cosine_similarity(cluster['centroid'], current_emb, dim=0).item()
            if score > CLUSTERING_THRESHOLD and score > best_score:
                best_score = score
                matched_idx = c_idx
        
        if matched_idx != -1:
            c = clusters[matched_idx]
            n = len(c['ids'])
            c['centroid'] = (c['centroid'] * n + current_emb) / (n + 1)
            c['total_dur'] += duration
            if lang == sub_lang:
                c['sub_lang_dur'] += duration
            c['ids'].append(i)
            
        elif is_valid_sample:
            clusters.append({
                'centroid': current_emb,
                'total_dur': duration,
                'sub_lang_dur': duration if lang == sub_lang else 0.0,
                'ids': [i],
                'role': 'Unassigned'
            })
            
    if not clusters:
        print("âš ï¸ í™”ì ë¶„ì„ ì‹¤íŒ¨. íƒœê¹… ì—†ì´ ì›ë³¸ ë°˜í™˜.")
        return segments

    # --- 2ë‹¨ê³„: ì—­í• (Role) ë¶€ì—¬ ---
    clusters.sort(key=lambda x: x['total_dur'], reverse=True)
    
    clusters[0]['role'] = 'Instructor'
    print(f"   ğŸ¥‡ ê°•ì‚¬(Instructor) í™•ì •: ì´ {clusters[0]['total_dur']:.1f}ì´ˆ ë°œí™”")

    for c in clusters[1:]:
        ratio = c['sub_lang_dur'] / c['total_dur'] if c['total_dur'] > 0 else 0
        if ratio > FOREIGNER_RATIO_LIMIT:
            c['role'] = 'Native_Speaker'
            print(f"   ğŸ‘½ ì›ì–´ë¯¼(Native) ê°ì§€: {sub_lang} ë¹„ìœ¨ {ratio*100:.1f}%")
        else:
            c['role'] = 'Third_Party'
            print(f"   ğŸ‘¥ ì œ3ì(Third_Party) ë¶„ë¥˜: ë°œí™”ëŸ‰ {c['total_dur']:.1f}ì´ˆ")

    # --- 3ë‹¨ê³„: ì„¸ê·¸ë¨¼íŠ¸ì— íƒœê¹… ë° ê°•ì‚¬ êµì • ---
    changed_count = 0
    
    for c in clusters:
        role = c['role']
        
        for seg_idx in c['ids']:
            seg = segments[seg_idx]
            
            # 1. í™”ì ì—­í•  íƒœê¹…
            seg['speaker_role'] = role
            
            # 2. ê°•ì‚¬ êµì • ë¡œì§
            if role == 'Instructor':
                lang = seg['lang']
                duration = seg['end'] - seg['start']
                
                # ê°•ì‚¬ê°€ ì“´ ì§§ì€ ì™¸êµ­ì–´ -> í•œêµ­ì–´ë¡œ ë³€ê²½
                if lang == sub_lang and duration < INSTRUCTOR_SHORT_LIMIT:
                    old_lang = lang
                    new_lang = 'ko'
                    
                    print(f"    ğŸ‘‰ [êµì •] {seg['start']:.1f}ì´ˆ: ê°•ì‚¬ì˜ ì§§ì€ ì™¸êµ­ì–´ -> 'ko'ë¡œ ë³€ê²½")
                    
                    seg['original_lang'] = old_lang
                    # â˜… ë³€ê²½ëœ ë¶€ë¶„: change_logì— êµ¬ì²´ì ì¸ ë³€ê²½ ë‚´ì—­ ê¸°ë¡
                    seg['change_log'] = f"Instructor Correction ({old_lang} -> {new_lang})"
                    seg['lang'] = new_lang
                    
                    changed_count += 1
            
            # ì›ì–´ë¯¼/ì œ3ìëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ

    print(f"âœ… í™”ì íƒœê¹… ë° êµì • ì™„ë£Œ. (ì´ {changed_count}êµ¬ê°„ ìˆ˜ì •ë¨)")
    return segments




def run_stt_and_save_srt(waveform, sample_rate, audio_path, segments, output_folder, instructor_prompt, done_path):
    """
    [Human-in-the-loop ëª¨ë“œ]
    1. íŒŒì¼ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ì €ì¥í•˜ì—¬ STT ì•ˆì •ì„± í™•ë³´.
    2. ì¸ì‹ì´ ì•ˆ ëœ êµ¬ê°„(Gap)ì„ ê³„ì‚°í•˜ì—¬ '### ë¯¸ì¸ì‹ ###' ìë§‰ ìë™ ìƒì„±.
    """
    print("\nğŸš€ 4. STT ë° ìë§‰ ë³‘í•© ì‹œì‘ (Gap Detection Mode)...")
    
    if stt_model is None:
        print("âŒ Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ STTë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        all_word_subs = []
        
        # 1. ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ CPU NumPyë¡œ ë³€í™˜
        if isinstance(waveform, torch.Tensor):
            full_audio_np = waveform.squeeze().cpu().numpy()
        else:
            full_audio_np = np.array(waveform).squeeze()
            
        sr = sample_rate
        total_samples = len(full_audio_np)

        # âš™ï¸ ì„¤ì •ê°’
        PAD_SECONDS = 0.2        # ì•ë’¤ ì—¬ìœ  (Whisper ì¸ì‹ë¥  í–¥ìƒìš©)
        GAP_THRESHOLD = 2.0      # ì´ ì‹œê°„(ì´ˆ) ì´ìƒ ë¹„ë©´ 'ëˆ„ë½'ìœ¼ë¡œ ê°„ì£¼
        
        pad_samples = int(PAD_SECONDS * sr)
        
        # ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        temp_wav_path = os.path.join(output_folder, "temp_gap_process.wav")

        for i, seg in enumerate(segments, 1):
            start_time, end_time = seg['start'], seg['end']
            lang = seg['lang'] if seg['lang'] != 'unknown' else None

            print(f"  - [{i}/{len(segments)}] ì²˜ë¦¬ ì¤‘: {start_time:.2f}s ~ {end_time:.2f}s ({lang})")

            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            
            # 1. ì˜¤ë””ì˜¤ ìë¥´ê¸° (íŒ¨ë”© ì ìš©)
            padded_start_sample = max(0, start_sample - pad_samples)
            padded_end_sample = min(total_samples, end_sample + pad_samples)
            
            segment_audio = full_audio_np[padded_start_sample:padded_end_sample]
            
            # 2. íŒŒì¼ ì €ì¥ (ì•ˆì „ì„± í™•ë³´)
            sf.write(temp_wav_path, segment_audio, sr)
            
            # 3. STT ì‹¤í–‰
            result = stt_model.transcribe(
                temp_wav_path,
                language=lang,
                initial_prompt=instructor_prompt if (lang == MAIN_LANGUAGE) else None,
                word_timestamps=True,
                condition_on_previous_text=False,
                temperature=0.0,
                fp16=True,
                task='transcribe',
                no_speech_threshold=0.95 # íŒŒì¼ ëª¨ë“œë¼ ê¸°ë³¸ê°’ì— ê°€ê¹ê²Œ ë‘  (ë„ˆë¬´ ë‚®ì¶”ë©´ í™˜ê° ë°œìƒ)
            )

            # 4. ê²°ê³¼ ë¶„ì„ ë° Gap ì±„ìš°ê¸°
            
            # ê¸°ì¤€ ì‹œê°„ ì„¤ì • (íŒ¨ë”©ëœ ì˜¤ë””ì˜¤ì˜ ì‹œì‘ì )
            real_start_seconds = padded_start_sample / sr
            offset = timedelta(seconds=real_start_seconds)
            
            # VAD ê¸°ì¤€ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
            audio_duration = len(segment_audio) / sr

            # ë‹¨ì–´ ì¶”ì¶œ
            words = []
            for s in result.segments:
                for w in s.words:
                    if w.word.strip():
                        words.append(w)
            
            # ==========================================================
            # ğŸš¨ CASE 1: ì™„ì „ ì‹¤íŒ¨ (Total Fail)
            # ==========================================================
            if not words:
                print(f"    âš ï¸ í…ìŠ¤íŠ¸ ë¯¸ê²€ì¶œ -> 'íŒë… ë¶ˆê°€' ë§ˆì»¤ ìƒì„± ({audio_duration:.2f}s)")
                # VAD êµ¬ê°„ ì „ì²´ë¥¼ ë¹ˆì¹¸ ìë§‰ìœ¼ë¡œ ìƒì„±
                start_ts = offset
                end_ts = offset + timedelta(seconds=audio_duration)
                
                # íŒ¨ë”© ë•Œë¬¸ì— ê²¹ì¹  ìˆ˜ ìˆìœ¼ë‹ˆ ì‚´ì§ ë³´ì •
                content = f"### íŒë… ë¶ˆê°€ êµ¬ê°„ ({audio_duration:.1f}s) ###"
                all_word_subs.append(srt.Subtitle(index=0, start=start_ts, end=end_ts, content=content))
                
                continue # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¡œ

            # ==========================================================
            # ğŸš¨ CASE 2: ë¶€ë¶„ ëˆ„ë½ (Partial Gap)
            # ==========================================================
            first_word_start = words[0].start
            last_word_end = words[-1].end
            
            # (A) ì•ë¶€ë¶„ ëˆ„ë½ (Head Gap)
            if first_word_start > GAP_THRESHOLD:
                gap_duration = first_word_start
                print(f"    âš ï¸ ì•ë¶€ë¶„ ëˆ„ë½ ê°ì§€: {gap_duration:.2f}s")
                
                g_start = offset
                g_end = offset + timedelta(seconds=first_word_start)
                all_word_subs.append(srt.Subtitle(index=0, start=g_start, end=g_end, 
                                                  content=f"### ì•ë¶€ë¶„ ë¯¸ì¸ì‹ ({gap_duration:.1f}s) ###"))

            # (B) ì •ìƒ í…ìŠ¤íŠ¸ ì¶”ê°€
            for w in words:
                start_ts = timedelta(seconds=w.start) + offset
                end_ts = timedelta(seconds=w.end) + offset
                all_word_subs.append(srt.Subtitle(index=0, start=start_ts, end=end_ts, content=w.word.strip()))

            # (C) ë’·ë¶€ë¶„ ëˆ„ë½ (Tail Gap)
            if (audio_duration - last_word_end) > GAP_THRESHOLD:
                gap_duration = audio_duration - last_word_end
                print(f"    âš ï¸ ë’·ë¶€ë¶„ ëˆ„ë½ ê°ì§€: {gap_duration:.2f}s")
                
                g_start = offset + timedelta(seconds=last_word_end)
                g_end = offset + timedelta(seconds=audio_duration)
                all_word_subs.append(srt.Subtitle(index=0, start=g_start, end=g_end, 
                                                  content=f"### ë’·ë¶€ë¶„ ë¯¸ì¸ì‹ ({gap_duration:.1f}s) ###"))

        # 5. ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_wav_path):
            os.remove(temp_wav_path)

        if not all_word_subs:
            print("  - [ê²½ê³ ] ìƒì„±ëœ ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # STTë¡œ ìƒì„±ëœ ë‹¨ì–´ ìë§‰ë“¤ì„ ë³‘í•©
        merged_subs = merge_subtitle_objects(all_word_subs)
        
        for idx, sub in enumerate(merged_subs, 1):
            sub.index = idx

        srt_content = srt.compose(merged_subs)
        base_filename = Path(audio_path).stem
        srt_filename = f"{base_filename}{FILENAME_SUFFIX}.srt"
        srt_filepath = Path(output_folder) / srt_filename

        with open(srt_filepath, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        print(f"âœ… ìµœì¢… ë³‘í•© SRT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {srt_filepath}")

    except Exception as e:
        print(f"âŒ STT/ë³‘í•© ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        # ì²­ì†Œ
        if 'temp_wav_path' in locals() and os.path.exists(temp_wav_path):
            try: os.remove(temp_wav_path)
            except: pass

        if done_path:
            print("  - ì‘ì—… ì™„ë£Œ í›„ íŒŒì¼ ì´ë™ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            try:
                shutil.move(audio_path, done_path)
                print(f"âœ… ì›ë³¸ WAV íŒŒì¼ ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_path).name}")
            except Exception as e:
                 print(f"âŒ íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print("âœ… ì‘ì—… ì™„ë£Œ. íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ìœ ì§€ë©ë‹ˆë‹¤.")
            
            
            
            
def run_stt_and_save_srt_no_file(waveform, sample_rate, audio_path, segments, output_folder, instructor_prompt, done_path):
    """
    [Human-in-the-loop ëª¨ë“œ - ë©”ëª¨ë¦¬ ê°€ì† ë²„ì „]
    1. íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šê³  ë©”ëª¨ë¦¬(NumPy)ì—ì„œ ì§ì ‘ ì²˜ë¦¬í•˜ì—¬ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
    2. 'int16 ì •ê·œí™”' íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì €ì¥ ë°©ì‹ê³¼ ë™ì¼í•œ ì¸ì‹ë¥ ì„ í™•ë³´í•©ë‹ˆë‹¤.
    3. ì¸ì‹ì´ ì•ˆ ëœ êµ¬ê°„(Gap)ì„ ê³„ì‚°í•˜ì—¬ '### ë¯¸ì¸ì‹ ###' ìë§‰ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("\nğŸš€ 4. STT ë° ìë§‰ ë³‘í•© ì‹œì‘ (Memory Gap Detection Mode)...")
    
    if stt_model is None:
        print("âŒ Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ STTë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        all_word_subs = []
        
        # 1. ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ CPU NumPyë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        if isinstance(waveform, torch.Tensor):
            full_audio_np = waveform.squeeze().cpu().numpy()
        else:
            full_audio_np = np.array(waveform).squeeze()
            
        sr = sample_rate
        total_samples = len(full_audio_np)

        # âš™ï¸ ì„¤ì •ê°’
        PAD_SECONDS = 0.2        # ì•ë’¤ ì—¬ìœ 
        GAP_THRESHOLD = 2.0      # ëˆ„ë½ íŒë‹¨ ê¸°ì¤€ (ì´ˆ)
        
        pad_samples = int(PAD_SECONDS * sr)
        
        # (íŒŒì¼ ê²½ë¡œ ìƒì„± ë¡œì§ ì‚­ì œë¨)

        for i, seg in enumerate(segments, 1):
            start_time, end_time = seg['start'], seg['end']
            lang = seg['lang'] if seg['lang'] != 'unknown' else None

            print(f"  - [{i}/{len(segments)}] ì²˜ë¦¬ ì¤‘: {start_time:.2f}s ~ {end_time:.2f}s ({lang})")

            start_sample = int(start_time * sr)
            end_sample = int(end_time * sr)
            
            # 1. ì˜¤ë””ì˜¤ ìë¥´ê¸° (íŒ¨ë”© ì ìš©)
            padded_start_sample = max(0, start_sample - pad_samples)
            padded_end_sample = min(total_samples, end_sample + pad_samples)
            
            segment_audio = full_audio_np[padded_start_sample:padded_end_sample]
            
            # í˜¹ì‹œ ëª¨ë¥¼ ì°¨ì› ì¶•ì†Œ
            if segment_audio.ndim > 1:
                segment_audio = segment_audio.flatten()

            # =======================================================
            # ğŸ”¥ [í•µì‹¬] "ê°€ìƒ íŒŒì¼ ì €ì¥" íš¨ê³¼ (Int16 Quantization)
            # íŒŒì¼ì„ ì§ì ‘ ì €ì¥í•˜ì§€ ì•Šê³ ë„, ì €ì¥í•œ ê²ƒê³¼ ë˜‘ê°™ì€ ìŒì§ˆ ìƒíƒœë¡œ ë§Œë“­ë‹ˆë‹¤.
            # =======================================================
            # 1. -1.0 ~ 1.0 í´ë¦¬í•‘
            segment_audio = np.clip(segment_audio, -1.0, 1.0)
            # 2. int16 ë³€í™˜ (íŒŒì¼ ì €ì¥ íš¨ê³¼)
            segment_audio_int16 = (segment_audio * 32767).astype(np.int16)
            # 3. float32 ë³µêµ¬ (Whisper ì…ë ¥ìš©)
            segment_audio_clean = segment_audio_int16.astype(np.float32) / 32767.0
            # =======================================================
            
            # 3. STT ì‹¤í–‰ (NumPy ë°°ì—´ ì§ì ‘ ì…ë ¥)
            result = stt_model.transcribe(
                segment_audio_clean,  # <- íŒŒì¼ ê²½ë¡œ ëŒ€ì‹  ë°°ì—´ ì…ë ¥
                language=lang,
                initial_prompt=instructor_prompt if (lang == MAIN_LANGUAGE) else None,
                word_timestamps=True,
                condition_on_previous_text=False,
                temperature=0.0,
                fp16=True,
                task='transcribe',
                no_speech_threshold=0.95 
            )

            # 4. ê²°ê³¼ ë¶„ì„ ë° Gap ì±„ìš°ê¸° (ë¡œì§ ë™ì¼)
            
            # ê¸°ì¤€ ì‹œê°„ ì„¤ì •
            real_start_seconds = padded_start_sample / sr
            offset = timedelta(seconds=real_start_seconds)
            
            # ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
            audio_duration = len(segment_audio_clean) / sr

            # ë‹¨ì–´ ì¶”ì¶œ
            words = []
            for s in result.segments:
                for w in s.words:
                    if w.word.strip():
                        words.append(w)
            
            # ----------------------------------------------------------
            # ğŸš¨ CASE 1: ì™„ì „ ì‹¤íŒ¨ (Total Fail)
            # ----------------------------------------------------------
            if not words:
                print(f"    âš ï¸ í…ìŠ¤íŠ¸ ë¯¸ê²€ì¶œ -> 'íŒë… ë¶ˆê°€' ë§ˆì»¤ ìƒì„± ({audio_duration:.2f}s)")
                start_ts = offset
                end_ts = offset + timedelta(seconds=audio_duration)
                content = f"### íŒë… ë¶ˆê°€ êµ¬ê°„ ({audio_duration:.1f}s) ###"
                all_word_subs.append(srt.Subtitle(index=0, start=start_ts, end=end_ts, content=content))
                continue 

            # ----------------------------------------------------------
            # ğŸš¨ CASE 2: ë¶€ë¶„ ëˆ„ë½ (Partial Gap)
            # ----------------------------------------------------------
            first_word_start = words[0].start
            last_word_end = words[-1].end
            
            # (A) ì•ë¶€ë¶„ ëˆ„ë½ (Head Gap)
            if first_word_start > GAP_THRESHOLD:
                gap_duration = first_word_start
                print(f"    âš ï¸ ì•ë¶€ë¶„ ëˆ„ë½ ê°ì§€: {gap_duration:.2f}s")
                g_start = offset
                g_end = offset + timedelta(seconds=first_word_start)
                all_word_subs.append(srt.Subtitle(index=0, start=g_start, end=g_end, 
                                                  content=f"### ì•ë¶€ë¶„ ë¯¸ì¸ì‹ ({gap_duration:.1f}s) ###"))

            # (B) ì •ìƒ í…ìŠ¤íŠ¸ ì¶”ê°€
            for w in words:
                start_ts = timedelta(seconds=w.start) + offset
                end_ts = timedelta(seconds=w.end) + offset
                all_word_subs.append(srt.Subtitle(index=0, start=start_ts, end=end_ts, content=w.word.strip()))

            # (C) ë’·ë¶€ë¶„ ëˆ„ë½ (Tail Gap)
            if (audio_duration - last_word_end) > GAP_THRESHOLD:
                gap_duration = audio_duration - last_word_end
                print(f"    âš ï¸ ë’·ë¶€ë¶„ ëˆ„ë½ ê°ì§€: {gap_duration:.2f}s")
                g_start = offset + timedelta(seconds=last_word_end)
                g_end = offset + timedelta(seconds=audio_duration)
                all_word_subs.append(srt.Subtitle(index=0, start=g_start, end=g_end, 
                                                  content=f"### ë’·ë¶€ë¶„ ë¯¸ì¸ì‹ ({gap_duration:.1f}s) ###"))

        # (íŒŒì¼ ì‚­ì œ ë¡œì§ ì œê±°ë¨)

        if not all_word_subs:
            print("  - [ê²½ê³ ] ìƒì„±ëœ ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # STTë¡œ ìƒì„±ëœ ë‹¨ì–´ ìë§‰ë“¤ì„ ë³‘í•©
        merged_subs = merge_subtitle_objects(all_word_subs)
        
        for idx, sub in enumerate(merged_subs, 1):
            sub.index = idx

        srt_content = srt.compose(merged_subs)
        base_filename = Path(audio_path).stem
        srt_filename = f"{base_filename}{FILENAME_SUFFIX}.srt"
        srt_filepath = Path(output_folder) / srt_filename

        with open(srt_filepath, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        print(f"âœ… ìµœì¢… ë³‘í•© SRT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {srt_filepath}")

    except Exception as e:
        print(f"âŒ STT/ë³‘í•© ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        # (íŒŒì¼ ì´ë™ ë¡œì§ì€ ë™ì¼)
        if done_path:
            print("  - ì‘ì—… ì™„ë£Œ í›„ íŒŒì¼ ì´ë™ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            try:
                shutil.move(audio_path, done_path)
                print(f"âœ… ì›ë³¸ WAV íŒŒì¼ ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_path).name}")
            except Exception as e:
                 print(f"âŒ íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print("âœ… ì‘ì—… ì™„ë£Œ. íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ìœ ì§€ë©ë‹ˆë‹¤.")
            



def run_stt_and_save_srt_no_merge(waveform, sample_rate, audio_path, segments, output_folder, instructor_prompt, done_path):
    """STT ìˆ˜í–‰ í›„, ë³‘í•© ì—†ì´ ë‹¨ì–´ ë‹¨ìœ„(Word-level) ìë§‰ì„ ê·¸ëŒ€ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\nğŸš€ 4. STT ë° ë‹¨ì–´ ë‹¨ìœ„ ìë§‰ ìƒì„± ì‹œì‘...")
    
    if stt_model is None:
        print("âŒ Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ STTë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        all_word_subs = []
        audio_waveform, sr = waveform, sample_rate

        for i, seg in enumerate(segments, 1):
            start_time, end_time = seg['start'], seg['end']
            lang = seg['lang'] if seg['lang'] != 'unknown' else None

            print(f"   - [{i}/{len(segments)}] STT êµ¬ê°„ ì²˜ë¦¬ ì¤‘: {start_time:.2f}s ~ {end_time:.2f}s (ì–¸ì–´: {lang or 'ìë™ ê°ì§€'})")

            start_sample, end_sample = int(start_time * sr), int(end_time * sr)
            # segment_audio = audio_waveform[:, start_sample:end_sample][0]
            segment_tensor = audio_waveform[:, start_sample:end_sample][0]
            
            # 1. í…ì„œê°€ GPUì— ìˆë‹¤ë©´ CPUë¡œ ë‚´ë¦¼
            if isinstance(segment_tensor, torch.Tensor):
                segment_audio = segment_tensor.cpu().numpy()
            else:
                segment_audio = np.array(segment_tensor)
            
            # 2. ë°ì´í„° íƒ€ì…ì„ float32ë¡œ ê°•ì œ ë³€í™˜ (Whisperê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” í¬ë§·)
            segment_audio = segment_audio.astype(np.float32)

            result = stt_model.transcribe(
                segment_audio,
                language=lang,
                initial_prompt=instructor_prompt if (lang == MAIN_LANGUAGE) else None,
                word_timestamps=True,
                condition_on_previous_text=False,
                temperature=0.0,
                fp16=True,
                task='transcribe'
            )

            offset = timedelta(seconds=start_time)
            
            # ë‹¨ì–´ ë‹¨ìœ„ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‘ ë‹´ìŠµë‹ˆë‹¤.
            for segment in result.segments:
                for word in segment.words:
                    start_ts = timedelta(seconds=word.start) + offset
                    end_ts = timedelta(seconds=word.end) + offset
                    content = word.word.strip()
                    
                    if content:
                        # indexëŠ” ë‚˜ì¤‘ì— ì¼ê´„ì ìœ¼ë¡œ ë§¤ê¹ë‹ˆë‹¤ (0ìœ¼ë¡œ ì„ì‹œ ì €ì¥)
                        all_word_subs.append(srt.Subtitle(index=0, start=start_ts, end=end_ts, content=content))
        
        if not all_word_subs:
            print("   - [ê²½ê³ ] STT ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ì—†ì–´ SRT íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # â–¼â–¼â–¼ [ìˆ˜ì •ëœ ë¶€ë¶„] ìë§‰ ë³‘í•© ë¡œì§ ì œê±° â–¼â–¼â–¼
        # merged_subs = merge_subtitle_objects(all_word_subs) <--- ì´ ì¤„ì„ ì‚­ì œ/ì£¼ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.
        
        print(f"   - ìë§‰ ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤. ì´ {len(all_word_subs)}ê°œì˜ ë‹¨ì–´ê°€ ì €ì¥ë©ë‹ˆë‹¤.")
        
        # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤(ìˆœë²ˆ)ë¥¼ 1ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ë§¤ê¹ë‹ˆë‹¤.
        for idx, sub in enumerate(all_word_subs, 1):
            sub.index = idx

        # ë³‘í•©ëœ ìë§‰ ëŒ€ì‹  ë‹¨ì–´ ìë§‰(all_word_subs)ì„ ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        srt_content = srt.compose(all_word_subs)
        
        base_filename = Path(audio_path).stem
        # íŒŒì¼ëª…ì— _WORDS ë“±ì„ ë¶™ì—¬ì„œ êµ¬ë¶„ì„ ì§“ê³  ì‹¶ìœ¼ì‹œë©´ ì•„ë˜ ì¤„ì„ ìˆ˜ì •í•˜ì„¸ìš”.
        srt_filename = f"{base_filename}{FILENAME_SUFFIX}.srt" 
        srt_filepath = Path(output_folder) / srt_filename

        with open(srt_filepath, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        print(f"âœ… ìµœì¢… ë‹¨ì–´ ë‹¨ìœ„ SRT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {srt_filepath}")

    except Exception as e:
        print(f"âŒ STT ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        if done_path:
            print("   - ì‘ì—… ì™„ë£Œ í›„ íŒŒì¼ ì´ë™ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            try:
                shutil.move(audio_path, done_path)
                print(f"âœ… ì›ë³¸ WAV íŒŒì¼ ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_path).name}")
            except shutil.Error as move_e:
                print(f"âŒ íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {move_e}")
            except Exception as e:
                 print(f"âŒ íŒŒì¼ ì´ë™ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print("âœ… ì‘ì—… ì™„ë£Œ. íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ìœ ì§€ë©ë‹ˆë‹¤.")
            
            
            

def vad_annotation_to_srt_empty_with_lang(final_merged, output_folder, audio_path, file_suffix):
    print("\nğŸ“„ 4. SRT íŒŒì¼ ìƒì„± ì‹œì‘...")

    subs = []
    idx = 1
    for seg in final_merged:
        start, end = float(seg['start']), float(seg['end'])
        
        start_td, end_td = timedelta(seconds=start), timedelta(seconds=end)
        if end_td <= start_td: end_td = start_td + timedelta(milliseconds=10)
        
        content_items = []
        for key, value in seg.items():
            if key not in ['start', 'end']:
                content_items.append(f"{key}: {value}")
                
        if not content_items:
            content_str = " "
        else:
            content_str = "|".join(content_items)
                
        subs.append(srt.Subtitle(index=idx, start=start_td, end=end_td, content=content_str))
        idx += 1
        
    print(f"   - ìµœì¢… SRTì— í¬í•¨ë  êµ¬ê°„ ìˆ˜: {len(subs)}ê°œ")
    srt_text = srt.compose(subs)
    
    base_filename = Path(audio_path).stem # ì›ë³¸ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
    srt_filename = f"{base_filename}{FILENAME_SUFFIX}_{file_suffix}.srt" # ìƒˆ íŒŒì¼ëª…
    srt_filepath = Path(output_folder) / srt_filename # í´ë” ê²½ë¡œì™€ íŒŒì¼ëª… ì¡°í•©
    Path(srt_filepath).write_text(srt_text, encoding="utf-8")
    if not subs: print("\n[âš ï¸ ê²½ê³ ] ìµœì¢… SRT íŒŒì¼ì— í¬í•¨ëœ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"âœ… SRT ì €ì¥ ì™„ë£Œ: {srt_filepath} (ì´ {len(subs)}êµ¬ê°„)")




# ========== ë©”ì¸ ì‹¤í–‰ ë¡œì§ ==========
if __name__ == "__main__":
    root = tk.Tk()
    root.withdraw()
    
    # ğŸ”¥ [ìˆ˜ì • 1] ë³€ìˆ˜ ì´ˆê¸°í™” (ì•ˆì „ì¥ì¹˜)
    # ì—‘ì…€ì„ ì„ íƒí•˜ì§€ ì•Šë”ë¼ë„ ë³€ìˆ˜ê°€ ì¡´ì¬í•´ì•¼ ë‚˜ì¤‘ì— ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.
    lang_map = {}
    sorted_list = []

    print("\nğŸ“Š ê°•ì˜ ì •ë³´ê°€ ë‹´ê¸´ ì—‘ì…€ íŒŒì¼ì´ ìˆë‹¤ë©´ ì„ íƒí•˜ì„¸ìš”. (ì·¨ì†Œ ì‹œ ìë™ ê°ì§€ ëª¨ë“œë¡œ ë™ì‘)")
    xlsx_file_path = filedialog.askopenfilename(title="xlsx íŒŒì¼ ì„ íƒ (ì„ íƒ ì•ˆ í•¨: ì·¨ì†Œ)", filetypes=[("xlsx File", "*.xlsx")])
    
    if xlsx_file_path:
        try:
            lang_map, sorted_list = read_xlsx_and_create_dict(xlsx_file_path)
            print(f"âœ… ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(lang_map)}ê°œì˜ ê°•ì˜ ì •ë³´")
        except Exception as e:
            print(f"âš ï¸ ì—‘ì…€ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (ìë™ ê°ì§€ ëª¨ë“œë¡œ ì „í™˜): {e}")
    else:
        print("âš ï¸ ì—‘ì…€ íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³´ì¡°ì–¸ì–´ëŠ” ì˜¤ë””ì˜¤ ë¶„ì„ì„ í†µí•´ ìë™ ê°ì§€ë©ë‹ˆë‹¤.")

    if lang_id_model is not None and stt_model is not None:
        print("\nğŸµ ë¶„ì„í•  WAV íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (16kHz, mono ê¶Œì¥, ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)")
        audio_files = filedialog.askopenfilenames(
            title="WAV íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
            filetypes=[("WAV Files", "*.wav")]
        )

        if not audio_files:
            print("âŒ íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            print(f"\nì´ {len(audio_files)}ê°œì˜ íŒŒì¼ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
            print("\nğŸ’¾ ê²°ê³¼ SRT íŒŒì¼ì„ ì €ì¥í•  í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            output_path = filedialog.askdirectory(title="SRT íŒŒì¼ì„ ì €ì¥í•  í´ë” ì„ íƒ")
            
            print("\nğŸ“‚ ì™„ë£Œëœ WAV íŒŒì¼ì„ ì´ë™ì‹œí‚¬ í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš” (ì·¨ì†Œ ì‹œ ì´ë™ ì•ˆ í•¨).")
            done_path = filedialog.askdirectory(title="ì™„ë£Œëœ WAV íŒŒì¼ì„ ì´ë™ì‹œí‚¬ í´ë” ì„ íƒ")

            if not output_path:
                print("âŒ ê²°ê³¼ ì €ì¥ í´ë”ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            else:
                if not done_path:
                    print("\nâš ï¸ 'ì™„ë£Œ' í´ë”ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì²˜ë¦¬ëœ íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ê·¸ëŒ€ë¡œ ë‚¨ìŠµë‹ˆë‹¤.")

                for i, audio_file in enumerate(list(audio_files), 1):
                    if not os.path.exists(audio_file):
                        print(f"âš ï¸ [{i}/{len(audio_files)}] íŒŒì¼ '{os.path.basename(audio_file)}'ê°€ ì´ë¯¸ ì´ë™ë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    print(f"\n{'='*60}")
                    print(f"â–¶ï¸  [{i}/{len(audio_files)}] íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(audio_file)}")
                    print(f"{'='*60}")

                    non_silent_segments = get_non_silent_segments_ffmpeg(audio_file)
                    # ffmpegë¥¼ ì´ìš©í•´ ë¨¼ì € ì¹¨ë¬µêµ¬ê°„ì„ ì œì™¸í•©ë‹ˆë‹¤.
                    
                    # ---------- NEW: ìŒì•… êµ¬ê°„ ê°ì§€ + ë³‘í•© ----------
                    ina_segments = detect_music_segments(audio_file)          # (label, start, end) ë¦¬ìŠ¤íŠ¸
                    music_blocks = build_music_blocks(ina_segments, short_speech_max=1.0)  # 1ì´ˆ ì´í•˜ speechëŠ” ìŒì•…ìœ¼ë¡œ í¡ìˆ˜

                    # ffmpeg ê²°ê³¼ê°€ "full_audio"ì¸ ê²½ìš°(ì¹¨ë¬µ ëª» ì°¾ì€ ê²½ìš°)
                    if non_silent_segments == "full_audio":
                        if music_blocks:
                            # ì „ì²´ ê¸¸ì´ì—ì„œ ìŒì•… ë¸”ë¡ë§Œ ë¹¼ê³  ë‚˜ë¨¸ì§€ë§Œ ì‚¬ìš©
                            try:
                                audio_for_duration = AudioSegment.from_file(str(audio_file))
                                total_dur = len(audio_for_duration) / 1000.0  # ms â†’ sec
                            except Exception as e:
                                print(f"   [ê²½ê³ ] ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° ì‹¤íŒ¨: {e}")
                                total_dur = None

                            if total_dur is not None:
                                base_segments = [{"start": 0.0, "end": total_dur}]
                                non_silent_segments = remove_music_from_non_silent(base_segments, music_blocks)
                            else:
                                # ê¸¸ì´ ê³„ì‚° ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ full_audio ìœ ì§€
                                pass
                    else:
                        # í‰ì†Œì—ëŠ” ffmpeg ìœ ì„± êµ¬ê°„ì—ì„œ ìŒì•… ë¸”ë¡ ì œê±°
                        non_silent_segments = remove_music_from_non_silent(non_silent_segments, music_blocks)

                    # ìŒì•… ì œê±° í›„ ë‚¨ì€ ìœ ì„± êµ¬ê°„ì´ ì—†ìœ¼ë©´ ì´ íŒŒì¼ì€ STTë¥¼ ìŠ¤í‚µ
                    if not non_silent_segments:
                        print("   [ì •ë³´] ìŒì•… êµ¬ê°„ì„ ì œê±°í•˜ê³  ë‚˜ë‹ˆ ë‚¨ëŠ” êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. ì´ íŒŒì¼ STT ê±´ë„ˆëœ€.")
                        continue

                    if non_silent_segments: # FFmpeg ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚˜ë©´
                        print("\nğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ì¤‘...")
                        try:
                            audio_loader = Audio(sample_rate=16000, mono=True)
                            waveform, sample_rate = audio_loader(audio_file)
                            print("âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”© ì™„ë£Œ.")
                        except Exception as e:
                            print(f"[ì¹˜ëª…ì  ì˜¤ë¥˜] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                            exit()
                    
                    vad_annotation = extract_segments_2stage(waveform, sample_rate, non_silent_segments) 
                    # VAD ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
                    segment_with_lang = detect_language_for_vad_segments(vad_annotation, waveform, sample_rate, lang_id_model) 
                    # pyannote VAD Annotation ê²°ê³¼ì™€ ì´ë¯¸ ë¡œë“œëœ waveformì„ ì‚¬ìš©í•´ ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
                    segment_with_lang_and_music = tag_noise_by_music_blacklist_iterative(segment_with_lang, ina_segments, waveform, sample_rate, verification_model, threshold=0.4, max_iterations=2)
                    
                    segment_with_lang_and_music2 = apply_sandwich_smoothing(segment_with_lang_and_music, max_duration=1.0)
                    print(f"ğŸ§¹ ìŒì•… ì œê±° ì „: {len(segment_with_lang)}ê°œ")
                    
                    
                    segment_with_lang = [seg for seg in segment_with_lang_and_music2 if seg.get('audio_type') != 'noise_music']
                    print(f"ğŸ§¹ ìŒì•… ì œê±° í›„: {len(segment_with_lang)}ê°œ")
                    
                    
                    # sub_language = None if (SUB_LANGUAGE==None) else SUB_LANGUAGE
                    instructor_prompt = None
                    target_languages = [MAIN_LANGUAGE]
                    if SUB_LANGUAGE == None:
                        sub_language = select_sub_language(audio_file, lang_map, sorted_list, segment_with_lang) 
                        # ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì„ í† ëŒ€ë¡œ ë³´ì¡°ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
                    elif SUB_LANGUAGE == "no_sub":
                        sub_language = None
                    else:
                        sub_language = SUB_LANGUAGE

                    if sub_language != None:
                        target_languages.append(sub_language)
                        
                    instructor_prompt = INSTRUCTOR_PROMPT_DICT.get(sub_language)
                    # ì„¤ì •ëœ ë³´ì¡°ì–¸ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ìš©í•  í”„ë¡¬í”„íŠ¸ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.

                    if THIRD_LANG == 'no_third':
                        third_lang = None
                    else:
                        third_lang = define_third_language(segment_with_lang, target_languages)
                    # ì œ 3ì–¸ì–´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

                    segment_with_lang_still_unknown_in = convert_to_unknown(third_lang, segment_with_lang, target_languages)
                    # ì œ 3ì–¸ì–´ì™€ íƒ€ê²Ÿì–¸ì–´, unknownì„ ì œì™¸í•œ ì–¸ì–´ëŠ” ëª¨ë‘ unknownìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
                    segment_with_lang_without_unknown = merge_unknown(segment_with_lang_still_unknown_in)
                    # ì–¸ì–´ê°€ unknownì¸ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì§ì „ ì„¸ê·¸ë¨¼íŠ¸ì— í¡ìˆ˜ì‹œí‚µë‹ˆë‹¤.

     
                    
                    if segment_with_lang:
                        # final_segment = refine_segments_with_speaker_analysis(segment_with_lang_without_unknown, waveform, sample_rate, verification_model, sub_language)
                        final_segment_2 = final_merge_VAD_by_lang(segment_with_lang_without_unknown, sub_language, third_lang, MAX_DURATION, MAX_GAP, MAX_MERGED_DURATION)
                        final_segment_3 = redetect_language_for_merged_segments(final_segment_2, waveform, sample_rate, lang_id_model, sub_language, third_lang)
                        run_stt_and_save_srt_no_file(waveform, sample_rate, audio_file, final_segment_3, output_path, instructor_prompt, done_path)
                        # run_stt_and_save_srt_no_merge(waveform, sample_rate, audio_file, final_segment_2, output_path, instructor_prompt, done_path)
                        # ìë§‰ë³‘í•© ì „ ë‹¨ì–´ë‹¨ìœ„ srt í™•ì¸ ì½”ë“œ
                        # vad_annotation_to_srt_empty_with_lang(segment_with_lang, output_path, audio_file, file_suffix="vad_lang")
                        # vad_annotation_to_srt_empty_with_lang(segment_with_lang_and_music2, output_path, audio_file, file_suffix="vad_lang_with_music")
                        # vad_annotation_to_srt_empty_with_lang(segment_with_lang_still_unknown_in, output_path, audio_file, file_suffix="vad_lang_with_unknown")
                        # vad_annotation_to_srt_empty_with_lang(segment_with_lang_without_unknown, output_path, audio_file, file_suffix="vad_lang_without_unknown")
                        # vad_annotation_to_srt_empty_with_lang(final_segment, output_path, audio_file, file_suffix="merged_vad")
                        # vad_annotation_to_srt_empty_with_lang(final_segment_2, output_path, audio_file, file_suffix="merged_vad_with_speaker")
                        # vad_annotation_to_srt_empty_with_lang(final_segment_3, output_path, audio_file, file_suffix="merged_vad_with_speaker_2")
                        # STT ì§ì „ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í™•ì¸í•  ë•Œ ì‚¬ìš©í•  ì½”ë“œ
                    
                    else:
                        print("âŒ STTë¥¼ ì§„í–‰í•  ìœ íš¨í•œ êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
                        if done_path:
                            print("  - ì›ë³¸ íŒŒì¼ì„ 'ì™„ë£Œ' í´ë”ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                            try:
                                shutil.move(audio_file, done_path)
                                print(f"  - íŒŒì¼ ì´ë™ ì™„ë£Œ: {Path(done_path) / Path(audio_file).name}")
                            except Exception as e:
                                print(f"  - íŒŒì¼ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        else:
                            print("âœ… ì‘ì—… ì™„ë£Œ. íŒŒì¼ì€ ì›ë³¸ ìœ„ì¹˜ì— ìœ ì§€ë©ë‹ˆë‹¤.")


                print("\n\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ í•„ìˆ˜ ëª¨ë¸ ì¤‘ ì¼ë¶€ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")